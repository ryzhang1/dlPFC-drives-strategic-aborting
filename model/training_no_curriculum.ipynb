{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from Environment import Env\n",
    "from Agent_LSTM import *\n",
    "from pathlib import Path\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(datapath, seed_number, Actor, Critic, \n",
    "             expnoise_std=0.8, TOTAL_EPISODE=5e4, value_noise_std=0):\n",
    "    # get configures\n",
    "    arg = config.ConfigGain(datapath)\n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.save()\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "\n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg, Actor, Critic)\n",
    "    agent.episodic = False\n",
    "    agent.actor.value_noise_std = value_noise_std\n",
    "    \n",
    "    # define exploration noise\n",
    "    noise = ActionNoise(arg.ACTION_DIM, mean=0, std=expnoise_std)\n",
    "    \n",
    "    # Loop now\n",
    "    tot_t = 0\n",
    "    episode = agent.initial_episode\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    actor_loss_log = 0\n",
    "    critic_loss_log = 0\n",
    "    num_update = 1e-5\n",
    "    dist_log = []\n",
    "\n",
    "    LOG_FREQ = 100\n",
    "    REPLAY_PERIOD = 4\n",
    "    PRE_LEARN_PERIOD = arg.BATCH_SIZE * 50\n",
    "    random_stop = True\n",
    "\n",
    "    # Start loop\n",
    "    while episode < TOTAL_EPISODE:\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        x = env.reset()\n",
    "        agent.bstep.reset(env.pro_gains)\n",
    "        \n",
    "        last_action = torch.zeros(1, 1, arg.ACTION_DIM)\n",
    "        last_action_raw = last_action.clone()\n",
    "\n",
    "        state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                           env.target_position_obs.view(1, 1, -1), \n",
    "                           torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "\n",
    "        hiddenin = None\n",
    "        tend = 0\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(arg.EPISODE_LEN):\n",
    "            # 1. Check start threshold.\n",
    "            if not cross_start_threshold and (last_action.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state \n",
    "            # and previous hidden & cell states of LSTM units.\n",
    "            action, action_raw, hiddenout = agent.select_action(state, hiddenin, action_noise=noise)\n",
    "            if random_stop and np.random.rand() > 0.95:\n",
    "                action = torch.zeros_like(action)\n",
    "\n",
    "            # 3. Track next x in the environment.\n",
    "            next_x, reached_target, relative_dist = env(x, action, t - tend)\n",
    "\n",
    "            # 4. Next observation given next x.\n",
    "            next_ox = agent.bstep(next_x)\n",
    "            next_state = torch.cat([next_ox.view(1, 1, -1), action, env.target_position_obs.view(1, 1, -1),\n",
    "                                    torch.ones(1, 1, 1) * t - tend + 1], dim=2).to(arg.device)\n",
    "\n",
    "            # 5. Check whether stop.\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if stopped.          \n",
    "            if is_stop and cross_start_threshold:\n",
    "                reward = env.return_reward(x, reward_mode='mixed')\n",
    "                done = torch.ones(1, 1, 1)\n",
    "            else:\n",
    "                reward = torch.zeros(1, 1, 1)\n",
    "                done = torch.zeros(1, 1, 1)\n",
    "\n",
    "            # 7. Append data.\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            # 8. Update timestep.\n",
    "            last_action_raw = action_raw\n",
    "            last_action = action\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            hiddenin = hiddenout\n",
    "            tot_t += 1\n",
    "\n",
    "            # 9. Update model.\n",
    "            if len(agent.memory.memory) > PRE_LEARN_PERIOD and tot_t % REPLAY_PERIOD == 0:\n",
    "                actor_loss, critic_loss = agent.learn()\n",
    "                actor_loss_log += actor_loss\n",
    "                critic_loss_log += critic_loss\n",
    "                num_update += 1\n",
    "\n",
    "            # 10. whether break.\n",
    "            if is_stop and cross_start_threshold:\n",
    "                step_log.append(t + 1 - tend)\n",
    "                reward_log.append(reward.item())\n",
    "                rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "                dist_log.append(relative_dist.item())\n",
    "                # initialize a trial\n",
    "                cross_start_threshold = False\n",
    "                x = env.reset()\n",
    "                agent.bstep.reset(env.pro_gains)\n",
    "                state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                                   env.target_position_obs.view(1, 1, -1),\n",
    "                                   torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "                tend = t + 1\n",
    "\n",
    "\n",
    "        # store the last state\n",
    "        states.append(state)\n",
    "        # End of one trial, store trajectory into buffer.\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions).to(arg.device)\n",
    "        rewards = torch.cat(rewards).to(arg.device)\n",
    "        dones = torch.cat(dones).to(arg.device)\n",
    "        agent.memory.push(states, actions, rewards, dones) \n",
    "\n",
    "        # store mirrored trajectories reflected along y-axis\n",
    "        agent.memory.push(*agent.mirror_traj(states, actions), rewards, dones) \n",
    "\n",
    "        if episode % LOG_FREQ == LOG_FREQ - 1:\n",
    "            # save\n",
    "            agent.save(save_memory=False, episode=episode, pre_phase=False, full_param=True)\n",
    "            \n",
    "            print(f\"t: {tot_t}, Ep: {episode}, action std: {noise.std:0.2f}\")\n",
    "            print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "                  f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "                  f\"reward rate: {np.sum(reward_log) / np.sum(step_log):0.3f}, \"\n",
    "                  f\"rewarded fraction: {np.mean(rewarded_trial_log):0.3f}, \"\n",
    "                  f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "                  f\"critic loss: {critic_loss_log / num_update:0.3f}, \"\n",
    "                  f\"actor loss: {-actor_loss_log / (num_update/2):0.3f}\")\n",
    "            \n",
    "            reward_rate = np.sum(reward_log) / np.sum(step_log)\n",
    "            if reward_rate > 0.2:\n",
    "                random_stop = False\n",
    "                noise.reset(mean=0, std=0.5)\n",
    "                \n",
    "                \n",
    "            #if np.mean(rewarded_trial_log) > 0.85:\n",
    "            #    break\n",
    "\n",
    "            reward_log = []\n",
    "            rewarded_trial_log = []\n",
    "            step_log = []\n",
    "            actor_loss_log = 0\n",
    "            critic_loss_log = 0\n",
    "            num_update = 1e-5\n",
    "            dist_log = []\n",
    "            \n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ['Actor_novalue_learnb']\n",
    "critics = ['Critic']\n",
    "seeds = [[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]]\n",
    "expnoise_std = 0.8\n",
    "TOTAL_EPISODE = 1e4\n",
    "folder_path = Path('D:/quitting_data/agents_no_curriculum')\n",
    "\n",
    "value_noise_std = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rz31\\AppData\\Local\\anaconda3\\envs\\InductiveBias\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\rz31\\AppData\\Local\\anaconda3\\envs\\InductiveBias\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\rz31\\AppData\\Local\\Temp\\ipykernel_17932\\2583562541.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
      "C:\\Users\\rz31\\AppData\\Local\\Temp\\ipykernel_17932\\2583562541.py:150: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  reward_rate = np.sum(reward_log) / np.sum(step_log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 17.609, mean reward: 0.302, reward rate: 0.017, rewarded fraction: 0.018, relative distance: 263.212, critic loss: 0.022, actor loss: 0.005\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 17.509, mean reward: 0.365, reward rate: 0.021, rewarded fraction: 0.026, relative distance: 253.123, critic loss: 0.052, actor loss: -0.009\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.357, mean reward: 0.452, reward rate: 0.028, rewarded fraction: 0.034, relative distance: 249.282, critic loss: 0.078, actor loss: 0.009\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.140, mean reward: 0.318, reward rate: 0.020, rewarded fraction: 0.019, relative distance: 248.078, critic loss: 0.094, actor loss: -0.040\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.079, mean reward: 0.352, reward rate: 0.022, rewarded fraction: 0.024, relative distance: 250.402, critic loss: 0.074, actor loss: -0.104\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.121, mean reward: 0.220, reward rate: 0.014, rewarded fraction: 0.012, relative distance: 261.592, critic loss: 0.064, actor loss: -0.131\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.448, mean reward: 0.187, reward rate: 0.011, rewarded fraction: 0.010, relative distance: 272.542, critic loss: 0.046, actor loss: -0.168\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.302, mean reward: 0.181, reward rate: 0.011, rewarded fraction: 0.010, relative distance: 278.548, critic loss: 0.038, actor loss: -0.202\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.716, mean reward: 0.165, reward rate: 0.010, rewarded fraction: 0.010, relative distance: 269.931, critic loss: 0.033, actor loss: -0.217\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.039, mean reward: 0.185, reward rate: 0.012, rewarded fraction: 0.012, relative distance: 262.038, critic loss: 0.027, actor loss: -0.211\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 15.804, mean reward: 0.189, reward rate: 0.012, rewarded fraction: 0.014, relative distance: 267.973, critic loss: 0.027, actor loss: -0.219\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 17.451, mean reward: 0.155, reward rate: 0.009, rewarded fraction: 0.008, relative distance: 265.812, critic loss: 0.027, actor loss: -0.232\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 15.784, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 282.263, critic loss: 0.022, actor loss: -0.225\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 17.064, mean reward: 0.088, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 287.399, critic loss: 0.018, actor loss: -0.215\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.390, mean reward: 0.067, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 280.006, critic loss: 0.017, actor loss: -0.198\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 15.866, mean reward: 0.042, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 280.941, critic loss: 0.014, actor loss: -0.181\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 15.414, mean reward: 0.059, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 293.743, critic loss: 0.013, actor loss: -0.168\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 15.885, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 298.003, critic loss: 0.011, actor loss: -0.161\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.389, mean reward: 0.104, reward rate: 0.006, rewarded fraction: 0.006, relative distance: 285.806, critic loss: 0.011, actor loss: -0.150\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.522, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 298.308, critic loss: 0.011, actor loss: -0.140\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.196, mean reward: 0.034, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 296.333, critic loss: 0.009, actor loss: -0.133\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.550, mean reward: 0.066, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 297.342, critic loss: 0.010, actor loss: -0.128\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.550, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 313.951, critic loss: 0.012, actor loss: -0.122\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 15.484, mean reward: 0.028, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 299.963, critic loss: 0.008, actor loss: -0.115\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 17.264, mean reward: 0.069, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 305.420, critic loss: 0.008, actor loss: -0.108\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 17.408, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 298.289, critic loss: 0.007, actor loss: -0.097\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 17.002, mean reward: 0.053, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 303.419, critic loss: 0.008, actor loss: -0.089\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.470, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 309.067, critic loss: 0.010, actor loss: -0.084\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 15.672, mean reward: 0.046, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 311.583, critic loss: 0.007, actor loss: -0.078\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 15.831, mean reward: 0.024, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 304.443, critic loss: 0.006, actor loss: -0.074\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 15.911, mean reward: 0.025, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 302.126, critic loss: 0.005, actor loss: -0.070\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.665, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 307.188, critic loss: 0.006, actor loss: -0.066\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 16.816, mean reward: 0.022, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 305.424, critic loss: 0.005, actor loss: -0.060\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 16.725, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 305.347, critic loss: 0.006, actor loss: -0.052\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.697, mean reward: 0.025, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 315.971, critic loss: 0.005, actor loss: -0.047\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 17.225, mean reward: 0.050, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 293.320, critic loss: 0.006, actor loss: -0.045\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 16.248, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 295.011, critic loss: 0.005, actor loss: -0.043\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 17.547, mean reward: 0.034, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 305.105, critic loss: 0.005, actor loss: -0.046\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 15.815, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 299.721, critic loss: 0.004, actor loss: -0.047\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 17.876, mean reward: 0.021, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 305.820, critic loss: 0.004, actor loss: -0.049\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 16.111, mean reward: 0.028, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 306.486, critic loss: 0.004, actor loss: -0.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 16.029, mean reward: 0.045, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 303.478, critic loss: 0.004, actor loss: -0.056\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 17.548, mean reward: 0.021, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 308.248, critic loss: 0.003, actor loss: -0.058\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 17.057, mean reward: 0.073, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 303.360, critic loss: 0.003, actor loss: -0.058\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.018, mean reward: 0.025, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 310.639, critic loss: 0.005, actor loss: -0.057\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 16.446, mean reward: 0.021, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 307.887, critic loss: 0.004, actor loss: -0.057\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 15.593, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 308.686, critic loss: 0.004, actor loss: -0.058\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 16.688, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 304.245, critic loss: 0.003, actor loss: -0.058\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 16.146, mean reward: 0.026, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 314.835, critic loss: 0.003, actor loss: -0.057\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 16.832, mean reward: 0.020, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 304.555, critic loss: 0.003, actor loss: -0.056\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 17.207, mean reward: 0.024, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 310.158, critic loss: 0.004, actor loss: -0.055\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 16.072, mean reward: 0.028, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 306.982, critic loss: 0.003, actor loss: -0.052\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 17.058, mean reward: 0.030, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 317.987, critic loss: 0.002, actor loss: -0.048\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 15.300, mean reward: 0.060, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 306.090, critic loss: 0.003, actor loss: -0.046\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 16.588, mean reward: 0.049, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 306.422, critic loss: 0.004, actor loss: -0.044\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 16.667, mean reward: 0.021, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 312.027, critic loss: 0.003, actor loss: -0.045\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 15.958, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 312.398, critic loss: 0.003, actor loss: -0.046\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 15.951, mean reward: 0.063, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 304.163, critic loss: 0.004, actor loss: -0.047\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 15.169, mean reward: 0.045, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 310.732, critic loss: 0.004, actor loss: -0.048\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 16.732, mean reward: 0.051, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 311.918, critic loss: 0.003, actor loss: -0.046\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 16.611, mean reward: 0.029, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 317.847, critic loss: 0.003, actor loss: -0.044\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 16.677, mean reward: 0.030, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 314.900, critic loss: 0.003, actor loss: -0.046\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 16.738, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 316.264, critic loss: 0.003, actor loss: -0.047\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 15.159, mean reward: 0.028, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 311.148, critic loss: 0.003, actor loss: -0.050\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 15.790, mean reward: 0.042, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 311.613, critic loss: 0.003, actor loss: -0.054\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 15.757, mean reward: 0.087, reward rate: 0.005, rewarded fraction: 0.006, relative distance: 309.352, critic loss: 0.003, actor loss: -0.056\n",
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 15.904, mean reward: 0.021, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 319.410, critic loss: 0.003, actor loss: -0.059\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 15.576, mean reward: 0.025, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 315.561, critic loss: 0.004, actor loss: -0.062\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 16.033, mean reward: 0.026, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 320.173, critic loss: 0.003, actor loss: -0.065\n",
      "t: 740000, Ep: 7399, action std: 0.80\n",
      "mean steps: 15.836, mean reward: 0.030, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 319.577, critic loss: 0.004, actor loss: -0.067\n",
      "t: 750000, Ep: 7499, action std: 0.80\n",
      "mean steps: 16.752, mean reward: 0.049, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 312.106, critic loss: 0.003, actor loss: -0.070\n",
      "t: 760000, Ep: 7599, action std: 0.80\n",
      "mean steps: 16.440, mean reward: 0.028, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 323.240, critic loss: 0.005, actor loss: -0.074\n",
      "t: 770000, Ep: 7699, action std: 0.80\n",
      "mean steps: 16.145, mean reward: 0.041, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 319.139, critic loss: 0.003, actor loss: -0.078\n",
      "t: 780000, Ep: 7799, action std: 0.80\n",
      "mean steps: 17.464, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 319.666, critic loss: 0.003, actor loss: -0.082\n",
      "t: 790000, Ep: 7899, action std: 0.80\n",
      "mean steps: 16.199, mean reward: 0.018, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 336.658, critic loss: 0.004, actor loss: -0.083\n",
      "t: 800000, Ep: 7999, action std: 0.80\n",
      "mean steps: 16.320, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 359.229, critic loss: 0.003, actor loss: -0.078\n",
      "t: 810000, Ep: 8099, action std: 0.80\n",
      "mean steps: 16.433, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 366.158, critic loss: 0.004, actor loss: -0.070\n",
      "t: 820000, Ep: 8199, action std: 0.80\n",
      "mean steps: 16.080, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 358.455, critic loss: 0.002, actor loss: -0.063\n",
      "t: 830000, Ep: 8299, action std: 0.80\n",
      "mean steps: 14.278, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 355.384, critic loss: 0.013, actor loss: -0.043\n",
      "t: 840000, Ep: 8399, action std: 0.80\n",
      "mean steps: 7.436, mean reward: 0.033, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 297.442, critic loss: 0.051, actor loss: 0.012\n",
      "t: 850000, Ep: 8499, action std: 0.80\n",
      "mean steps: 6.303, mean reward: 0.202, reward rate: 0.032, rewarded fraction: 0.011, relative distance: 260.627, critic loss: 0.043, actor loss: 0.032\n",
      "t: 860000, Ep: 8599, action std: 0.80\n",
      "mean steps: 6.190, mean reward: 0.499, reward rate: 0.081, rewarded fraction: 0.039, relative distance: 250.305, critic loss: 0.070, actor loss: 0.246\n",
      "t: 870000, Ep: 8699, action std: 0.80\n",
      "mean steps: 6.072, mean reward: 0.785, reward rate: 0.129, rewarded fraction: 0.066, relative distance: 258.585, critic loss: 0.180, actor loss: 0.693\n",
      "t: 880000, Ep: 8799, action std: 0.80\n",
      "mean steps: 6.143, mean reward: 0.920, reward rate: 0.150, rewarded fraction: 0.080, relative distance: 243.826, critic loss: 0.392, actor loss: 1.472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 890000, Ep: 8899, action std: 0.80\n",
      "mean steps: 5.691, mean reward: 0.740, reward rate: 0.130, rewarded fraction: 0.060, relative distance: 240.826, critic loss: 0.475, actor loss: 2.514\n",
      "t: 900000, Ep: 8999, action std: 0.80\n",
      "mean steps: 5.900, mean reward: 1.163, reward rate: 0.197, rewarded fraction: 0.104, relative distance: 240.325, critic loss: 0.420, actor loss: 3.354\n",
      "t: 910000, Ep: 9099, action std: 0.80\n",
      "mean steps: 5.978, mean reward: 1.112, reward rate: 0.186, rewarded fraction: 0.098, relative distance: 236.775, critic loss: 0.521, actor loss: 4.308\n",
      "t: 920000, Ep: 9199, action std: 0.80\n",
      "mean steps: 6.314, mean reward: 1.438, reward rate: 0.228, rewarded fraction: 0.129, relative distance: 226.516, critic loss: 0.552, actor loss: 5.368\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 7.536, mean reward: 2.789, reward rate: 0.370, rewarded fraction: 0.262, relative distance: 199.780, critic loss: 0.577, actor loss: 6.474\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 7.367, mean reward: 2.751, reward rate: 0.373, rewarded fraction: 0.263, relative distance: 203.222, critic loss: 0.591, actor loss: 7.599\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 7.512, mean reward: 2.842, reward rate: 0.378, rewarded fraction: 0.269, relative distance: 201.129, critic loss: 0.654, actor loss: 8.724\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 7.916, mean reward: 3.431, reward rate: 0.433, rewarded fraction: 0.329, relative distance: 187.728, critic loss: 0.696, actor loss: 9.751\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 7.941, mean reward: 3.274, reward rate: 0.412, rewarded fraction: 0.309, relative distance: 190.759, critic loss: 0.738, actor loss: 10.767\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 8.054, mean reward: 3.480, reward rate: 0.432, rewarded fraction: 0.335, relative distance: 191.308, critic loss: 0.776, actor loss: 11.832\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 8.520, mean reward: 3.570, reward rate: 0.419, rewarded fraction: 0.338, relative distance: 184.221, critic loss: 0.785, actor loss: 12.870\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 8.509, mean reward: 3.653, reward rate: 0.429, rewarded fraction: 0.350, relative distance: 188.503, critic loss: 0.793, actor loss: 13.835\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 14.604, mean reward: 0.406, reward rate: 0.028, rewarded fraction: 0.031, relative distance: 252.859, critic loss: 0.034, actor loss: 0.034\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 8.356, mean reward: 0.205, reward rate: 0.025, rewarded fraction: 0.014, relative distance: 266.062, critic loss: 0.087, actor loss: 0.422\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 11.978, mean reward: 0.578, reward rate: 0.048, rewarded fraction: 0.039, relative distance: 215.630, critic loss: 0.125, actor loss: 1.026\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 11.789, mean reward: 0.727, reward rate: 0.062, rewarded fraction: 0.054, relative distance: 209.450, critic loss: 0.205, actor loss: 2.500\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 10.119, mean reward: 1.025, reward rate: 0.101, rewarded fraction: 0.083, relative distance: 195.919, critic loss: 0.406, actor loss: 3.496\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 10.175, mean reward: 1.429, reward rate: 0.140, rewarded fraction: 0.119, relative distance: 183.089, critic loss: 0.683, actor loss: 4.453\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 9.806, mean reward: 1.743, reward rate: 0.178, rewarded fraction: 0.150, relative distance: 185.232, critic loss: 1.062, actor loss: 5.648\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 8.112, mean reward: 1.591, reward rate: 0.196, rewarded fraction: 0.138, relative distance: 206.621, critic loss: 1.381, actor loss: 7.631\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 7.822, mean reward: 1.601, reward rate: 0.205, rewarded fraction: 0.144, relative distance: 212.195, critic loss: 1.223, actor loss: 9.325\n",
      "t: 140000, Ep: 1399, action std: 0.50\n",
      "mean steps: 9.605, mean reward: 3.453, reward rate: 0.359, rewarded fraction: 0.325, relative distance: 177.405, critic loss: 1.081, actor loss: 10.910\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 9.469, mean reward: 3.752, reward rate: 0.396, rewarded fraction: 0.360, relative distance: 177.990, critic loss: 1.114, actor loss: 12.368\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 9.801, mean reward: 4.072, reward rate: 0.415, rewarded fraction: 0.389, relative distance: 172.359, critic loss: 1.187, actor loss: 13.550\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 10.472, mean reward: 4.530, reward rate: 0.433, rewarded fraction: 0.434, relative distance: 159.974, critic loss: 1.191, actor loss: 14.638\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 11.271, mean reward: 5.044, reward rate: 0.448, rewarded fraction: 0.481, relative distance: 143.315, critic loss: 1.210, actor loss: 15.735\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 11.819, mean reward: 5.280, reward rate: 0.447, rewarded fraction: 0.504, relative distance: 136.024, critic loss: 1.168, actor loss: 16.833\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 12.287, mean reward: 5.799, reward rate: 0.472, rewarded fraction: 0.554, relative distance: 123.056, critic loss: 1.111, actor loss: 17.909\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 13.048, mean reward: 6.080, reward rate: 0.466, rewarded fraction: 0.576, relative distance: 110.306, critic loss: 1.083, actor loss: 18.923\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 12.878, mean reward: 6.080, reward rate: 0.472, rewarded fraction: 0.576, relative distance: 111.670, critic loss: 1.110, actor loss: 19.868\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 13.584, mean reward: 6.663, reward rate: 0.490, rewarded fraction: 0.639, relative distance: 102.088, critic loss: 1.133, actor loss: 20.693\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 13.983, mean reward: 6.259, reward rate: 0.448, rewarded fraction: 0.595, relative distance: 108.370, critic loss: 1.135, actor loss: 21.353\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 14.894, mean reward: 6.873, reward rate: 0.461, rewarded fraction: 0.657, relative distance: 92.780, critic loss: 1.104, actor loss: 21.851\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 14.987, mean reward: 7.218, reward rate: 0.482, rewarded fraction: 0.697, relative distance: 88.120, critic loss: 1.087, actor loss: 22.184\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 13.571, mean reward: 6.633, reward rate: 0.489, rewarded fraction: 0.636, relative distance: 106.820, critic loss: 1.101, actor loss: 22.499\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 13.320, mean reward: 6.390, reward rate: 0.480, rewarded fraction: 0.616, relative distance: 115.451, critic loss: 1.119, actor loss: 22.734\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 12.615, mean reward: 5.892, reward rate: 0.467, rewarded fraction: 0.563, relative distance: 124.236, critic loss: 1.135, actor loss: 22.924\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 12.470, mean reward: 6.239, reward rate: 0.500, rewarded fraction: 0.604, relative distance: 119.795, critic loss: 1.135, actor loss: 23.108\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 12.468, mean reward: 6.079, reward rate: 0.488, rewarded fraction: 0.586, relative distance: 125.515, critic loss: 1.146, actor loss: 23.282\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 12.565, mean reward: 6.111, reward rate: 0.486, rewarded fraction: 0.591, relative distance: 126.096, critic loss: 1.137, actor loss: 23.484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 12.162, mean reward: 5.830, reward rate: 0.479, rewarded fraction: 0.559, relative distance: 130.551, critic loss: 1.120, actor loss: 23.658\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 12.407, mean reward: 6.176, reward rate: 0.498, rewarded fraction: 0.597, relative distance: 123.330, critic loss: 1.100, actor loss: 23.823\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 12.232, mean reward: 6.129, reward rate: 0.501, rewarded fraction: 0.595, relative distance: 126.686, critic loss: 1.084, actor loss: 23.940\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 12.476, mean reward: 5.908, reward rate: 0.474, rewarded fraction: 0.563, relative distance: 124.839, critic loss: 1.103, actor loss: 24.046\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 12.620, mean reward: 6.289, reward rate: 0.498, rewarded fraction: 0.606, relative distance: 121.684, critic loss: 1.092, actor loss: 24.165\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.778, mean reward: 6.320, reward rate: 0.495, rewarded fraction: 0.606, relative distance: 116.706, critic loss: 1.093, actor loss: 24.288\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.787, mean reward: 6.399, reward rate: 0.500, rewarded fraction: 0.616, relative distance: 116.876, critic loss: 1.074, actor loss: 24.381\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 12.886, mean reward: 6.307, reward rate: 0.489, rewarded fraction: 0.606, relative distance: 117.623, critic loss: 1.068, actor loss: 24.476\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 13.081, mean reward: 6.500, reward rate: 0.497, rewarded fraction: 0.623, relative distance: 112.422, critic loss: 1.082, actor loss: 24.519\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 13.020, mean reward: 6.305, reward rate: 0.484, rewarded fraction: 0.604, relative distance: 117.224, critic loss: 1.068, actor loss: 24.563\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.944, mean reward: 6.164, reward rate: 0.476, rewarded fraction: 0.590, relative distance: 120.083, critic loss: 1.069, actor loss: 24.612\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.875, mean reward: 6.415, reward rate: 0.498, rewarded fraction: 0.618, relative distance: 116.882, critic loss: 1.062, actor loss: 24.637\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 13.141, mean reward: 6.507, reward rate: 0.495, rewarded fraction: 0.625, relative distance: 112.684, critic loss: 1.055, actor loss: 24.679\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.983, mean reward: 6.678, reward rate: 0.514, rewarded fraction: 0.646, relative distance: 109.929, critic loss: 1.050, actor loss: 24.726\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 13.199, mean reward: 6.662, reward rate: 0.505, rewarded fraction: 0.645, relative distance: 111.446, critic loss: 1.042, actor loss: 24.746\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 12.944, mean reward: 6.452, reward rate: 0.498, rewarded fraction: 0.620, relative distance: 115.344, critic loss: 1.038, actor loss: 24.765\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 12.872, mean reward: 6.270, reward rate: 0.487, rewarded fraction: 0.603, relative distance: 119.138, critic loss: 1.064, actor loss: 24.773\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 13.439, mean reward: 6.648, reward rate: 0.495, rewarded fraction: 0.641, relative distance: 108.696, critic loss: 1.043, actor loss: 24.785\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 13.365, mean reward: 6.820, reward rate: 0.510, rewarded fraction: 0.659, relative distance: 105.825, critic loss: 1.042, actor loss: 24.809\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 12.879, mean reward: 6.255, reward rate: 0.486, rewarded fraction: 0.597, relative distance: 117.608, critic loss: 1.041, actor loss: 24.810\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 12.171, mean reward: 6.088, reward rate: 0.500, rewarded fraction: 0.586, relative distance: 126.952, critic loss: 1.039, actor loss: 24.848\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 13.101, mean reward: 6.498, reward rate: 0.496, rewarded fraction: 0.627, relative distance: 115.246, critic loss: 1.020, actor loss: 24.876\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.513, mean reward: 6.211, reward rate: 0.496, rewarded fraction: 0.600, relative distance: 126.578, critic loss: 1.012, actor loss: 24.903\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 13.039, mean reward: 6.311, reward rate: 0.484, rewarded fraction: 0.601, relative distance: 116.743, critic loss: 1.043, actor loss: 24.883\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.972, mean reward: 6.445, reward rate: 0.497, rewarded fraction: 0.617, relative distance: 113.156, critic loss: 1.046, actor loss: 24.888\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.894, mean reward: 6.367, reward rate: 0.494, rewarded fraction: 0.613, relative distance: 117.406, critic loss: 1.035, actor loss: 24.909\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.918, mean reward: 6.700, reward rate: 0.519, rewarded fraction: 0.648, relative distance: 111.755, critic loss: 1.017, actor loss: 24.945\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.810, mean reward: 6.274, reward rate: 0.490, rewarded fraction: 0.598, relative distance: 119.014, critic loss: 1.039, actor loss: 24.954\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 13.341, mean reward: 6.829, reward rate: 0.512, rewarded fraction: 0.660, relative distance: 106.696, critic loss: 1.026, actor loss: 24.952\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 13.307, mean reward: 6.854, reward rate: 0.515, rewarded fraction: 0.661, relative distance: 103.540, critic loss: 1.045, actor loss: 24.984\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 13.059, mean reward: 6.489, reward rate: 0.497, rewarded fraction: 0.625, relative distance: 113.894, critic loss: 1.029, actor loss: 24.979\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.299, mean reward: 6.604, reward rate: 0.497, rewarded fraction: 0.641, relative distance: 114.452, critic loss: 1.011, actor loss: 25.006\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 13.382, mean reward: 6.809, reward rate: 0.509, rewarded fraction: 0.657, relative distance: 105.351, critic loss: 1.047, actor loss: 25.056\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.992, mean reward: 6.716, reward rate: 0.517, rewarded fraction: 0.650, relative distance: 109.746, critic loss: 1.030, actor loss: 25.054\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 13.195, mean reward: 6.550, reward rate: 0.496, rewarded fraction: 0.634, relative distance: 116.201, critic loss: 1.025, actor loss: 25.021\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.953, mean reward: 6.588, reward rate: 0.509, rewarded fraction: 0.637, relative distance: 114.773, critic loss: 1.021, actor loss: 25.009\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.695, mean reward: 6.347, reward rate: 0.500, rewarded fraction: 0.612, relative distance: 119.633, critic loss: 1.021, actor loss: 24.982\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 13.160, mean reward: 6.567, reward rate: 0.499, rewarded fraction: 0.634, relative distance: 115.099, critic loss: 1.009, actor loss: 24.976\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.791, mean reward: 6.392, reward rate: 0.500, rewarded fraction: 0.615, relative distance: 118.257, critic loss: 1.003, actor loss: 25.005\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 13.128, mean reward: 6.746, reward rate: 0.514, rewarded fraction: 0.652, relative distance: 108.190, critic loss: 1.018, actor loss: 25.015\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 13.064, mean reward: 6.561, reward rate: 0.502, rewarded fraction: 0.634, relative distance: 115.070, critic loss: 1.007, actor loss: 25.040\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 13.066, mean reward: 6.737, reward rate: 0.516, rewarded fraction: 0.649, relative distance: 109.587, critic loss: 1.028, actor loss: 25.036\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 13.249, mean reward: 6.511, reward rate: 0.491, rewarded fraction: 0.621, relative distance: 110.015, critic loss: 1.031, actor loss: 25.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 13.467, mean reward: 6.874, reward rate: 0.510, rewarded fraction: 0.663, relative distance: 105.437, critic loss: 1.004, actor loss: 24.986\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 13.695, mean reward: 7.124, reward rate: 0.520, rewarded fraction: 0.693, relative distance: 100.444, critic loss: 0.988, actor loss: 24.990\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 13.438, mean reward: 6.907, reward rate: 0.514, rewarded fraction: 0.669, relative distance: 107.222, critic loss: 1.030, actor loss: 24.983\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 13.822, mean reward: 6.889, reward rate: 0.498, rewarded fraction: 0.667, relative distance: 107.831, critic loss: 1.003, actor loss: 24.987\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 13.418, mean reward: 6.809, reward rate: 0.507, rewarded fraction: 0.661, relative distance: 110.873, critic loss: 0.984, actor loss: 24.954\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 13.932, mean reward: 7.186, reward rate: 0.516, rewarded fraction: 0.697, relative distance: 98.843, critic loss: 0.996, actor loss: 24.967\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 13.890, mean reward: 6.917, reward rate: 0.498, rewarded fraction: 0.664, relative distance: 101.742, critic loss: 1.008, actor loss: 24.953\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 13.650, mean reward: 7.059, reward rate: 0.517, rewarded fraction: 0.686, relative distance: 103.874, critic loss: 1.020, actor loss: 24.947\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 13.045, mean reward: 6.425, reward rate: 0.493, rewarded fraction: 0.619, relative distance: 119.120, critic loss: 1.043, actor loss: 24.941\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 13.444, mean reward: 6.746, reward rate: 0.502, rewarded fraction: 0.646, relative distance: 106.477, critic loss: 1.901, actor loss: 24.886\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 13.486, mean reward: 6.528, reward rate: 0.484, rewarded fraction: 0.622, relative distance: 110.849, critic loss: 1.663, actor loss: 24.938\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 13.631, mean reward: 6.847, reward rate: 0.502, rewarded fraction: 0.657, relative distance: 102.088, critic loss: 1.080, actor loss: 25.038\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 13.935, mean reward: 7.288, reward rate: 0.523, rewarded fraction: 0.710, relative distance: 93.254, critic loss: 1.068, actor loss: 25.121\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 13.623, mean reward: 7.028, reward rate: 0.516, rewarded fraction: 0.677, relative distance: 99.232, critic loss: 1.029, actor loss: 25.149\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 13.179, mean reward: 6.855, reward rate: 0.520, rewarded fraction: 0.666, relative distance: 109.412, critic loss: 1.035, actor loss: 25.164\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 13.824, mean reward: 7.215, reward rate: 0.522, rewarded fraction: 0.702, relative distance: 96.939, critic loss: 1.046, actor loss: 25.142\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 13.678, mean reward: 6.560, reward rate: 0.480, rewarded fraction: 0.626, relative distance: 107.618, critic loss: 1.071, actor loss: 25.076\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 13.404, mean reward: 6.772, reward rate: 0.505, rewarded fraction: 0.658, relative distance: 112.695, critic loss: 1.011, actor loss: 25.055\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 13.480, mean reward: 6.885, reward rate: 0.511, rewarded fraction: 0.665, relative distance: 106.582, critic loss: 0.973, actor loss: 25.031\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.097, mean reward: 6.680, reward rate: 0.510, rewarded fraction: 0.646, relative distance: 110.655, critic loss: 1.036, actor loss: 25.019\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.152, mean reward: 6.698, reward rate: 0.509, rewarded fraction: 0.646, relative distance: 111.175, critic loss: 1.056, actor loss: 24.983\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 13.319, mean reward: 6.835, reward rate: 0.513, rewarded fraction: 0.662, relative distance: 106.509, critic loss: 1.063, actor loss: 24.957\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.293, mean reward: 7.018, reward rate: 0.528, rewarded fraction: 0.683, relative distance: 104.867, critic loss: 1.037, actor loss: 24.966\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 13.760, mean reward: 6.991, reward rate: 0.508, rewarded fraction: 0.671, relative distance: 97.995, critic loss: 1.091, actor loss: 24.940\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 13.848, mean reward: 7.108, reward rate: 0.513, rewarded fraction: 0.687, relative distance: 98.187, critic loss: 1.047, actor loss: 24.880\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.573, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 356.869, critic loss: 0.002, actor loss: 0.024\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 8.522, mean reward: 0.032, reward rate: 0.004, rewarded fraction: 0.001, relative distance: 312.301, critic loss: 0.003, actor loss: 0.007\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 6.881, mean reward: 0.024, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 314.165, critic loss: 0.006, actor loss: 0.019\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 12.677, mean reward: 0.397, reward rate: 0.031, rewarded fraction: 0.029, relative distance: 262.074, critic loss: 0.026, actor loss: 0.166\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.769, mean reward: 0.758, reward rate: 0.045, rewarded fraction: 0.060, relative distance: 224.841, critic loss: 0.080, actor loss: 0.254\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.245, mean reward: 0.708, reward rate: 0.044, rewarded fraction: 0.053, relative distance: 222.599, critic loss: 0.118, actor loss: 0.206\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.447, mean reward: 0.851, reward rate: 0.052, rewarded fraction: 0.066, relative distance: 225.708, critic loss: 0.126, actor loss: 0.132\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.021, mean reward: 0.839, reward rate: 0.052, rewarded fraction: 0.067, relative distance: 219.642, critic loss: 0.149, actor loss: 0.062\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.671, mean reward: 0.753, reward rate: 0.045, rewarded fraction: 0.058, relative distance: 217.830, critic loss: 0.159, actor loss: 0.024\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.712, mean reward: 0.541, reward rate: 0.032, rewarded fraction: 0.038, relative distance: 218.754, critic loss: 0.151, actor loss: 0.008\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 16.340, mean reward: 0.603, reward rate: 0.037, rewarded fraction: 0.045, relative distance: 222.657, critic loss: 0.120, actor loss: -0.018\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.451, mean reward: 0.755, reward rate: 0.046, rewarded fraction: 0.058, relative distance: 215.094, critic loss: 0.102, actor loss: -0.045\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.438, mean reward: 0.782, reward rate: 0.048, rewarded fraction: 0.059, relative distance: 228.899, critic loss: 0.087, actor loss: -0.065\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 15.830, mean reward: 0.532, reward rate: 0.034, rewarded fraction: 0.035, relative distance: 227.498, critic loss: 0.078, actor loss: -0.095\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.002, mean reward: 0.728, reward rate: 0.045, rewarded fraction: 0.058, relative distance: 214.299, critic loss: 0.068, actor loss: -0.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.006, mean reward: 0.859, reward rate: 0.054, rewarded fraction: 0.071, relative distance: 223.469, critic loss: 0.066, actor loss: -0.146\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 7.458, mean reward: 0.066, reward rate: 0.009, rewarded fraction: 0.002, relative distance: 264.231, critic loss: 0.335, actor loss: 0.285\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 6.619, mean reward: 0.075, reward rate: 0.011, rewarded fraction: 0.002, relative distance: 268.051, critic loss: 0.230, actor loss: 0.256\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 6.404, mean reward: 0.075, reward rate: 0.012, rewarded fraction: 0.003, relative distance: 255.630, critic loss: 0.223, actor loss: 0.268\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 6.921, mean reward: 0.366, reward rate: 0.053, rewarded fraction: 0.021, relative distance: 230.678, critic loss: 0.271, actor loss: 0.348\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 7.025, mean reward: 0.899, reward rate: 0.128, rewarded fraction: 0.074, relative distance: 217.332, critic loss: 0.585, actor loss: 1.010\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 6.936, mean reward: 0.942, reward rate: 0.136, rewarded fraction: 0.078, relative distance: 230.114, critic loss: 0.979, actor loss: 1.967\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 7.386, mean reward: 1.532, reward rate: 0.207, rewarded fraction: 0.139, relative distance: 224.197, critic loss: 1.159, actor loss: 3.256\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 9.674, mean reward: 3.421, reward rate: 0.354, rewarded fraction: 0.324, relative distance: 184.151, critic loss: 1.207, actor loss: 4.645\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 10.022, mean reward: 4.251, reward rate: 0.424, rewarded fraction: 0.406, relative distance: 166.255, critic loss: 1.217, actor loss: 6.223\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 10.645, mean reward: 4.765, reward rate: 0.448, rewarded fraction: 0.451, relative distance: 157.661, critic loss: 1.145, actor loss: 7.976\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 11.592, mean reward: 5.608, reward rate: 0.484, rewarded fraction: 0.541, relative distance: 139.009, critic loss: 0.986, actor loss: 9.480\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 11.812, mean reward: 5.384, reward rate: 0.456, rewarded fraction: 0.517, relative distance: 143.914, critic loss: 0.967, actor loss: 10.831\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 12.089, mean reward: 5.569, reward rate: 0.461, rewarded fraction: 0.529, relative distance: 135.074, critic loss: 1.033, actor loss: 12.170\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 12.447, mean reward: 5.969, reward rate: 0.480, rewarded fraction: 0.575, relative distance: 127.593, critic loss: 1.024, actor loss: 13.422\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 13.371, mean reward: 6.354, reward rate: 0.475, rewarded fraction: 0.608, relative distance: 113.768, critic loss: 1.016, actor loss: 14.597\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 13.133, mean reward: 6.201, reward rate: 0.472, rewarded fraction: 0.598, relative distance: 121.518, critic loss: 1.009, actor loss: 15.762\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 13.239, mean reward: 6.480, reward rate: 0.489, rewarded fraction: 0.623, relative distance: 114.040, critic loss: 0.989, actor loss: 16.870\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.555, mean reward: 6.095, reward rate: 0.485, rewarded fraction: 0.585, relative distance: 125.664, critic loss: 0.990, actor loss: 17.839\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 13.268, mean reward: 6.633, reward rate: 0.500, rewarded fraction: 0.636, relative distance: 109.791, critic loss: 0.989, actor loss: 18.757\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 13.562, mean reward: 6.824, reward rate: 0.503, rewarded fraction: 0.664, relative distance: 108.533, critic loss: 0.983, actor loss: 19.614\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 13.828, mean reward: 6.942, reward rate: 0.502, rewarded fraction: 0.673, relative distance: 104.990, critic loss: 0.982, actor loss: 20.406\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 13.806, mean reward: 6.803, reward rate: 0.493, rewarded fraction: 0.661, relative distance: 110.603, critic loss: 0.982, actor loss: 21.122\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 13.550, mean reward: 6.679, reward rate: 0.493, rewarded fraction: 0.641, relative distance: 108.492, critic loss: 0.996, actor loss: 21.692\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 13.444, mean reward: 6.950, reward rate: 0.517, rewarded fraction: 0.676, relative distance: 107.833, critic loss: 1.001, actor loss: 22.185\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 13.458, mean reward: 6.662, reward rate: 0.495, rewarded fraction: 0.643, relative distance: 111.219, critic loss: 1.020, actor loss: 22.621\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.941, mean reward: 6.385, reward rate: 0.493, rewarded fraction: 0.614, relative distance: 118.049, critic loss: 1.016, actor loss: 22.982\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 12.852, mean reward: 6.337, reward rate: 0.493, rewarded fraction: 0.610, relative distance: 120.956, critic loss: 1.014, actor loss: 23.298\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 13.022, mean reward: 6.415, reward rate: 0.493, rewarded fraction: 0.618, relative distance: 117.331, critic loss: 1.030, actor loss: 23.566\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 12.461, mean reward: 6.042, reward rate: 0.485, rewarded fraction: 0.581, relative distance: 126.502, critic loss: 1.039, actor loss: 23.855\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 13.315, mean reward: 6.580, reward rate: 0.494, rewarded fraction: 0.636, relative distance: 112.438, critic loss: 1.024, actor loss: 24.069\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 13.261, mean reward: 6.653, reward rate: 0.502, rewarded fraction: 0.641, relative distance: 111.352, critic loss: 1.009, actor loss: 24.296\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 12.952, mean reward: 6.598, reward rate: 0.509, rewarded fraction: 0.636, relative distance: 113.503, critic loss: 1.038, actor loss: 24.439\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 13.093, mean reward: 6.601, reward rate: 0.504, rewarded fraction: 0.640, relative distance: 115.688, critic loss: 1.049, actor loss: 24.606\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 13.209, mean reward: 6.655, reward rate: 0.504, rewarded fraction: 0.643, relative distance: 113.110, critic loss: 1.031, actor loss: 24.764\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 13.458, mean reward: 6.474, reward rate: 0.481, rewarded fraction: 0.625, relative distance: 116.563, critic loss: 1.040, actor loss: 24.858\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 13.006, mean reward: 6.577, reward rate: 0.506, rewarded fraction: 0.642, relative distance: 120.790, critic loss: 1.024, actor loss: 24.969\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.286, mean reward: 6.260, reward rate: 0.510, rewarded fraction: 0.601, relative distance: 123.864, critic loss: 1.055, actor loss: 25.074\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.943, mean reward: 6.468, reward rate: 0.500, rewarded fraction: 0.624, relative distance: 118.177, critic loss: 1.070, actor loss: 25.103\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 13.104, mean reward: 6.623, reward rate: 0.505, rewarded fraction: 0.638, relative distance: 113.012, critic loss: 1.077, actor loss: 25.164\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.843, mean reward: 6.454, reward rate: 0.503, rewarded fraction: 0.629, relative distance: 122.215, critic loss: 1.076, actor loss: 25.223\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 13.097, mean reward: 6.382, reward rate: 0.487, rewarded fraction: 0.615, relative distance: 119.486, critic loss: 1.067, actor loss: 25.311\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 13.045, mean reward: 6.725, reward rate: 0.516, rewarded fraction: 0.654, relative distance: 113.154, critic loss: 1.089, actor loss: 25.322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 13.459, mean reward: 6.864, reward rate: 0.510, rewarded fraction: 0.666, relative distance: 109.082, critic loss: 1.093, actor loss: 25.361\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.477, mean reward: 6.608, reward rate: 0.490, rewarded fraction: 0.636, relative distance: 113.028, critic loss: 1.106, actor loss: 25.413\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.421, mean reward: 6.444, reward rate: 0.519, rewarded fraction: 0.621, relative distance: 121.007, critic loss: 1.099, actor loss: 25.433\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.750, mean reward: 6.250, reward rate: 0.490, rewarded fraction: 0.600, relative distance: 122.203, critic loss: 1.082, actor loss: 25.477\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 13.024, mean reward: 6.758, reward rate: 0.519, rewarded fraction: 0.657, relative distance: 112.759, critic loss: 1.102, actor loss: 25.516\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.972, mean reward: 6.463, reward rate: 0.498, rewarded fraction: 0.623, relative distance: 117.834, critic loss: 1.130, actor loss: 25.547\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.561, mean reward: 6.169, reward rate: 0.491, rewarded fraction: 0.591, relative distance: 124.346, critic loss: 1.112, actor loss: 25.565\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 13.131, mean reward: 6.536, reward rate: 0.498, rewarded fraction: 0.632, relative distance: 115.687, critic loss: 1.131, actor loss: 25.585\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 13.159, mean reward: 6.570, reward rate: 0.499, rewarded fraction: 0.630, relative distance: 114.074, critic loss: 1.112, actor loss: 25.623\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 13.092, mean reward: 6.647, reward rate: 0.508, rewarded fraction: 0.642, relative distance: 112.915, critic loss: 1.115, actor loss: 25.604\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 13.195, mean reward: 6.872, reward rate: 0.521, rewarded fraction: 0.668, relative distance: 109.201, critic loss: 1.102, actor loss: 25.608\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 13.397, mean reward: 6.834, reward rate: 0.510, rewarded fraction: 0.663, relative distance: 108.518, critic loss: 1.107, actor loss: 25.586\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 12.760, mean reward: 6.452, reward rate: 0.506, rewarded fraction: 0.625, relative distance: 118.770, critic loss: 1.111, actor loss: 25.575\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 13.424, mean reward: 6.740, reward rate: 0.502, rewarded fraction: 0.644, relative distance: 106.114, critic loss: 1.119, actor loss: 25.571\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 13.206, mean reward: 6.650, reward rate: 0.504, rewarded fraction: 0.643, relative distance: 113.214, critic loss: 1.113, actor loss: 25.584\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 13.067, mean reward: 6.677, reward rate: 0.511, rewarded fraction: 0.646, relative distance: 112.615, critic loss: 1.118, actor loss: 25.606\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 13.048, mean reward: 6.557, reward rate: 0.503, rewarded fraction: 0.630, relative distance: 114.581, critic loss: 1.116, actor loss: 25.633\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 13.193, mean reward: 6.827, reward rate: 0.517, rewarded fraction: 0.660, relative distance: 108.367, critic loss: 1.127, actor loss: 25.610\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 13.061, mean reward: 6.726, reward rate: 0.515, rewarded fraction: 0.652, relative distance: 111.431, critic loss: 1.136, actor loss: 25.596\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.752, mean reward: 6.483, reward rate: 0.508, rewarded fraction: 0.625, relative distance: 117.962, critic loss: 1.147, actor loss: 25.632\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 11.643, mean reward: 5.699, reward rate: 0.489, rewarded fraction: 0.551, relative distance: 148.154, critic loss: 1.142, actor loss: 25.649\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 11.745, mean reward: 5.993, reward rate: 0.510, rewarded fraction: 0.582, relative distance: 140.484, critic loss: 1.089, actor loss: 25.663\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 11.227, mean reward: 5.763, reward rate: 0.513, rewarded fraction: 0.554, relative distance: 145.227, critic loss: 1.093, actor loss: 25.739\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 11.630, mean reward: 6.018, reward rate: 0.517, rewarded fraction: 0.577, relative distance: 135.884, critic loss: 1.121, actor loss: 25.762\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 11.032, mean reward: 5.701, reward rate: 0.517, rewarded fraction: 0.547, relative distance: 147.013, critic loss: 1.107, actor loss: 25.805\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 11.460, mean reward: 5.740, reward rate: 0.501, rewarded fraction: 0.548, relative distance: 144.939, critic loss: 1.153, actor loss: 25.851\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 10.860, mean reward: 5.574, reward rate: 0.513, rewarded fraction: 0.537, relative distance: 156.267, critic loss: 1.122, actor loss: 25.880\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 10.623, mean reward: 5.607, reward rate: 0.528, rewarded fraction: 0.540, relative distance: 152.242, critic loss: 1.111, actor loss: 25.930\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 10.168, mean reward: 5.375, reward rate: 0.529, rewarded fraction: 0.523, relative distance: 164.641, critic loss: 1.114, actor loss: 25.922\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 10.827, mean reward: 5.496, reward rate: 0.508, rewarded fraction: 0.528, relative distance: 157.033, critic loss: 1.129, actor loss: 25.947\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 10.578, mean reward: 5.510, reward rate: 0.521, rewarded fraction: 0.534, relative distance: 159.274, critic loss: 1.099, actor loss: 25.993\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 10.266, mean reward: 5.507, reward rate: 0.536, rewarded fraction: 0.533, relative distance: 161.698, critic loss: 1.133, actor loss: 26.015\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 10.003, mean reward: 5.406, reward rate: 0.540, rewarded fraction: 0.527, relative distance: 167.440, critic loss: 1.119, actor loss: 26.033\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 9.824, mean reward: 5.139, reward rate: 0.523, rewarded fraction: 0.496, relative distance: 172.423, critic loss: 1.111, actor loss: 26.063\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 10.246, mean reward: 5.329, reward rate: 0.520, rewarded fraction: 0.514, relative distance: 165.381, critic loss: 1.125, actor loss: 26.066\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 10.249, mean reward: 5.557, reward rate: 0.542, rewarded fraction: 0.541, relative distance: 160.221, critic loss: 1.140, actor loss: 26.066\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 10.197, mean reward: 5.639, reward rate: 0.553, rewarded fraction: 0.550, relative distance: 160.114, critic loss: 1.108, actor loss: 26.109\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 9.946, mean reward: 5.407, reward rate: 0.544, rewarded fraction: 0.526, relative distance: 165.739, critic loss: 1.134, actor loss: 26.161\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 11.255, mean reward: 0.102, reward rate: 0.009, rewarded fraction: 0.005, relative distance: 270.488, critic loss: 0.008, actor loss: 0.037\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.655, mean reward: 0.349, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 257.529, critic loss: 0.038, actor loss: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 14.785, mean reward: 0.462, reward rate: 0.031, rewarded fraction: 0.033, relative distance: 249.794, critic loss: 0.062, actor loss: 0.211\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 8.687, mean reward: 0.452, reward rate: 0.052, rewarded fraction: 0.035, relative distance: 234.545, critic loss: 0.121, actor loss: 0.692\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 13.031, mean reward: 0.923, reward rate: 0.071, rewarded fraction: 0.075, relative distance: 208.115, critic loss: 0.180, actor loss: 1.810\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 11.242, mean reward: 0.686, reward rate: 0.061, rewarded fraction: 0.048, relative distance: 198.153, critic loss: 0.324, actor loss: 3.012\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 10.547, mean reward: 1.130, reward rate: 0.107, rewarded fraction: 0.093, relative distance: 190.952, critic loss: 0.851, actor loss: 4.092\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 10.340, mean reward: 1.328, reward rate: 0.128, rewarded fraction: 0.113, relative distance: 200.979, critic loss: 1.052, actor loss: 5.369\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 8.518, mean reward: 1.683, reward rate: 0.198, rewarded fraction: 0.152, relative distance: 214.739, critic loss: 1.017, actor loss: 7.078\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 7.783, mean reward: 1.802, reward rate: 0.231, rewarded fraction: 0.163, relative distance: 213.082, critic loss: 0.846, actor loss: 8.598\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 9.431, mean reward: 3.672, reward rate: 0.389, rewarded fraction: 0.349, relative distance: 176.842, critic loss: 0.841, actor loss: 9.933\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 9.895, mean reward: 4.204, reward rate: 0.425, rewarded fraction: 0.404, relative distance: 166.381, critic loss: 1.055, actor loss: 11.204\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 9.921, mean reward: 4.137, reward rate: 0.417, rewarded fraction: 0.394, relative distance: 169.041, critic loss: 1.187, actor loss: 12.472\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 10.194, mean reward: 4.487, reward rate: 0.440, rewarded fraction: 0.428, relative distance: 165.058, critic loss: 1.259, actor loss: 13.706\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 10.255, mean reward: 4.543, reward rate: 0.443, rewarded fraction: 0.433, relative distance: 160.452, critic loss: 1.240, actor loss: 14.962\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 9.990, mean reward: 4.242, reward rate: 0.425, rewarded fraction: 0.402, relative distance: 168.446, critic loss: 1.173, actor loss: 16.115\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 10.446, mean reward: 4.831, reward rate: 0.462, rewarded fraction: 0.465, relative distance: 158.043, critic loss: 1.132, actor loss: 17.192\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 10.522, mean reward: 4.807, reward rate: 0.457, rewarded fraction: 0.460, relative distance: 159.738, critic loss: 1.110, actor loss: 18.123\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 10.716, mean reward: 4.943, reward rate: 0.461, rewarded fraction: 0.472, relative distance: 154.742, critic loss: 1.115, actor loss: 18.957\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 10.860, mean reward: 5.116, reward rate: 0.471, rewarded fraction: 0.492, relative distance: 151.554, critic loss: 1.130, actor loss: 19.665\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 10.992, mean reward: 5.492, reward rate: 0.500, rewarded fraction: 0.533, relative distance: 145.292, critic loss: 1.123, actor loss: 20.305\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 11.195, mean reward: 5.567, reward rate: 0.497, rewarded fraction: 0.542, relative distance: 145.562, critic loss: 1.126, actor loss: 20.969\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 11.751, mean reward: 5.586, reward rate: 0.475, rewarded fraction: 0.538, relative distance: 139.654, critic loss: 1.135, actor loss: 21.609\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 11.960, mean reward: 5.730, reward rate: 0.479, rewarded fraction: 0.548, relative distance: 132.734, critic loss: 1.124, actor loss: 22.188\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 12.107, mean reward: 5.828, reward rate: 0.481, rewarded fraction: 0.560, relative distance: 130.942, critic loss: 1.107, actor loss: 22.678\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 12.100, mean reward: 5.831, reward rate: 0.482, rewarded fraction: 0.561, relative distance: 132.484, critic loss: 1.106, actor loss: 23.129\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 12.348, mean reward: 5.644, reward rate: 0.457, rewarded fraction: 0.534, relative distance: 131.710, critic loss: 1.120, actor loss: 23.574\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 13.299, mean reward: 6.527, reward rate: 0.491, rewarded fraction: 0.626, relative distance: 109.824, critic loss: 1.121, actor loss: 23.963\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 13.789, mean reward: 6.819, reward rate: 0.494, rewarded fraction: 0.655, relative distance: 101.333, critic loss: 1.096, actor loss: 24.261\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 14.131, mean reward: 6.978, reward rate: 0.494, rewarded fraction: 0.671, relative distance: 97.051, critic loss: 1.093, actor loss: 24.548\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 14.173, mean reward: 6.787, reward rate: 0.479, rewarded fraction: 0.648, relative distance: 100.531, critic loss: 1.096, actor loss: 24.800\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 14.142, mean reward: 6.811, reward rate: 0.482, rewarded fraction: 0.648, relative distance: 98.445, critic loss: 1.098, actor loss: 25.027\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 13.881, mean reward: 6.824, reward rate: 0.492, rewarded fraction: 0.654, relative distance: 103.448, critic loss: 1.102, actor loss: 25.205\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 14.367, mean reward: 6.905, reward rate: 0.481, rewarded fraction: 0.659, relative distance: 96.068, critic loss: 1.099, actor loss: 25.367\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 13.646, mean reward: 6.957, reward rate: 0.510, rewarded fraction: 0.674, relative distance: 104.410, critic loss: 1.102, actor loss: 25.500\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 13.658, mean reward: 6.820, reward rate: 0.499, rewarded fraction: 0.656, relative distance: 104.823, critic loss: 1.099, actor loss: 25.617\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 13.695, mean reward: 6.759, reward rate: 0.494, rewarded fraction: 0.650, relative distance: 105.624, critic loss: 1.104, actor loss: 25.740\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 14.037, mean reward: 7.084, reward rate: 0.505, rewarded fraction: 0.684, relative distance: 99.098, critic loss: 1.063, actor loss: 25.892\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 14.131, mean reward: 6.918, reward rate: 0.490, rewarded fraction: 0.670, relative distance: 103.193, critic loss: 1.071, actor loss: 26.046\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 14.171, mean reward: 7.178, reward rate: 0.507, rewarded fraction: 0.695, relative distance: 96.339, critic loss: 1.071, actor loss: 26.163\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 14.557, mean reward: 7.200, reward rate: 0.495, rewarded fraction: 0.697, relative distance: 91.810, critic loss: 1.071, actor loss: 26.261\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 14.099, mean reward: 7.110, reward rate: 0.504, rewarded fraction: 0.684, relative distance: 97.310, critic loss: 1.053, actor loss: 26.334\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 14.542, mean reward: 7.553, reward rate: 0.519, rewarded fraction: 0.733, relative distance: 87.279, critic loss: 1.072, actor loss: 26.396\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 14.236, mean reward: 7.238, reward rate: 0.508, rewarded fraction: 0.704, relative distance: 94.974, critic loss: 1.083, actor loss: 26.466\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 14.047, mean reward: 7.139, reward rate: 0.508, rewarded fraction: 0.693, relative distance: 96.796, critic loss: 1.037, actor loss: 26.518\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 14.527, mean reward: 7.189, reward rate: 0.495, rewarded fraction: 0.690, relative distance: 90.686, critic loss: 1.050, actor loss: 26.581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 14.184, mean reward: 6.943, reward rate: 0.490, rewarded fraction: 0.671, relative distance: 102.904, critic loss: 1.057, actor loss: 26.602\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 14.856, mean reward: 7.581, reward rate: 0.510, rewarded fraction: 0.739, relative distance: 85.756, critic loss: 1.069, actor loss: 26.675\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 14.167, mean reward: 7.045, reward rate: 0.497, rewarded fraction: 0.680, relative distance: 97.595, critic loss: 1.078, actor loss: 26.715\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 14.881, mean reward: 7.506, reward rate: 0.504, rewarded fraction: 0.727, relative distance: 85.823, critic loss: 1.061, actor loss: 26.761\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 14.073, mean reward: 7.180, reward rate: 0.510, rewarded fraction: 0.698, relative distance: 99.593, critic loss: 1.077, actor loss: 26.797\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 14.881, mean reward: 7.370, reward rate: 0.495, rewarded fraction: 0.714, relative distance: 91.488, critic loss: 1.071, actor loss: 26.799\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 15.184, mean reward: 7.654, reward rate: 0.504, rewarded fraction: 0.741, relative distance: 79.954, critic loss: 1.067, actor loss: 26.865\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 15.131, mean reward: 7.768, reward rate: 0.513, rewarded fraction: 0.752, relative distance: 79.493, critic loss: 1.054, actor loss: 26.835\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 14.447, mean reward: 6.882, reward rate: 0.476, rewarded fraction: 0.665, relative distance: 102.486, critic loss: 1.034, actor loss: 26.864\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 15.235, mean reward: 7.722, reward rate: 0.507, rewarded fraction: 0.747, relative distance: 80.861, critic loss: 1.063, actor loss: 26.882\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 14.873, mean reward: 7.478, reward rate: 0.503, rewarded fraction: 0.717, relative distance: 82.302, critic loss: 1.034, actor loss: 26.871\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 14.279, mean reward: 7.285, reward rate: 0.510, rewarded fraction: 0.704, relative distance: 91.326, critic loss: 1.035, actor loss: 26.899\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 14.974, mean reward: 7.529, reward rate: 0.503, rewarded fraction: 0.729, relative distance: 86.350, critic loss: 1.058, actor loss: 26.928\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 14.754, mean reward: 7.434, reward rate: 0.504, rewarded fraction: 0.715, relative distance: 84.166, critic loss: 1.034, actor loss: 26.935\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 14.739, mean reward: 7.406, reward rate: 0.502, rewarded fraction: 0.716, relative distance: 88.289, critic loss: 1.064, actor loss: 26.904\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 15.138, mean reward: 7.530, reward rate: 0.497, rewarded fraction: 0.727, relative distance: 84.458, critic loss: 1.044, actor loss: 26.920\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 13.755, mean reward: 6.956, reward rate: 0.506, rewarded fraction: 0.667, relative distance: 99.316, critic loss: 1.070, actor loss: 26.918\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 14.946, mean reward: 7.401, reward rate: 0.495, rewarded fraction: 0.715, relative distance: 88.259, critic loss: 1.082, actor loss: 26.906\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 14.690, mean reward: 7.458, reward rate: 0.508, rewarded fraction: 0.724, relative distance: 90.001, critic loss: 1.045, actor loss: 26.885\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 14.128, mean reward: 6.903, reward rate: 0.489, rewarded fraction: 0.666, relative distance: 101.550, critic loss: 1.021, actor loss: 26.832\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 14.612, mean reward: 7.352, reward rate: 0.503, rewarded fraction: 0.714, relative distance: 92.118, critic loss: 1.042, actor loss: 26.837\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 14.301, mean reward: 7.334, reward rate: 0.513, rewarded fraction: 0.708, relative distance: 90.687, critic loss: 1.037, actor loss: 26.837\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 14.440, mean reward: 7.116, reward rate: 0.493, rewarded fraction: 0.685, relative distance: 97.331, critic loss: 1.052, actor loss: 26.801\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 14.402, mean reward: 7.133, reward rate: 0.495, rewarded fraction: 0.686, relative distance: 93.968, critic loss: 1.055, actor loss: 26.822\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 14.009, mean reward: 7.153, reward rate: 0.511, rewarded fraction: 0.689, relative distance: 95.503, critic loss: 1.039, actor loss: 26.789\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 14.392, mean reward: 7.328, reward rate: 0.509, rewarded fraction: 0.708, relative distance: 88.528, critic loss: 1.051, actor loss: 26.803\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 14.275, mean reward: 7.137, reward rate: 0.500, rewarded fraction: 0.688, relative distance: 95.122, critic loss: 1.085, actor loss: 26.790\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.336, mean reward: 6.977, reward rate: 0.487, rewarded fraction: 0.673, relative distance: 98.000, critic loss: 1.086, actor loss: 26.784\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 14.514, mean reward: 7.525, reward rate: 0.518, rewarded fraction: 0.731, relative distance: 88.750, critic loss: 1.065, actor loss: 26.779\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 14.700, mean reward: 7.500, reward rate: 0.510, rewarded fraction: 0.729, relative distance: 87.210, critic loss: 1.063, actor loss: 26.807\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 14.824, mean reward: 7.595, reward rate: 0.512, rewarded fraction: 0.734, relative distance: 84.822, critic loss: 1.083, actor loss: 26.796\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 13.968, mean reward: 6.981, reward rate: 0.500, rewarded fraction: 0.673, relative distance: 98.154, critic loss: 1.063, actor loss: 26.727\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 14.649, mean reward: 7.542, reward rate: 0.515, rewarded fraction: 0.732, relative distance: 88.025, critic loss: 1.051, actor loss: 26.704\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 14.203, mean reward: 7.307, reward rate: 0.514, rewarded fraction: 0.709, relative distance: 93.404, critic loss: 1.056, actor loss: 26.672\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 15.327, mean reward: 7.971, reward rate: 0.520, rewarded fraction: 0.779, relative distance: 75.802, critic loss: 1.039, actor loss: 26.641\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 14.371, mean reward: 7.232, reward rate: 0.503, rewarded fraction: 0.699, relative distance: 93.324, critic loss: 1.042, actor loss: 26.627\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 14.425, mean reward: 7.301, reward rate: 0.506, rewarded fraction: 0.710, relative distance: 95.044, critic loss: 1.066, actor loss: 26.619\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 14.491, mean reward: 7.239, reward rate: 0.500, rewarded fraction: 0.697, relative distance: 90.485, critic loss: 1.052, actor loss: 26.631\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 15.047, mean reward: 7.534, reward rate: 0.501, rewarded fraction: 0.729, relative distance: 83.390, critic loss: 1.028, actor loss: 26.651\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 14.519, mean reward: 7.454, reward rate: 0.513, rewarded fraction: 0.722, relative distance: 87.680, critic loss: 1.046, actor loss: 26.666\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 14.474, mean reward: 7.377, reward rate: 0.510, rewarded fraction: 0.717, relative distance: 88.594, critic loss: 1.051, actor loss: 26.650\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 14.776, mean reward: 7.795, reward rate: 0.528, rewarded fraction: 0.760, relative distance: 81.442, critic loss: 1.035, actor loss: 26.623\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 14.481, mean reward: 7.290, reward rate: 0.503, rewarded fraction: 0.703, relative distance: 89.817, critic loss: 1.057, actor loss: 26.612\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 14.101, mean reward: 7.309, reward rate: 0.518, rewarded fraction: 0.708, relative distance: 95.562, critic loss: 1.066, actor loss: 26.581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 14.407, mean reward: 7.323, reward rate: 0.508, rewarded fraction: 0.711, relative distance: 92.609, critic loss: 1.036, actor loss: 26.551\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 15.026, mean reward: 7.619, reward rate: 0.507, rewarded fraction: 0.735, relative distance: 80.804, critic loss: 1.046, actor loss: 26.566\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 14.420, mean reward: 7.476, reward rate: 0.518, rewarded fraction: 0.728, relative distance: 90.137, critic loss: 1.029, actor loss: 26.555\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 14.448, mean reward: 7.385, reward rate: 0.511, rewarded fraction: 0.716, relative distance: 91.053, critic loss: 1.034, actor loss: 26.493\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 14.772, mean reward: 7.362, reward rate: 0.498, rewarded fraction: 0.712, relative distance: 91.076, critic loss: 1.048, actor loss: 26.503\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 14.872, mean reward: 7.930, reward rate: 0.533, rewarded fraction: 0.772, relative distance: 77.716, critic loss: 1.025, actor loss: 26.486\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 6.631, mean reward: 0.123, reward rate: 0.019, rewarded fraction: 0.007, relative distance: 275.173, critic loss: 0.017, actor loss: 0.033\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 12.221, mean reward: 0.363, reward rate: 0.030, rewarded fraction: 0.024, relative distance: 230.567, critic loss: 0.064, actor loss: 0.553\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.145, mean reward: 0.500, reward rate: 0.031, rewarded fraction: 0.038, relative distance: 252.496, critic loss: 0.102, actor loss: 1.039\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.450, mean reward: 0.404, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 254.896, critic loss: 0.121, actor loss: 0.987\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.860, mean reward: 0.482, reward rate: 0.030, rewarded fraction: 0.036, relative distance: 256.207, critic loss: 0.104, actor loss: 0.802\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 7.500, mean reward: 0.077, reward rate: 0.010, rewarded fraction: 0.003, relative distance: 262.631, critic loss: 0.137, actor loss: 1.294\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 17.019, mean reward: 0.350, reward rate: 0.021, rewarded fraction: 0.023, relative distance: 257.619, critic loss: 0.243, actor loss: 1.412\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.687, mean reward: 0.328, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 254.863, critic loss: 0.140, actor loss: 0.515\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 15.971, mean reward: 0.329, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 258.771, critic loss: 0.054, actor loss: 0.516\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 17.709, mean reward: 0.429, reward rate: 0.024, rewarded fraction: 0.033, relative distance: 260.461, critic loss: 0.044, actor loss: 0.415\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 16.564, mean reward: 0.403, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 254.400, critic loss: 0.041, actor loss: 0.334\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 15.948, mean reward: 0.479, reward rate: 0.030, rewarded fraction: 0.038, relative distance: 251.468, critic loss: 0.037, actor loss: 0.268\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.981, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 256.787, critic loss: 0.035, actor loss: 0.207\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 6.807, mean reward: 0.043, reward rate: 0.006, rewarded fraction: 0.001, relative distance: 276.701, critic loss: 0.128, actor loss: 0.282\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 7.576, mean reward: 0.108, reward rate: 0.014, rewarded fraction: 0.006, relative distance: 271.628, critic loss: 0.133, actor loss: 0.301\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 9.341, mean reward: 0.630, reward rate: 0.067, rewarded fraction: 0.048, relative distance: 210.605, critic loss: 0.196, actor loss: 0.390\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 7.335, mean reward: 0.611, reward rate: 0.083, rewarded fraction: 0.044, relative distance: 212.244, critic loss: 0.593, actor loss: 1.074\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 12.647, mean reward: 0.936, reward rate: 0.074, rewarded fraction: 0.075, relative distance: 213.658, critic loss: 0.599, actor loss: 1.981\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.267, mean reward: 1.460, reward rate: 0.090, rewarded fraction: 0.120, relative distance: 189.782, critic loss: 0.524, actor loss: 2.482\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.484, mean reward: 1.453, reward rate: 0.088, rewarded fraction: 0.124, relative distance: 200.801, critic loss: 0.216, actor loss: 2.320\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 15.970, mean reward: 1.249, reward rate: 0.078, rewarded fraction: 0.101, relative distance: 202.185, critic loss: 0.082, actor loss: 1.873\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 15.315, mean reward: 1.412, reward rate: 0.092, rewarded fraction: 0.121, relative distance: 196.213, critic loss: 0.072, actor loss: 1.478\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 15.962, mean reward: 1.184, reward rate: 0.074, rewarded fraction: 0.093, relative distance: 198.971, critic loss: 0.064, actor loss: 1.226\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 9.812, mean reward: 0.453, reward rate: 0.046, rewarded fraction: 0.032, relative distance: 230.019, critic loss: 0.212, actor loss: 1.238\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 7.007, mean reward: 0.593, reward rate: 0.085, rewarded fraction: 0.042, relative distance: 211.682, critic loss: 0.325, actor loss: 1.509\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 6.861, mean reward: 0.650, reward rate: 0.095, rewarded fraction: 0.048, relative distance: 217.810, critic loss: 0.548, actor loss: 1.859\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 7.176, mean reward: 0.830, reward rate: 0.116, rewarded fraction: 0.068, relative distance: 214.898, critic loss: 0.719, actor loss: 2.392\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 7.701, mean reward: 1.295, reward rate: 0.168, rewarded fraction: 0.115, relative distance: 209.880, critic loss: 0.737, actor loss: 2.947\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 8.121, mean reward: 1.652, reward rate: 0.203, rewarded fraction: 0.150, relative distance: 204.736, critic loss: 0.785, actor loss: 3.603\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 9.569, mean reward: 3.521, reward rate: 0.368, rewarded fraction: 0.337, relative distance: 171.213, critic loss: 0.901, actor loss: 4.432\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 10.567, mean reward: 4.374, reward rate: 0.414, rewarded fraction: 0.419, relative distance: 160.959, critic loss: 0.904, actor loss: 5.508\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 12.505, mean reward: 5.776, reward rate: 0.462, rewarded fraction: 0.556, relative distance: 128.450, critic loss: 0.931, actor loss: 6.887\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 14.100, mean reward: 6.734, reward rate: 0.478, rewarded fraction: 0.641, relative distance: 94.968, critic loss: 0.887, actor loss: 8.413\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 14.526, mean reward: 6.537, reward rate: 0.450, rewarded fraction: 0.620, relative distance: 97.335, critic loss: 0.803, actor loss: 9.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 14.143, mean reward: 6.511, reward rate: 0.460, rewarded fraction: 0.622, relative distance: 101.565, critic loss: 0.801, actor loss: 11.483\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 14.402, mean reward: 7.033, reward rate: 0.488, rewarded fraction: 0.677, relative distance: 90.045, critic loss: 0.829, actor loss: 12.817\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 15.226, mean reward: 7.189, reward rate: 0.472, rewarded fraction: 0.694, relative distance: 87.645, critic loss: 0.819, actor loss: 14.051\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 14.851, mean reward: 6.791, reward rate: 0.457, rewarded fraction: 0.647, relative distance: 94.905, critic loss: 0.811, actor loss: 15.194\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 14.218, mean reward: 6.554, reward rate: 0.461, rewarded fraction: 0.620, relative distance: 92.556, critic loss: 0.822, actor loss: 16.234\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 15.271, mean reward: 7.007, reward rate: 0.459, rewarded fraction: 0.674, relative distance: 88.048, critic loss: 0.820, actor loss: 17.153\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 14.535, mean reward: 6.733, reward rate: 0.463, rewarded fraction: 0.644, relative distance: 95.061, critic loss: 0.837, actor loss: 17.944\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 14.175, mean reward: 6.404, reward rate: 0.452, rewarded fraction: 0.607, relative distance: 99.608, critic loss: 0.864, actor loss: 18.631\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 14.997, mean reward: 7.037, reward rate: 0.469, rewarded fraction: 0.673, relative distance: 86.966, critic loss: 0.872, actor loss: 19.298\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 15.809, mean reward: 7.270, reward rate: 0.460, rewarded fraction: 0.688, relative distance: 75.174, critic loss: 0.829, actor loss: 19.887\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 15.722, mean reward: 7.523, reward rate: 0.479, rewarded fraction: 0.719, relative distance: 70.973, critic loss: 0.816, actor loss: 20.461\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 14.849, mean reward: 7.257, reward rate: 0.489, rewarded fraction: 0.694, relative distance: 81.320, critic loss: 0.864, actor loss: 20.966\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 15.737, mean reward: 7.531, reward rate: 0.479, rewarded fraction: 0.717, relative distance: 70.403, critic loss: 0.861, actor loss: 21.401\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 15.396, mean reward: 7.553, reward rate: 0.491, rewarded fraction: 0.731, relative distance: 78.646, critic loss: 0.867, actor loss: 21.815\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 15.086, mean reward: 7.252, reward rate: 0.481, rewarded fraction: 0.698, relative distance: 84.133, critic loss: 0.875, actor loss: 22.215\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 15.239, mean reward: 7.348, reward rate: 0.482, rewarded fraction: 0.706, relative distance: 81.899, critic loss: 0.878, actor loss: 22.554\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 15.684, mean reward: 7.377, reward rate: 0.470, rewarded fraction: 0.702, relative distance: 71.721, critic loss: 0.900, actor loss: 22.821\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 14.747, mean reward: 6.785, reward rate: 0.460, rewarded fraction: 0.643, relative distance: 88.271, critic loss: 0.917, actor loss: 23.068\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 14.487, mean reward: 6.810, reward rate: 0.470, rewarded fraction: 0.650, relative distance: 94.013, critic loss: 0.929, actor loss: 23.269\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 14.735, mean reward: 7.374, reward rate: 0.500, rewarded fraction: 0.714, relative distance: 83.781, critic loss: 0.959, actor loss: 23.454\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 15.105, mean reward: 7.260, reward rate: 0.481, rewarded fraction: 0.694, relative distance: 81.211, critic loss: 0.966, actor loss: 23.623\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 15.252, mean reward: 7.259, reward rate: 0.476, rewarded fraction: 0.690, relative distance: 76.908, critic loss: 0.950, actor loss: 23.827\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 15.591, mean reward: 7.690, reward rate: 0.493, rewarded fraction: 0.747, relative distance: 78.914, critic loss: 0.943, actor loss: 23.997\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 14.883, mean reward: 7.361, reward rate: 0.495, rewarded fraction: 0.711, relative distance: 85.275, critic loss: 0.922, actor loss: 24.093\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 15.602, mean reward: 7.764, reward rate: 0.498, rewarded fraction: 0.753, relative distance: 71.890, critic loss: 0.903, actor loss: 24.224\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 15.497, mean reward: 7.611, reward rate: 0.491, rewarded fraction: 0.733, relative distance: 76.401, critic loss: 0.926, actor loss: 24.277\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 15.249, mean reward: 7.076, reward rate: 0.464, rewarded fraction: 0.672, relative distance: 84.241, critic loss: 0.969, actor loss: 24.345\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 14.617, mean reward: 6.927, reward rate: 0.474, rewarded fraction: 0.662, relative distance: 88.856, critic loss: 0.940, actor loss: 24.406\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 15.167, mean reward: 7.571, reward rate: 0.499, rewarded fraction: 0.727, relative distance: 79.538, critic loss: 0.919, actor loss: 24.405\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 14.951, mean reward: 7.311, reward rate: 0.489, rewarded fraction: 0.702, relative distance: 81.162, critic loss: 0.934, actor loss: 24.478\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 15.125, mean reward: 7.389, reward rate: 0.489, rewarded fraction: 0.709, relative distance: 78.041, critic loss: 0.946, actor loss: 24.498\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 14.796, mean reward: 7.444, reward rate: 0.503, rewarded fraction: 0.717, relative distance: 83.292, critic loss: 0.916, actor loss: 24.520\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 14.558, mean reward: 7.319, reward rate: 0.503, rewarded fraction: 0.712, relative distance: 90.718, critic loss: 0.923, actor loss: 24.577\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 15.036, mean reward: 7.524, reward rate: 0.500, rewarded fraction: 0.731, relative distance: 82.984, critic loss: 0.923, actor loss: 24.634\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 15.328, mean reward: 7.255, reward rate: 0.473, rewarded fraction: 0.694, relative distance: 82.704, critic loss: 0.938, actor loss: 24.665\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 15.043, mean reward: 7.125, reward rate: 0.474, rewarded fraction: 0.677, relative distance: 85.297, critic loss: 0.936, actor loss: 24.649\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 14.537, mean reward: 7.474, reward rate: 0.514, rewarded fraction: 0.720, relative distance: 79.874, critic loss: 0.996, actor loss: 24.674\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 15.147, mean reward: 7.477, reward rate: 0.494, rewarded fraction: 0.723, relative distance: 83.870, critic loss: 0.958, actor loss: 24.668\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 15.304, mean reward: 7.495, reward rate: 0.490, rewarded fraction: 0.724, relative distance: 82.043, critic loss: 0.919, actor loss: 24.705\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.984, mean reward: 7.408, reward rate: 0.494, rewarded fraction: 0.715, relative distance: 80.575, critic loss: 0.975, actor loss: 24.752\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 15.508, mean reward: 7.533, reward rate: 0.486, rewarded fraction: 0.723, relative distance: 80.753, critic loss: 0.950, actor loss: 24.747\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 15.418, mean reward: 8.058, reward rate: 0.523, rewarded fraction: 0.783, relative distance: 68.770, critic loss: 0.918, actor loss: 24.793\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 14.534, mean reward: 7.381, reward rate: 0.508, rewarded fraction: 0.707, relative distance: 82.618, critic loss: 0.958, actor loss: 24.859\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 14.799, mean reward: 7.420, reward rate: 0.501, rewarded fraction: 0.718, relative distance: 85.095, critic loss: 0.946, actor loss: 24.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 15.217, mean reward: 7.399, reward rate: 0.486, rewarded fraction: 0.708, relative distance: 79.316, critic loss: 1.005, actor loss: 24.825\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 15.837, mean reward: 7.738, reward rate: 0.489, rewarded fraction: 0.750, relative distance: 75.149, critic loss: 0.959, actor loss: 24.875\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 14.857, mean reward: 7.461, reward rate: 0.502, rewarded fraction: 0.718, relative distance: 82.943, critic loss: 0.959, actor loss: 24.911\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 14.972, mean reward: 7.683, reward rate: 0.513, rewarded fraction: 0.739, relative distance: 76.062, critic loss: 0.974, actor loss: 24.926\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 14.836, mean reward: 7.718, reward rate: 0.520, rewarded fraction: 0.750, relative distance: 82.051, critic loss: 0.927, actor loss: 24.935\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 14.584, mean reward: 7.050, reward rate: 0.483, rewarded fraction: 0.677, relative distance: 90.208, critic loss: 0.939, actor loss: 24.996\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 15.167, mean reward: 7.633, reward rate: 0.503, rewarded fraction: 0.745, relative distance: 84.251, critic loss: 0.935, actor loss: 24.985\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 15.074, mean reward: 7.436, reward rate: 0.493, rewarded fraction: 0.710, relative distance: 79.050, critic loss: 0.981, actor loss: 24.978\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 16.109, mean reward: 7.763, reward rate: 0.482, rewarded fraction: 0.742, relative distance: 65.151, critic loss: 0.990, actor loss: 24.930\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 14.925, mean reward: 7.526, reward rate: 0.504, rewarded fraction: 0.725, relative distance: 80.850, critic loss: 0.970, actor loss: 24.929\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 14.945, mean reward: 7.588, reward rate: 0.508, rewarded fraction: 0.734, relative distance: 78.629, critic loss: 0.927, actor loss: 24.954\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 14.796, mean reward: 7.521, reward rate: 0.508, rewarded fraction: 0.733, relative distance: 84.268, critic loss: 0.953, actor loss: 24.936\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 14.868, mean reward: 7.216, reward rate: 0.485, rewarded fraction: 0.690, relative distance: 82.520, critic loss: 0.987, actor loss: 24.941\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 15.020, mean reward: 7.436, reward rate: 0.495, rewarded fraction: 0.714, relative distance: 81.531, critic loss: 1.035, actor loss: 24.881\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 15.386, mean reward: 7.639, reward rate: 0.497, rewarded fraction: 0.737, relative distance: 77.383, critic loss: 0.997, actor loss: 24.860\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 14.975, mean reward: 7.258, reward rate: 0.485, rewarded fraction: 0.691, relative distance: 82.570, critic loss: 1.010, actor loss: 24.847\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 15.127, mean reward: 7.659, reward rate: 0.506, rewarded fraction: 0.742, relative distance: 79.532, critic loss: 0.964, actor loss: 24.914\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 14.762, mean reward: 7.287, reward rate: 0.494, rewarded fraction: 0.704, relative distance: 89.259, critic loss: 0.984, actor loss: 24.903\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 356.096, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 5.333, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 297.217, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 4.750, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 348.501, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.883, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 298.123, critic loss: 0.000, actor loss: 0.032\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.436, mean reward: 0.115, reward rate: 0.007, rewarded fraction: 0.006, relative distance: 282.084, critic loss: 0.005, actor loss: 0.072\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 15.607, mean reward: 0.115, reward rate: 0.007, rewarded fraction: 0.006, relative distance: 279.102, critic loss: 0.016, actor loss: 0.070\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.384, mean reward: 0.087, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 274.293, critic loss: 0.023, actor loss: 0.060\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.412, mean reward: 0.123, reward rate: 0.008, rewarded fraction: 0.006, relative distance: 268.211, critic loss: 0.026, actor loss: 0.057\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 17.200, mean reward: 0.078, reward rate: 0.005, rewarded fraction: 0.002, relative distance: 270.514, critic loss: 0.027, actor loss: 0.062\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 15.293, mean reward: 0.111, reward rate: 0.007, rewarded fraction: 0.005, relative distance: 266.265, critic loss: 0.029, actor loss: 0.056\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.194, mean reward: 0.071, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 270.683, critic loss: 0.029, actor loss: 0.046\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.206, mean reward: 0.091, reward rate: 0.006, rewarded fraction: 0.004, relative distance: 272.460, critic loss: 0.031, actor loss: 0.037\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.492, mean reward: 0.065, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 267.740, critic loss: 0.029, actor loss: 0.023\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 15.025, mean reward: 0.079, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 269.988, critic loss: 0.029, actor loss: 0.012\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.795, mean reward: 0.161, reward rate: 0.010, rewarded fraction: 0.011, relative distance: 271.542, critic loss: 0.036, actor loss: -0.002\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.510, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 275.057, critic loss: 0.035, actor loss: -0.015\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 17.772, mean reward: 0.186, reward rate: 0.010, rewarded fraction: 0.011, relative distance: 261.561, critic loss: 0.032, actor loss: -0.023\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 17.020, mean reward: 0.103, reward rate: 0.006, rewarded fraction: 0.004, relative distance: 262.538, critic loss: 0.036, actor loss: -0.038\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.544, mean reward: 0.082, reward rate: 0.005, rewarded fraction: 0.002, relative distance: 261.510, critic loss: 0.037, actor loss: -0.057\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.586, mean reward: 0.171, reward rate: 0.010, rewarded fraction: 0.010, relative distance: 251.595, critic loss: 0.039, actor loss: -0.074\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 15.022, mean reward: 0.296, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 248.532, critic loss: 0.036, actor loss: -0.093\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 17.139, mean reward: 0.133, reward rate: 0.008, rewarded fraction: 0.006, relative distance: 259.888, critic loss: 0.036, actor loss: -0.105\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.302, mean reward: 0.153, reward rate: 0.009, rewarded fraction: 0.010, relative distance: 264.315, critic loss: 0.035, actor loss: -0.110\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.098, mean reward: 0.132, reward rate: 0.008, rewarded fraction: 0.008, relative distance: 266.905, critic loss: 0.033, actor loss: -0.106\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 15.919, mean reward: 0.168, reward rate: 0.011, rewarded fraction: 0.012, relative distance: 270.468, critic loss: 0.035, actor loss: -0.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.855, mean reward: 0.159, reward rate: 0.009, rewarded fraction: 0.010, relative distance: 252.719, critic loss: 0.033, actor loss: -0.106\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.442, mean reward: 0.086, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 268.955, critic loss: 0.028, actor loss: -0.098\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.683, mean reward: 0.102, reward rate: 0.006, rewarded fraction: 0.006, relative distance: 282.019, critic loss: 0.028, actor loss: -0.091\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.289, mean reward: 0.215, reward rate: 0.013, rewarded fraction: 0.016, relative distance: 278.388, critic loss: 0.024, actor loss: -0.086\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 15.807, mean reward: 0.179, reward rate: 0.011, rewarded fraction: 0.011, relative distance: 280.370, critic loss: 0.027, actor loss: -0.085\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.154, mean reward: 0.213, reward rate: 0.013, rewarded fraction: 0.012, relative distance: 273.562, critic loss: 0.026, actor loss: -0.084\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 16.359, mean reward: 0.223, reward rate: 0.014, rewarded fraction: 0.016, relative distance: 276.331, critic loss: 0.026, actor loss: -0.075\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 15.961, mean reward: 0.171, reward rate: 0.011, rewarded fraction: 0.012, relative distance: 282.874, critic loss: 0.025, actor loss: -0.072\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 15.584, mean reward: 0.233, reward rate: 0.015, rewarded fraction: 0.017, relative distance: 282.671, critic loss: 0.023, actor loss: -0.081\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.757, mean reward: 0.249, reward rate: 0.015, rewarded fraction: 0.019, relative distance: 285.620, critic loss: 0.022, actor loss: -0.085\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 15.803, mean reward: 0.085, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 290.129, critic loss: 0.020, actor loss: -0.091\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 15.723, mean reward: 0.078, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 288.820, critic loss: 0.019, actor loss: -0.091\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.203, mean reward: 0.130, reward rate: 0.008, rewarded fraction: 0.006, relative distance: 290.445, critic loss: 0.019, actor loss: -0.093\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 16.594, mean reward: 0.120, reward rate: 0.007, rewarded fraction: 0.008, relative distance: 291.909, critic loss: 0.019, actor loss: -0.097\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 15.902, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 299.475, critic loss: 0.019, actor loss: -0.098\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 16.882, mean reward: 0.058, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 292.200, critic loss: 0.017, actor loss: -0.094\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 16.722, mean reward: 0.062, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 299.857, critic loss: 0.016, actor loss: -0.092\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 16.046, mean reward: 0.053, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 297.025, critic loss: 0.017, actor loss: -0.091\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 16.109, mean reward: 0.067, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 297.624, critic loss: 0.015, actor loss: -0.088\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 16.132, mean reward: 0.116, reward rate: 0.007, rewarded fraction: 0.008, relative distance: 275.714, critic loss: 0.017, actor loss: -0.084\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 16.261, mean reward: 0.186, reward rate: 0.011, rewarded fraction: 0.012, relative distance: 266.802, critic loss: 0.017, actor loss: -0.079\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 15.559, mean reward: 0.315, reward rate: 0.020, rewarded fraction: 0.023, relative distance: 260.542, critic loss: 0.018, actor loss: -0.078\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.243, mean reward: 0.055, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 270.434, critic loss: 0.019, actor loss: -0.079\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 15.849, mean reward: 0.126, reward rate: 0.008, rewarded fraction: 0.006, relative distance: 269.020, critic loss: 0.021, actor loss: -0.084\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 16.532, mean reward: 0.165, reward rate: 0.010, rewarded fraction: 0.010, relative distance: 265.017, critic loss: 0.019, actor loss: -0.086\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 16.386, mean reward: 0.153, reward rate: 0.009, rewarded fraction: 0.010, relative distance: 268.751, critic loss: 0.021, actor loss: -0.089\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 16.606, mean reward: 0.125, reward rate: 0.008, rewarded fraction: 0.006, relative distance: 263.246, critic loss: 0.016, actor loss: -0.088\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 15.650, mean reward: 0.106, reward rate: 0.007, rewarded fraction: 0.004, relative distance: 275.888, critic loss: 0.015, actor loss: -0.085\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 16.671, mean reward: 0.114, reward rate: 0.007, rewarded fraction: 0.004, relative distance: 273.037, critic loss: 0.017, actor loss: -0.081\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 15.819, mean reward: 0.158, reward rate: 0.010, rewarded fraction: 0.006, relative distance: 270.855, critic loss: 0.014, actor loss: -0.080\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 8.180, mean reward: 0.067, reward rate: 0.008, rewarded fraction: 0.002, relative distance: 277.904, critic loss: 0.056, actor loss: -0.022\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 10.267, mean reward: 0.901, reward rate: 0.088, rewarded fraction: 0.072, relative distance: 201.297, critic loss: 0.084, actor loss: 0.161\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 7.790, mean reward: 0.749, reward rate: 0.096, rewarded fraction: 0.059, relative distance: 210.722, critic loss: 0.324, actor loss: 0.927\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 8.263, mean reward: 1.086, reward rate: 0.131, rewarded fraction: 0.091, relative distance: 204.484, critic loss: 0.597, actor loss: 1.567\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 7.527, mean reward: 0.993, reward rate: 0.132, rewarded fraction: 0.084, relative distance: 209.952, critic loss: 0.552, actor loss: 2.093\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 7.218, mean reward: 0.862, reward rate: 0.119, rewarded fraction: 0.073, relative distance: 214.775, critic loss: 0.428, actor loss: 2.556\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 6.822, mean reward: 0.829, reward rate: 0.122, rewarded fraction: 0.067, relative distance: 219.971, critic loss: 0.485, actor loss: 3.090\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 6.895, mean reward: 0.935, reward rate: 0.136, rewarded fraction: 0.078, relative distance: 219.331, critic loss: 0.551, actor loss: 3.689\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 6.966, mean reward: 1.040, reward rate: 0.149, rewarded fraction: 0.090, relative distance: 218.293, critic loss: 0.604, actor loss: 4.350\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 6.976, mean reward: 1.149, reward rate: 0.165, rewarded fraction: 0.101, relative distance: 218.771, critic loss: 0.709, actor loss: 5.079\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 7.111, mean reward: 1.227, reward rate: 0.173, rewarded fraction: 0.108, relative distance: 215.893, critic loss: 0.761, actor loss: 5.807\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 7.095, mean reward: 1.351, reward rate: 0.190, rewarded fraction: 0.120, relative distance: 214.991, critic loss: 0.821, actor loss: 6.532\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 7.062, mean reward: 1.231, reward rate: 0.174, rewarded fraction: 0.111, relative distance: 217.178, critic loss: 0.934, actor loss: 7.316\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 7.221, mean reward: 1.354, reward rate: 0.187, rewarded fraction: 0.121, relative distance: 214.928, critic loss: 0.924, actor loss: 7.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 7.265, mean reward: 1.363, reward rate: 0.188, rewarded fraction: 0.120, relative distance: 214.310, critic loss: 0.943, actor loss: 8.568\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 7.146, mean reward: 1.374, reward rate: 0.192, rewarded fraction: 0.124, relative distance: 215.714, critic loss: 0.965, actor loss: 9.123\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 7.219, mean reward: 1.471, reward rate: 0.204, rewarded fraction: 0.133, relative distance: 214.716, critic loss: 1.088, actor loss: 9.682\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 8.843, mean reward: 2.662, reward rate: 0.301, rewarded fraction: 0.247, relative distance: 181.638, critic loss: 1.123, actor loss: 10.235\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 8.811, mean reward: 2.793, reward rate: 0.317, rewarded fraction: 0.265, relative distance: 183.881, critic loss: 1.144, actor loss: 10.723\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 9.191, mean reward: 3.051, reward rate: 0.332, rewarded fraction: 0.286, relative distance: 177.738, critic loss: 1.161, actor loss: 11.178\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 8.879, mean reward: 3.277, reward rate: 0.369, rewarded fraction: 0.316, relative distance: 178.834, critic loss: 1.204, actor loss: 11.599\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 8.204, mean reward: 3.009, reward rate: 0.367, rewarded fraction: 0.288, relative distance: 192.605, critic loss: 1.179, actor loss: 12.122\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 8.488, mean reward: 3.270, reward rate: 0.385, rewarded fraction: 0.314, relative distance: 187.299, critic loss: 1.147, actor loss: 12.723\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 8.701, mean reward: 3.129, reward rate: 0.360, rewarded fraction: 0.294, relative distance: 186.092, critic loss: 1.181, actor loss: 13.350\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 9.062, mean reward: 3.515, reward rate: 0.388, rewarded fraction: 0.333, relative distance: 178.934, critic loss: 1.194, actor loss: 14.012\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 9.096, mean reward: 3.802, reward rate: 0.418, rewarded fraction: 0.366, relative distance: 176.115, critic loss: 1.180, actor loss: 14.643\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 9.346, mean reward: 3.836, reward rate: 0.410, rewarded fraction: 0.364, relative distance: 171.924, critic loss: 1.170, actor loss: 15.275\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 9.573, mean reward: 3.949, reward rate: 0.413, rewarded fraction: 0.372, relative distance: 167.976, critic loss: 1.178, actor loss: 15.914\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 9.772, mean reward: 4.240, reward rate: 0.434, rewarded fraction: 0.404, relative distance: 162.038, critic loss: 1.182, actor loss: 16.573\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 10.138, mean reward: 4.389, reward rate: 0.433, rewarded fraction: 0.414, relative distance: 155.559, critic loss: 1.148, actor loss: 17.199\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 10.107, mean reward: 4.345, reward rate: 0.430, rewarded fraction: 0.411, relative distance: 158.897, critic loss: 1.115, actor loss: 17.814\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 10.519, mean reward: 4.430, reward rate: 0.421, rewarded fraction: 0.420, relative distance: 156.906, critic loss: 1.118, actor loss: 18.362\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 10.616, mean reward: 4.284, reward rate: 0.404, rewarded fraction: 0.402, relative distance: 158.796, critic loss: 1.128, actor loss: 18.882\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 10.937, mean reward: 5.118, reward rate: 0.468, rewarded fraction: 0.489, relative distance: 142.957, critic loss: 1.127, actor loss: 19.387\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 11.167, mean reward: 5.049, reward rate: 0.452, rewarded fraction: 0.485, relative distance: 146.671, critic loss: 1.103, actor loss: 19.836\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 11.653, mean reward: 5.459, reward rate: 0.468, rewarded fraction: 0.522, relative distance: 133.790, critic loss: 1.083, actor loss: 20.271\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 11.915, mean reward: 5.468, reward rate: 0.459, rewarded fraction: 0.521, relative distance: 133.157, critic loss: 1.075, actor loss: 20.676\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.370, mean reward: 5.629, reward rate: 0.455, rewarded fraction: 0.537, relative distance: 129.488, critic loss: 1.056, actor loss: 21.059\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.362, mean reward: 5.662, reward rate: 0.458, rewarded fraction: 0.543, relative distance: 128.813, critic loss: 1.052, actor loss: 21.413\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.105, mean reward: 5.980, reward rate: 0.456, rewarded fraction: 0.569, relative distance: 115.352, critic loss: 1.042, actor loss: 21.689\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 13.200, mean reward: 6.172, reward rate: 0.468, rewarded fraction: 0.589, relative distance: 108.888, critic loss: 0.962, actor loss: 21.896\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.338, mean reward: 6.153, reward rate: 0.461, rewarded fraction: 0.588, relative distance: 111.510, critic loss: 0.988, actor loss: 22.133\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 13.195, mean reward: 6.069, reward rate: 0.460, rewarded fraction: 0.577, relative distance: 114.186, critic loss: 0.992, actor loss: 22.345\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.767, mean reward: 5.991, reward rate: 0.469, rewarded fraction: 0.570, relative distance: 120.819, critic loss: 1.015, actor loss: 22.568\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.517, mean reward: 0.084, reward rate: 0.005, rewarded fraction: 0.006, relative distance: 349.580, critic loss: 0.009, actor loss: 0.101\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.583, mean reward: 0.086, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 297.993, critic loss: 0.015, actor loss: 0.112\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 18.368, mean reward: 0.179, reward rate: 0.010, rewarded fraction: 0.011, relative distance: 272.809, critic loss: 0.020, actor loss: 0.109\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.408, mean reward: 0.270, reward rate: 0.016, rewarded fraction: 0.018, relative distance: 253.739, critic loss: 0.033, actor loss: 0.165\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.551, mean reward: 0.375, reward rate: 0.023, rewarded fraction: 0.028, relative distance: 259.250, critic loss: 0.064, actor loss: 0.183\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.069, mean reward: 0.332, reward rate: 0.021, rewarded fraction: 0.026, relative distance: 263.773, critic loss: 0.077, actor loss: 0.124\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.135, mean reward: 0.409, reward rate: 0.025, rewarded fraction: 0.032, relative distance: 260.210, critic loss: 0.084, actor loss: 0.008\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.754, mean reward: 0.401, reward rate: 0.024, rewarded fraction: 0.033, relative distance: 264.413, critic loss: 0.082, actor loss: -0.068\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.238, mean reward: 0.429, reward rate: 0.026, rewarded fraction: 0.030, relative distance: 255.137, critic loss: 0.062, actor loss: -0.097\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.779, mean reward: 0.340, reward rate: 0.020, rewarded fraction: 0.023, relative distance: 262.829, critic loss: 0.054, actor loss: -0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 18.026, mean reward: 0.161, reward rate: 0.009, rewarded fraction: 0.011, relative distance: 276.653, critic loss: 0.047, actor loss: -0.104\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 17.006, mean reward: 0.085, reward rate: 0.005, rewarded fraction: 0.006, relative distance: 314.762, critic loss: 0.036, actor loss: -0.126\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.650, mean reward: 0.061, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 357.922, critic loss: 0.034, actor loss: -0.132\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 16.442, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 351.288, critic loss: 0.028, actor loss: -0.131\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.874, mean reward: 0.056, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 345.982, critic loss: 0.027, actor loss: -0.137\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 15.955, mean reward: 0.057, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 323.268, critic loss: 0.025, actor loss: -0.136\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.209, mean reward: 0.104, reward rate: 0.006, rewarded fraction: 0.006, relative distance: 300.278, critic loss: 0.026, actor loss: -0.123\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.339, mean reward: 0.097, reward rate: 0.006, rewarded fraction: 0.006, relative distance: 298.315, critic loss: 0.025, actor loss: -0.121\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.040, mean reward: 0.090, reward rate: 0.006, rewarded fraction: 0.004, relative distance: 300.248, critic loss: 0.026, actor loss: -0.116\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.900, mean reward: 0.060, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 293.226, critic loss: 0.024, actor loss: -0.112\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.024, mean reward: 0.065, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 304.603, critic loss: 0.025, actor loss: -0.116\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 15.408, mean reward: 0.088, reward rate: 0.006, rewarded fraction: 0.006, relative distance: 299.820, critic loss: 0.025, actor loss: -0.119\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.230, mean reward: 0.110, reward rate: 0.007, rewarded fraction: 0.008, relative distance: 296.969, critic loss: 0.027, actor loss: -0.120\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.758, mean reward: 0.078, reward rate: 0.005, rewarded fraction: 0.004, relative distance: 290.640, critic loss: 0.026, actor loss: -0.125\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 15.889, mean reward: 0.107, reward rate: 0.007, rewarded fraction: 0.006, relative distance: 291.888, critic loss: 0.030, actor loss: -0.129\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.592, mean reward: 0.057, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 299.276, critic loss: 0.029, actor loss: -0.128\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 15.522, mean reward: 0.105, reward rate: 0.007, rewarded fraction: 0.007, relative distance: 295.044, critic loss: 0.028, actor loss: -0.127\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 10.991, mean reward: 0.040, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 297.437, critic loss: 0.069, actor loss: -0.074\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 10.816, mean reward: 0.817, reward rate: 0.076, rewarded fraction: 0.064, relative distance: 204.392, critic loss: 0.093, actor loss: 0.029\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 6.560, mean reward: 0.516, reward rate: 0.079, rewarded fraction: 0.038, relative distance: 222.066, critic loss: 0.279, actor loss: 0.479\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 7.175, mean reward: 0.649, reward rate: 0.090, rewarded fraction: 0.050, relative distance: 215.247, critic loss: 0.388, actor loss: 1.088\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 6.969, mean reward: 0.645, reward rate: 0.093, rewarded fraction: 0.050, relative distance: 216.155, critic loss: 0.583, actor loss: 1.902\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 7.254, mean reward: 0.760, reward rate: 0.105, rewarded fraction: 0.062, relative distance: 214.662, critic loss: 0.670, actor loss: 2.688\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 7.526, mean reward: 1.008, reward rate: 0.134, rewarded fraction: 0.084, relative distance: 208.810, critic loss: 0.892, actor loss: 3.607\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 6.959, mean reward: 0.971, reward rate: 0.139, rewarded fraction: 0.083, relative distance: 215.857, critic loss: 1.124, actor loss: 4.588\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 6.716, mean reward: 1.265, reward rate: 0.188, rewarded fraction: 0.114, relative distance: 218.866, critic loss: 1.106, actor loss: 5.477\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 6.558, mean reward: 1.081, reward rate: 0.165, rewarded fraction: 0.093, relative distance: 223.250, critic loss: 1.020, actor loss: 6.418\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 6.670, mean reward: 1.312, reward rate: 0.197, rewarded fraction: 0.119, relative distance: 222.619, critic loss: 0.915, actor loss: 7.404\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 6.538, mean reward: 1.219, reward rate: 0.186, rewarded fraction: 0.105, relative distance: 223.570, critic loss: 0.917, actor loss: 8.238\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 6.811, mean reward: 1.431, reward rate: 0.210, rewarded fraction: 0.129, relative distance: 220.476, critic loss: 0.943, actor loss: 9.052\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 8.308, mean reward: 2.675, reward rate: 0.322, rewarded fraction: 0.243, relative distance: 188.157, critic loss: 0.981, actor loss: 9.832\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 8.194, mean reward: 2.767, reward rate: 0.338, rewarded fraction: 0.257, relative distance: 188.279, critic loss: 1.001, actor loss: 10.592\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 8.391, mean reward: 2.953, reward rate: 0.352, rewarded fraction: 0.273, relative distance: 183.202, critic loss: 1.070, actor loss: 11.303\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 8.376, mean reward: 2.978, reward rate: 0.356, rewarded fraction: 0.277, relative distance: 185.896, critic loss: 1.137, actor loss: 12.030\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 8.721, mean reward: 3.316, reward rate: 0.380, rewarded fraction: 0.312, relative distance: 179.747, critic loss: 1.169, actor loss: 12.776\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 8.948, mean reward: 3.489, reward rate: 0.390, rewarded fraction: 0.326, relative distance: 174.782, critic loss: 1.177, actor loss: 13.522\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 9.275, mean reward: 3.703, reward rate: 0.399, rewarded fraction: 0.350, relative distance: 173.516, critic loss: 1.173, actor loss: 14.260\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 9.638, mean reward: 4.126, reward rate: 0.428, rewarded fraction: 0.395, relative distance: 167.945, critic loss: 1.178, actor loss: 15.028\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 10.549, mean reward: 4.645, reward rate: 0.440, rewarded fraction: 0.444, relative distance: 156.534, critic loss: 1.168, actor loss: 15.793\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 10.785, mean reward: 4.772, reward rate: 0.443, rewarded fraction: 0.454, relative distance: 152.154, critic loss: 1.138, actor loss: 16.627\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 11.159, mean reward: 5.015, reward rate: 0.449, rewarded fraction: 0.480, relative distance: 147.523, critic loss: 1.099, actor loss: 17.418\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 11.574, mean reward: 5.195, reward rate: 0.449, rewarded fraction: 0.493, relative distance: 140.150, critic loss: 1.069, actor loss: 18.125\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 11.849, mean reward: 5.298, reward rate: 0.447, rewarded fraction: 0.506, relative distance: 136.485, critic loss: 1.051, actor loss: 18.809\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 11.947, mean reward: 5.658, reward rate: 0.474, rewarded fraction: 0.549, relative distance: 133.571, critic loss: 1.048, actor loss: 19.394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.302, mean reward: 5.696, reward rate: 0.463, rewarded fraction: 0.546, relative distance: 128.624, critic loss: 1.042, actor loss: 19.906\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.847, mean reward: 5.981, reward rate: 0.466, rewarded fraction: 0.575, relative distance: 121.896, critic loss: 1.023, actor loss: 20.366\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.840, mean reward: 6.212, reward rate: 0.484, rewarded fraction: 0.601, relative distance: 120.618, critic loss: 1.007, actor loss: 20.804\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.819, mean reward: 5.965, reward rate: 0.465, rewarded fraction: 0.572, relative distance: 123.032, critic loss: 1.028, actor loss: 21.250\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 12.967, mean reward: 5.959, reward rate: 0.460, rewarded fraction: 0.566, relative distance: 117.310, critic loss: 1.014, actor loss: 21.594\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.318, mean reward: 6.220, reward rate: 0.467, rewarded fraction: 0.597, relative distance: 114.862, critic loss: 1.003, actor loss: 21.929\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 13.543, mean reward: 6.431, reward rate: 0.475, rewarded fraction: 0.623, relative distance: 111.102, critic loss: 0.988, actor loss: 22.253\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 13.923, mean reward: 6.663, reward rate: 0.479, rewarded fraction: 0.642, relative distance: 102.230, critic loss: 0.991, actor loss: 22.482\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 14.087, mean reward: 6.894, reward rate: 0.489, rewarded fraction: 0.663, relative distance: 96.376, critic loss: 1.001, actor loss: 22.730\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 13.632, mean reward: 6.682, reward rate: 0.490, rewarded fraction: 0.645, relative distance: 105.372, critic loss: 1.015, actor loss: 22.979\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 13.807, mean reward: 6.579, reward rate: 0.477, rewarded fraction: 0.631, relative distance: 103.869, critic loss: 1.014, actor loss: 23.183\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 14.521, mean reward: 7.167, reward rate: 0.494, rewarded fraction: 0.693, relative distance: 92.929, critic loss: 1.008, actor loss: 23.362\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 13.797, mean reward: 6.891, reward rate: 0.499, rewarded fraction: 0.666, relative distance: 101.654, critic loss: 1.007, actor loss: 23.539\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 13.911, mean reward: 6.853, reward rate: 0.493, rewarded fraction: 0.656, relative distance: 101.115, critic loss: 1.027, actor loss: 23.720\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 14.078, mean reward: 7.006, reward rate: 0.498, rewarded fraction: 0.677, relative distance: 98.187, critic loss: 1.014, actor loss: 23.831\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 14.018, mean reward: 6.955, reward rate: 0.496, rewarded fraction: 0.674, relative distance: 99.041, critic loss: 0.995, actor loss: 23.959\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 14.740, mean reward: 7.209, reward rate: 0.489, rewarded fraction: 0.695, relative distance: 90.952, critic loss: 1.000, actor loss: 24.064\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 14.125, mean reward: 6.882, reward rate: 0.487, rewarded fraction: 0.659, relative distance: 91.951, critic loss: 0.992, actor loss: 24.160\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 14.778, mean reward: 7.494, reward rate: 0.507, rewarded fraction: 0.726, relative distance: 84.082, critic loss: 0.996, actor loss: 24.303\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.042, mean reward: 6.906, reward rate: 0.492, rewarded fraction: 0.669, relative distance: 101.062, critic loss: 0.978, actor loss: 24.375\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 14.723, mean reward: 7.142, reward rate: 0.485, rewarded fraction: 0.685, relative distance: 90.507, critic loss: 0.985, actor loss: 24.442\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 13.983, mean reward: 6.727, reward rate: 0.481, rewarded fraction: 0.646, relative distance: 102.770, critic loss: 1.020, actor loss: 24.499\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 13.419, mean reward: 6.721, reward rate: 0.501, rewarded fraction: 0.644, relative distance: 104.286, critic loss: 1.029, actor loss: 24.527\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 13.856, mean reward: 6.749, reward rate: 0.487, rewarded fraction: 0.648, relative distance: 104.316, critic loss: 1.006, actor loss: 24.531\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 13.657, mean reward: 6.528, reward rate: 0.478, rewarded fraction: 0.626, relative distance: 109.234, critic loss: 1.035, actor loss: 24.590\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 14.363, mean reward: 7.077, reward rate: 0.493, rewarded fraction: 0.686, relative distance: 98.264, critic loss: 1.013, actor loss: 24.559\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 14.084, mean reward: 6.873, reward rate: 0.488, rewarded fraction: 0.660, relative distance: 99.094, critic loss: 0.991, actor loss: 24.550\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 14.633, mean reward: 7.285, reward rate: 0.498, rewarded fraction: 0.706, relative distance: 92.499, critic loss: 0.964, actor loss: 24.566\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 14.095, mean reward: 6.975, reward rate: 0.495, rewarded fraction: 0.671, relative distance: 98.665, critic loss: 0.964, actor loss: 24.621\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 14.331, mean reward: 7.152, reward rate: 0.499, rewarded fraction: 0.687, relative distance: 93.140, critic loss: 0.992, actor loss: 24.655\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 13.453, mean reward: 6.790, reward rate: 0.505, rewarded fraction: 0.657, relative distance: 109.190, critic loss: 0.998, actor loss: 24.719\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 14.028, mean reward: 7.038, reward rate: 0.502, rewarded fraction: 0.678, relative distance: 99.232, critic loss: 0.986, actor loss: 24.709\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 14.087, mean reward: 7.105, reward rate: 0.504, rewarded fraction: 0.683, relative distance: 95.196, critic loss: 0.981, actor loss: 24.690\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.948, mean reward: 6.800, reward rate: 0.525, rewarded fraction: 0.661, relative distance: 112.075, critic loss: 0.968, actor loss: 24.724\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 14.186, mean reward: 6.677, reward rate: 0.471, rewarded fraction: 0.635, relative distance: 103.231, critic loss: 0.979, actor loss: 24.726\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 13.708, mean reward: 7.198, reward rate: 0.525, rewarded fraction: 0.699, relative distance: 96.635, critic loss: 0.955, actor loss: 24.785\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.287, mean reward: 6.899, reward rate: 0.519, rewarded fraction: 0.663, relative distance: 102.610, critic loss: 0.946, actor loss: 24.774\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.259, mean reward: 6.559, reward rate: 0.495, rewarded fraction: 0.634, relative distance: 115.060, critic loss: 0.960, actor loss: 24.816\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 13.985, mean reward: 7.152, reward rate: 0.511, rewarded fraction: 0.690, relative distance: 97.203, critic loss: 0.972, actor loss: 24.829\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 14.188, mean reward: 7.286, reward rate: 0.514, rewarded fraction: 0.704, relative distance: 92.061, critic loss: 0.950, actor loss: 24.809\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 13.604, mean reward: 6.396, reward rate: 0.470, rewarded fraction: 0.609, relative distance: 111.903, critic loss: 0.959, actor loss: 24.865\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 14.144, mean reward: 7.249, reward rate: 0.513, rewarded fraction: 0.702, relative distance: 92.803, critic loss: 0.967, actor loss: 24.895\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 10.668, mean reward: 0.022, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 313.758, critic loss: 0.000, actor loss: 0.003\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 6.428, mean reward: 0.026, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 294.204, critic loss: 0.001, actor loss: 0.004\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 6.887, mean reward: 0.188, reward rate: 0.027, rewarded fraction: 0.011, relative distance: 246.271, critic loss: 0.019, actor loss: 0.121\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 11.478, mean reward: 0.489, reward rate: 0.043, rewarded fraction: 0.036, relative distance: 229.189, critic loss: 0.083, actor loss: 0.626\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.552, mean reward: 0.414, reward rate: 0.027, rewarded fraction: 0.034, relative distance: 257.173, critic loss: 0.123, actor loss: 1.220\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.744, mean reward: 0.278, reward rate: 0.017, rewarded fraction: 0.016, relative distance: 258.121, critic loss: 0.126, actor loss: 1.336\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.952, mean reward: 0.451, reward rate: 0.027, rewarded fraction: 0.036, relative distance: 256.745, critic loss: 0.099, actor loss: 1.183\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.927, mean reward: 0.434, reward rate: 0.026, rewarded fraction: 0.034, relative distance: 255.505, critic loss: 0.076, actor loss: 0.974\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.552, mean reward: 0.283, reward rate: 0.017, rewarded fraction: 0.020, relative distance: 264.636, critic loss: 0.063, actor loss: 0.810\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 17.788, mean reward: 0.255, reward rate: 0.014, rewarded fraction: 0.015, relative distance: 259.238, critic loss: 0.050, actor loss: 0.686\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 17.100, mean reward: 0.362, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 254.068, critic loss: 0.038, actor loss: 0.556\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 17.790, mean reward: 0.456, reward rate: 0.026, rewarded fraction: 0.036, relative distance: 258.617, critic loss: 0.033, actor loss: 0.432\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 18.007, mean reward: 0.272, reward rate: 0.015, rewarded fraction: 0.018, relative distance: 269.214, critic loss: 0.031, actor loss: 0.333\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 15.193, mean reward: 0.308, reward rate: 0.020, rewarded fraction: 0.019, relative distance: 254.938, critic loss: 0.028, actor loss: 0.266\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.595, mean reward: 0.372, reward rate: 0.022, rewarded fraction: 0.028, relative distance: 258.056, critic loss: 0.024, actor loss: 0.213\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 17.601, mean reward: 0.414, reward rate: 0.024, rewarded fraction: 0.030, relative distance: 258.601, critic loss: 0.026, actor loss: 0.166\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.254, mean reward: 0.312, reward rate: 0.019, rewarded fraction: 0.023, relative distance: 262.751, critic loss: 0.023, actor loss: 0.126\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 18.371, mean reward: 0.284, reward rate: 0.015, rewarded fraction: 0.017, relative distance: 270.647, critic loss: 0.022, actor loss: 0.089\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 17.256, mean reward: 0.308, reward rate: 0.018, rewarded fraction: 0.021, relative distance: 259.535, critic loss: 0.020, actor loss: 0.064\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.650, mean reward: 0.281, reward rate: 0.017, rewarded fraction: 0.016, relative distance: 257.933, critic loss: 0.020, actor loss: 0.046\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.296, mean reward: 0.372, reward rate: 0.023, rewarded fraction: 0.026, relative distance: 259.722, critic loss: 0.020, actor loss: 0.028\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.027, mean reward: 0.311, reward rate: 0.019, rewarded fraction: 0.019, relative distance: 259.615, critic loss: 0.020, actor loss: 0.009\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.065, mean reward: 0.415, reward rate: 0.026, rewarded fraction: 0.035, relative distance: 257.913, critic loss: 0.020, actor loss: -0.005\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.741, mean reward: 0.350, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 261.477, critic loss: 0.019, actor loss: -0.017\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.912, mean reward: 0.418, reward rate: 0.025, rewarded fraction: 0.033, relative distance: 249.116, critic loss: 0.019, actor loss: -0.028\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 17.102, mean reward: 0.269, reward rate: 0.016, rewarded fraction: 0.015, relative distance: 250.015, critic loss: 0.021, actor loss: -0.034\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 7.014, mean reward: 0.109, reward rate: 0.016, rewarded fraction: 0.005, relative distance: 251.825, critic loss: 0.189, actor loss: 0.187\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 6.277, mean reward: 0.313, reward rate: 0.050, rewarded fraction: 0.019, relative distance: 231.127, critic loss: 0.221, actor loss: 0.346\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 5.823, mean reward: 0.335, reward rate: 0.058, rewarded fraction: 0.019, relative distance: 233.085, critic loss: 0.308, actor loss: 0.514\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 6.705, mean reward: 0.502, reward rate: 0.075, rewarded fraction: 0.038, relative distance: 228.623, critic loss: 0.320, actor loss: 0.736\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 6.499, mean reward: 0.437, reward rate: 0.067, rewarded fraction: 0.030, relative distance: 229.042, critic loss: 0.366, actor loss: 0.976\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 7.308, mean reward: 0.906, reward rate: 0.124, rewarded fraction: 0.073, relative distance: 222.503, critic loss: 0.398, actor loss: 1.290\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 7.696, mean reward: 1.529, reward rate: 0.199, rewarded fraction: 0.136, relative distance: 213.535, critic loss: 0.564, actor loss: 1.900\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 7.548, mean reward: 1.604, reward rate: 0.212, rewarded fraction: 0.143, relative distance: 213.667, critic loss: 0.791, actor loss: 2.899\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 9.876, mean reward: 3.484, reward rate: 0.353, rewarded fraction: 0.320, relative distance: 168.461, critic loss: 0.888, actor loss: 4.094\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 10.598, mean reward: 4.295, reward rate: 0.405, rewarded fraction: 0.395, relative distance: 146.504, critic loss: 0.876, actor loss: 5.377\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 11.706, mean reward: 5.025, reward rate: 0.429, rewarded fraction: 0.472, relative distance: 133.261, critic loss: 0.864, actor loss: 6.736\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 12.401, mean reward: 5.318, reward rate: 0.429, rewarded fraction: 0.501, relative distance: 128.844, critic loss: 0.867, actor loss: 8.181\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.648, mean reward: 5.481, reward rate: 0.433, rewarded fraction: 0.512, relative distance: 124.659, critic loss: 0.831, actor loss: 9.664\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.557, mean reward: 5.858, reward rate: 0.466, rewarded fraction: 0.553, relative distance: 121.686, critic loss: 0.770, actor loss: 11.131\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 12.053, mean reward: 5.450, reward rate: 0.452, rewarded fraction: 0.514, relative distance: 133.503, critic loss: 0.759, actor loss: 12.412\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.843, mean reward: 5.662, reward rate: 0.441, rewarded fraction: 0.535, relative distance: 126.093, critic loss: 0.790, actor loss: 13.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 13.441, mean reward: 6.176, reward rate: 0.459, rewarded fraction: 0.587, relative distance: 115.133, critic loss: 0.815, actor loss: 14.659\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 14.036, mean reward: 6.265, reward rate: 0.446, rewarded fraction: 0.592, relative distance: 109.841, critic loss: 0.816, actor loss: 15.559\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 13.789, mean reward: 6.586, reward rate: 0.478, rewarded fraction: 0.633, relative distance: 106.879, critic loss: 0.808, actor loss: 16.373\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 13.713, mean reward: 6.370, reward rate: 0.465, rewarded fraction: 0.602, relative distance: 108.342, critic loss: 0.849, actor loss: 17.121\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 13.855, mean reward: 6.635, reward rate: 0.479, rewarded fraction: 0.633, relative distance: 104.077, critic loss: 0.841, actor loss: 17.814\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 13.532, mean reward: 6.554, reward rate: 0.484, rewarded fraction: 0.624, relative distance: 105.420, critic loss: 0.850, actor loss: 18.404\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 13.201, mean reward: 6.223, reward rate: 0.471, rewarded fraction: 0.591, relative distance: 114.375, critic loss: 0.869, actor loss: 18.894\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 12.737, mean reward: 6.155, reward rate: 0.483, rewarded fraction: 0.594, relative distance: 125.943, critic loss: 0.852, actor loss: 19.409\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.481, mean reward: 5.935, reward rate: 0.475, rewarded fraction: 0.566, relative distance: 124.728, critic loss: 0.898, actor loss: 19.780\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 12.394, mean reward: 5.912, reward rate: 0.477, rewarded fraction: 0.562, relative distance: 124.944, critic loss: 0.914, actor loss: 20.113\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.160, mean reward: 5.771, reward rate: 0.475, rewarded fraction: 0.548, relative distance: 132.085, critic loss: 0.936, actor loss: 20.480\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.220, mean reward: 6.079, reward rate: 0.497, rewarded fraction: 0.584, relative distance: 125.080, critic loss: 0.995, actor loss: 20.801\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 11.917, mean reward: 5.704, reward rate: 0.479, rewarded fraction: 0.545, relative distance: 135.325, critic loss: 1.002, actor loss: 21.156\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.485, mean reward: 6.073, reward rate: 0.486, rewarded fraction: 0.581, relative distance: 126.770, critic loss: 1.015, actor loss: 21.463\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.315, mean reward: 5.901, reward rate: 0.479, rewarded fraction: 0.556, relative distance: 122.761, critic loss: 1.020, actor loss: 21.771\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.314, mean reward: 6.089, reward rate: 0.494, rewarded fraction: 0.581, relative distance: 121.730, critic loss: 1.002, actor loss: 21.944\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 13.185, mean reward: 6.607, reward rate: 0.501, rewarded fraction: 0.634, relative distance: 108.863, critic loss: 0.966, actor loss: 22.117\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 12.859, mean reward: 6.257, reward rate: 0.487, rewarded fraction: 0.600, relative distance: 118.295, critic loss: 0.972, actor loss: 22.291\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.903, mean reward: 6.438, reward rate: 0.499, rewarded fraction: 0.615, relative distance: 110.843, critic loss: 0.999, actor loss: 22.526\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.325, mean reward: 5.928, reward rate: 0.481, rewarded fraction: 0.563, relative distance: 126.241, critic loss: 1.024, actor loss: 22.769\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.223, mean reward: 6.043, reward rate: 0.494, rewarded fraction: 0.580, relative distance: 125.168, critic loss: 1.063, actor loss: 22.996\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.901, mean reward: 6.186, reward rate: 0.480, rewarded fraction: 0.590, relative distance: 118.799, critic loss: 1.037, actor loss: 23.108\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.972, mean reward: 6.361, reward rate: 0.490, rewarded fraction: 0.603, relative distance: 111.449, critic loss: 1.006, actor loss: 23.229\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.668, mean reward: 6.371, reward rate: 0.503, rewarded fraction: 0.617, relative distance: 121.651, critic loss: 1.000, actor loss: 23.338\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.387, mean reward: 6.019, reward rate: 0.486, rewarded fraction: 0.570, relative distance: 121.901, critic loss: 1.026, actor loss: 23.432\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.564, mean reward: 6.408, reward rate: 0.510, rewarded fraction: 0.620, relative distance: 119.070, critic loss: 1.005, actor loss: 23.534\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.936, mean reward: 6.449, reward rate: 0.499, rewarded fraction: 0.621, relative distance: 115.206, critic loss: 1.016, actor loss: 23.650\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 13.284, mean reward: 6.124, reward rate: 0.461, rewarded fraction: 0.582, relative distance: 120.956, critic loss: 1.009, actor loss: 23.751\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 13.372, mean reward: 6.605, reward rate: 0.494, rewarded fraction: 0.634, relative distance: 107.745, critic loss: 1.013, actor loss: 23.828\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 13.151, mean reward: 6.398, reward rate: 0.487, rewarded fraction: 0.612, relative distance: 113.309, critic loss: 1.006, actor loss: 23.876\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.760, mean reward: 6.478, reward rate: 0.508, rewarded fraction: 0.626, relative distance: 117.125, critic loss: 1.010, actor loss: 23.991\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 13.199, mean reward: 6.450, reward rate: 0.489, rewarded fraction: 0.611, relative distance: 106.515, critic loss: 1.025, actor loss: 24.039\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.643, mean reward: 6.101, reward rate: 0.483, rewarded fraction: 0.583, relative distance: 120.547, critic loss: 1.029, actor loss: 24.066\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.776, mean reward: 6.596, reward rate: 0.516, rewarded fraction: 0.637, relative distance: 112.527, critic loss: 1.013, actor loss: 24.132\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.499, mean reward: 6.272, reward rate: 0.502, rewarded fraction: 0.605, relative distance: 120.779, critic loss: 1.021, actor loss: 24.140\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.768, mean reward: 6.039, reward rate: 0.473, rewarded fraction: 0.574, relative distance: 122.769, critic loss: 0.987, actor loss: 24.171\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.846, mean reward: 6.538, reward rate: 0.509, rewarded fraction: 0.629, relative distance: 112.971, critic loss: 1.033, actor loss: 24.257\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 12.946, mean reward: 6.562, reward rate: 0.507, rewarded fraction: 0.632, relative distance: 112.345, critic loss: 1.001, actor loss: 24.315\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 13.051, mean reward: 6.368, reward rate: 0.488, rewarded fraction: 0.607, relative distance: 113.055, critic loss: 1.040, actor loss: 24.368\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.574, mean reward: 6.022, reward rate: 0.479, rewarded fraction: 0.570, relative distance: 119.354, critic loss: 1.054, actor loss: 24.451\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 13.448, mean reward: 6.487, reward rate: 0.482, rewarded fraction: 0.614, relative distance: 105.986, critic loss: 1.032, actor loss: 24.513\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 13.421, mean reward: 6.575, reward rate: 0.490, rewarded fraction: 0.633, relative distance: 110.488, critic loss: 1.000, actor loss: 24.599\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.980, mean reward: 6.432, reward rate: 0.495, rewarded fraction: 0.615, relative distance: 114.990, critic loss: 1.038, actor loss: 24.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 13.171, mean reward: 6.625, reward rate: 0.503, rewarded fraction: 0.633, relative distance: 106.814, critic loss: 1.068, actor loss: 24.667\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.735, mean reward: 6.133, reward rate: 0.482, rewarded fraction: 0.582, relative distance: 119.123, critic loss: 1.035, actor loss: 24.680\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.340, mean reward: 6.270, reward rate: 0.508, rewarded fraction: 0.604, relative distance: 121.373, critic loss: 1.069, actor loss: 24.682\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 13.281, mean reward: 6.737, reward rate: 0.507, rewarded fraction: 0.645, relative distance: 103.805, critic loss: 1.034, actor loss: 24.664\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.852, mean reward: 6.341, reward rate: 0.493, rewarded fraction: 0.611, relative distance: 117.034, critic loss: 1.025, actor loss: 24.641\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.885, mean reward: 6.242, reward rate: 0.484, rewarded fraction: 0.598, relative distance: 122.391, critic loss: 1.046, actor loss: 24.633\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.076, mean reward: 6.590, reward rate: 0.504, rewarded fraction: 0.631, relative distance: 107.550, critic loss: 1.040, actor loss: 24.695\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 13.335, mean reward: 6.860, reward rate: 0.514, rewarded fraction: 0.665, relative distance: 105.885, critic loss: 1.003, actor loss: 24.683\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.082, mean reward: 7.008, reward rate: 0.536, rewarded fraction: 0.681, relative distance: 104.156, critic loss: 0.998, actor loss: 24.665\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 12.697, mean reward: 6.408, reward rate: 0.505, rewarded fraction: 0.612, relative distance: 116.310, critic loss: 1.080, actor loss: 24.639\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 13.282, mean reward: 6.767, reward rate: 0.509, rewarded fraction: 0.657, relative distance: 109.180, critic loss: 1.025, actor loss: 24.637\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 351.299, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 17.822, mean reward: 0.371, reward rate: 0.021, rewarded fraction: 0.028, relative distance: 262.040, critic loss: 0.021, actor loss: 0.124\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 17.158, mean reward: 0.283, reward rate: 0.017, rewarded fraction: 0.019, relative distance: 261.437, critic loss: 0.061, actor loss: 0.202\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 10.669, mean reward: 0.565, reward rate: 0.053, rewarded fraction: 0.040, relative distance: 213.251, critic loss: 0.099, actor loss: 0.406\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 11.254, mean reward: 1.226, reward rate: 0.109, rewarded fraction: 0.098, relative distance: 181.564, critic loss: 0.185, actor loss: 1.480\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 11.059, mean reward: 1.448, reward rate: 0.131, rewarded fraction: 0.122, relative distance: 177.802, critic loss: 0.296, actor loss: 2.810\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 10.008, mean reward: 1.724, reward rate: 0.172, rewarded fraction: 0.153, relative distance: 189.361, critic loss: 0.652, actor loss: 4.561\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 10.849, mean reward: 2.286, reward rate: 0.211, rewarded fraction: 0.208, relative distance: 183.062, critic loss: 1.198, actor loss: 6.716\n",
      "t: 120000, Ep: 1199, action std: 0.50\n",
      "mean steps: 11.949, mean reward: 4.286, reward rate: 0.359, rewarded fraction: 0.401, relative distance: 141.811, critic loss: 0.906, actor loss: 8.623\n",
      "t: 130000, Ep: 1299, action std: 0.50\n",
      "mean steps: 11.464, mean reward: 4.463, reward rate: 0.389, rewarded fraction: 0.424, relative distance: 152.388, critic loss: 0.785, actor loss: 10.573\n",
      "t: 140000, Ep: 1399, action std: 0.50\n",
      "mean steps: 11.714, mean reward: 4.870, reward rate: 0.416, rewarded fraction: 0.467, relative distance: 149.292, critic loss: 0.879, actor loss: 12.211\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 12.895, mean reward: 5.751, reward rate: 0.446, rewarded fraction: 0.543, relative distance: 124.335, critic loss: 1.044, actor loss: 13.618\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 13.880, mean reward: 6.091, reward rate: 0.439, rewarded fraction: 0.583, relative distance: 116.897, critic loss: 1.023, actor loss: 14.902\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 14.948, mean reward: 6.293, reward rate: 0.421, rewarded fraction: 0.596, relative distance: 103.165, critic loss: 0.964, actor loss: 16.128\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 15.041, mean reward: 6.594, reward rate: 0.438, rewarded fraction: 0.628, relative distance: 95.355, critic loss: 0.971, actor loss: 17.268\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 15.465, mean reward: 6.538, reward rate: 0.423, rewarded fraction: 0.625, relative distance: 100.916, critic loss: 0.997, actor loss: 18.270\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 15.285, mean reward: 6.580, reward rate: 0.430, rewarded fraction: 0.627, relative distance: 97.491, critic loss: 1.018, actor loss: 19.116\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 14.438, mean reward: 6.724, reward rate: 0.466, rewarded fraction: 0.649, relative distance: 102.155, critic loss: 1.084, actor loss: 19.843\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 14.475, mean reward: 6.202, reward rate: 0.428, rewarded fraction: 0.594, relative distance: 109.824, critic loss: 1.127, actor loss: 20.547\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 13.927, mean reward: 6.454, reward rate: 0.463, rewarded fraction: 0.624, relative distance: 109.765, critic loss: 1.141, actor loss: 21.172\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 13.781, mean reward: 6.294, reward rate: 0.457, rewarded fraction: 0.608, relative distance: 111.490, critic loss: 1.134, actor loss: 21.805\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 14.842, mean reward: 6.567, reward rate: 0.442, rewarded fraction: 0.629, relative distance: 100.297, critic loss: 1.117, actor loss: 22.272\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 14.073, mean reward: 6.690, reward rate: 0.475, rewarded fraction: 0.648, relative distance: 101.964, critic loss: 1.075, actor loss: 22.714\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 14.214, mean reward: 6.962, reward rate: 0.490, rewarded fraction: 0.676, relative distance: 99.692, critic loss: 1.067, actor loss: 23.140\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 13.973, mean reward: 6.974, reward rate: 0.499, rewarded fraction: 0.677, relative distance: 99.338, critic loss: 1.066, actor loss: 23.490\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 14.125, mean reward: 6.915, reward rate: 0.490, rewarded fraction: 0.668, relative distance: 99.223, critic loss: 1.059, actor loss: 23.787\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 13.546, mean reward: 6.607, reward rate: 0.488, rewarded fraction: 0.637, relative distance: 110.143, critic loss: 1.055, actor loss: 24.094\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 14.038, mean reward: 6.770, reward rate: 0.482, rewarded fraction: 0.653, relative distance: 104.356, critic loss: 1.063, actor loss: 24.388\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 14.238, mean reward: 6.743, reward rate: 0.474, rewarded fraction: 0.651, relative distance: 105.009, critic loss: 1.081, actor loss: 24.636\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 13.612, mean reward: 6.793, reward rate: 0.499, rewarded fraction: 0.652, relative distance: 100.400, critic loss: 1.088, actor loss: 24.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 13.271, mean reward: 6.657, reward rate: 0.502, rewarded fraction: 0.643, relative distance: 109.066, critic loss: 1.099, actor loss: 25.074\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 13.314, mean reward: 6.481, reward rate: 0.487, rewarded fraction: 0.624, relative distance: 114.444, critic loss: 1.096, actor loss: 25.284\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 13.618, mean reward: 6.927, reward rate: 0.509, rewarded fraction: 0.673, relative distance: 103.757, critic loss: 1.110, actor loss: 25.427\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 13.886, mean reward: 6.837, reward rate: 0.492, rewarded fraction: 0.663, relative distance: 105.722, critic loss: 1.104, actor loss: 25.604\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 13.564, mean reward: 6.963, reward rate: 0.513, rewarded fraction: 0.673, relative distance: 101.775, critic loss: 1.103, actor loss: 25.736\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 13.693, mean reward: 7.054, reward rate: 0.515, rewarded fraction: 0.681, relative distance: 99.337, critic loss: 1.078, actor loss: 25.871\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 13.777, mean reward: 6.620, reward rate: 0.481, rewarded fraction: 0.633, relative distance: 104.939, critic loss: 1.080, actor loss: 26.004\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 13.306, mean reward: 6.623, reward rate: 0.498, rewarded fraction: 0.638, relative distance: 108.261, critic loss: 1.096, actor loss: 26.144\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 13.713, mean reward: 6.477, reward rate: 0.472, rewarded fraction: 0.617, relative distance: 106.022, critic loss: 1.112, actor loss: 26.259\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 13.749, mean reward: 6.778, reward rate: 0.493, rewarded fraction: 0.654, relative distance: 104.244, critic loss: 1.098, actor loss: 26.353\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 14.158, mean reward: 6.690, reward rate: 0.473, rewarded fraction: 0.638, relative distance: 101.076, critic loss: 1.103, actor loss: 26.452\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 14.345, mean reward: 6.909, reward rate: 0.482, rewarded fraction: 0.665, relative distance: 96.531, critic loss: 1.110, actor loss: 26.543\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 13.881, mean reward: 6.746, reward rate: 0.486, rewarded fraction: 0.646, relative distance: 102.237, critic loss: 1.121, actor loss: 26.590\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 13.794, mean reward: 6.918, reward rate: 0.502, rewarded fraction: 0.667, relative distance: 98.970, critic loss: 1.105, actor loss: 26.658\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 13.875, mean reward: 7.174, reward rate: 0.517, rewarded fraction: 0.695, relative distance: 94.341, critic loss: 1.107, actor loss: 26.683\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 13.921, mean reward: 6.738, reward rate: 0.484, rewarded fraction: 0.645, relative distance: 103.069, critic loss: 1.115, actor loss: 26.717\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 13.817, mean reward: 6.839, reward rate: 0.495, rewarded fraction: 0.653, relative distance: 99.134, critic loss: 1.118, actor loss: 26.779\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 13.462, mean reward: 6.934, reward rate: 0.515, rewarded fraction: 0.668, relative distance: 101.759, critic loss: 1.128, actor loss: 26.821\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 13.893, mean reward: 7.220, reward rate: 0.520, rewarded fraction: 0.701, relative distance: 94.978, critic loss: 1.139, actor loss: 26.844\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 13.760, mean reward: 6.904, reward rate: 0.502, rewarded fraction: 0.668, relative distance: 103.968, critic loss: 1.125, actor loss: 26.868\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 14.124, mean reward: 7.153, reward rate: 0.506, rewarded fraction: 0.692, relative distance: 97.115, critic loss: 1.137, actor loss: 26.876\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 14.352, mean reward: 6.919, reward rate: 0.482, rewarded fraction: 0.662, relative distance: 97.956, critic loss: 1.122, actor loss: 26.893\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 13.484, mean reward: 6.559, reward rate: 0.486, rewarded fraction: 0.629, relative distance: 110.446, critic loss: 1.114, actor loss: 26.940\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 13.632, mean reward: 6.709, reward rate: 0.492, rewarded fraction: 0.647, relative distance: 106.363, critic loss: 1.117, actor loss: 26.934\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 13.937, mean reward: 7.115, reward rate: 0.510, rewarded fraction: 0.685, relative distance: 94.706, critic loss: 1.120, actor loss: 26.942\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 14.061, mean reward: 6.934, reward rate: 0.493, rewarded fraction: 0.670, relative distance: 100.426, critic loss: 1.111, actor loss: 26.972\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 14.678, mean reward: 7.265, reward rate: 0.495, rewarded fraction: 0.696, relative distance: 86.808, critic loss: 1.110, actor loss: 26.960\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 14.263, mean reward: 7.199, reward rate: 0.505, rewarded fraction: 0.694, relative distance: 92.322, critic loss: 1.106, actor loss: 26.990\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 14.032, mean reward: 6.814, reward rate: 0.486, rewarded fraction: 0.649, relative distance: 98.300, critic loss: 1.112, actor loss: 26.989\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 14.391, mean reward: 7.212, reward rate: 0.501, rewarded fraction: 0.694, relative distance: 88.670, critic loss: 1.125, actor loss: 27.000\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 14.173, mean reward: 6.960, reward rate: 0.491, rewarded fraction: 0.671, relative distance: 100.071, critic loss: 1.104, actor loss: 26.987\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 13.833, mean reward: 7.039, reward rate: 0.509, rewarded fraction: 0.680, relative distance: 98.849, critic loss: 1.108, actor loss: 26.972\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 13.837, mean reward: 6.939, reward rate: 0.502, rewarded fraction: 0.672, relative distance: 100.756, critic loss: 1.116, actor loss: 26.968\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 14.255, mean reward: 7.076, reward rate: 0.496, rewarded fraction: 0.680, relative distance: 94.160, critic loss: 1.132, actor loss: 26.978\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 14.047, mean reward: 6.849, reward rate: 0.488, rewarded fraction: 0.655, relative distance: 99.716, critic loss: 1.147, actor loss: 26.961\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 14.728, mean reward: 7.299, reward rate: 0.496, rewarded fraction: 0.709, relative distance: 91.909, critic loss: 1.121, actor loss: 26.906\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 13.688, mean reward: 7.040, reward rate: 0.514, rewarded fraction: 0.677, relative distance: 96.249, critic loss: 1.123, actor loss: 26.949\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 14.310, mean reward: 7.283, reward rate: 0.509, rewarded fraction: 0.704, relative distance: 91.382, critic loss: 1.108, actor loss: 26.911\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 14.065, mean reward: 7.183, reward rate: 0.511, rewarded fraction: 0.697, relative distance: 95.153, critic loss: 1.110, actor loss: 26.889\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 14.219, mean reward: 7.224, reward rate: 0.508, rewarded fraction: 0.701, relative distance: 95.435, critic loss: 1.109, actor loss: 26.900\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 14.120, mean reward: 6.783, reward rate: 0.480, rewarded fraction: 0.654, relative distance: 104.144, critic loss: 1.114, actor loss: 26.900\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 14.055, mean reward: 7.080, reward rate: 0.504, rewarded fraction: 0.684, relative distance: 95.546, critic loss: 1.122, actor loss: 26.882\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 13.923, mean reward: 6.704, reward rate: 0.482, rewarded fraction: 0.644, relative distance: 104.635, critic loss: 1.135, actor loss: 26.891\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 14.023, mean reward: 7.156, reward rate: 0.510, rewarded fraction: 0.687, relative distance: 91.252, critic loss: 1.165, actor loss: 26.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.049, mean reward: 7.069, reward rate: 0.503, rewarded fraction: 0.681, relative distance: 95.440, critic loss: 1.142, actor loss: 26.830\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 14.450, mean reward: 7.017, reward rate: 0.486, rewarded fraction: 0.673, relative distance: 96.491, critic loss: 1.121, actor loss: 26.851\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 14.302, mean reward: 7.018, reward rate: 0.491, rewarded fraction: 0.675, relative distance: 96.234, critic loss: 1.125, actor loss: 26.857\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 13.773, mean reward: 6.817, reward rate: 0.495, rewarded fraction: 0.657, relative distance: 103.380, critic loss: 1.135, actor loss: 26.818\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 13.805, mean reward: 6.861, reward rate: 0.497, rewarded fraction: 0.661, relative distance: 102.330, critic loss: 1.129, actor loss: 26.807\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 13.801, mean reward: 7.020, reward rate: 0.509, rewarded fraction: 0.677, relative distance: 95.504, critic loss: 1.140, actor loss: 26.779\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 14.232, mean reward: 7.206, reward rate: 0.506, rewarded fraction: 0.702, relative distance: 95.951, critic loss: 1.118, actor loss: 26.757\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 14.370, mean reward: 7.368, reward rate: 0.513, rewarded fraction: 0.716, relative distance: 92.761, critic loss: 1.107, actor loss: 26.782\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 14.200, mean reward: 6.940, reward rate: 0.489, rewarded fraction: 0.672, relative distance: 100.114, critic loss: 1.110, actor loss: 26.770\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 14.183, mean reward: 7.189, reward rate: 0.507, rewarded fraction: 0.697, relative distance: 94.143, critic loss: 1.145, actor loss: 26.774\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 14.271, mean reward: 7.262, reward rate: 0.509, rewarded fraction: 0.709, relative distance: 95.641, critic loss: 1.136, actor loss: 26.799\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 14.139, mean reward: 7.218, reward rate: 0.511, rewarded fraction: 0.702, relative distance: 96.021, critic loss: 1.233, actor loss: 26.817\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 14.092, mean reward: 6.898, reward rate: 0.490, rewarded fraction: 0.665, relative distance: 100.356, critic loss: 1.249, actor loss: 26.823\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 13.787, mean reward: 7.340, reward rate: 0.532, rewarded fraction: 0.717, relative distance: 95.450, critic loss: 1.144, actor loss: 26.841\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 14.524, mean reward: 7.203, reward rate: 0.496, rewarded fraction: 0.691, relative distance: 89.095, critic loss: 1.171, actor loss: 26.840\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 13.967, mean reward: 7.174, reward rate: 0.514, rewarded fraction: 0.702, relative distance: 99.610, critic loss: 1.169, actor loss: 26.887\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 14.085, mean reward: 7.232, reward rate: 0.513, rewarded fraction: 0.701, relative distance: 94.621, critic loss: 1.216, actor loss: 26.921\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 14.161, mean reward: 6.907, reward rate: 0.488, rewarded fraction: 0.667, relative distance: 102.152, critic loss: 1.180, actor loss: 26.932\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 14.217, mean reward: 7.012, reward rate: 0.493, rewarded fraction: 0.672, relative distance: 95.386, critic loss: 1.286, actor loss: 26.908\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 14.038, mean reward: 7.107, reward rate: 0.506, rewarded fraction: 0.687, relative distance: 93.354, critic loss: 1.262, actor loss: 26.882\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 14.817, mean reward: 7.429, reward rate: 0.501, rewarded fraction: 0.719, relative distance: 87.141, critic loss: 1.179, actor loss: 26.902\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 14.690, mean reward: 7.542, reward rate: 0.513, rewarded fraction: 0.735, relative distance: 84.741, critic loss: 1.150, actor loss: 26.924\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 13.792, mean reward: 6.912, reward rate: 0.501, rewarded fraction: 0.666, relative distance: 102.600, critic loss: 1.171, actor loss: 26.925\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 17.310, mean reward: 0.114, reward rate: 0.007, rewarded fraction: 0.008, relative distance: 317.633, critic loss: 0.013, actor loss: 0.066\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.751, mean reward: 0.384, reward rate: 0.023, rewarded fraction: 0.029, relative distance: 254.190, critic loss: 0.035, actor loss: 0.445\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.840, mean reward: 0.402, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 258.387, critic loss: 0.073, actor loss: 0.503\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.490, mean reward: 0.404, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 262.572, critic loss: 0.097, actor loss: 0.414\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 8.524, mean reward: 0.086, reward rate: 0.010, rewarded fraction: 0.005, relative distance: 281.611, critic loss: 0.103, actor loss: 0.398\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 11.883, mean reward: 0.345, reward rate: 0.029, rewarded fraction: 0.024, relative distance: 242.008, critic loss: 0.138, actor loss: 0.544\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 7.671, mean reward: 0.556, reward rate: 0.072, rewarded fraction: 0.041, relative distance: 218.684, critic loss: 0.461, actor loss: 1.323\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 7.973, mean reward: 0.680, reward rate: 0.085, rewarded fraction: 0.051, relative distance: 216.835, critic loss: 0.608, actor loss: 1.868\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 7.959, mean reward: 0.791, reward rate: 0.099, rewarded fraction: 0.063, relative distance: 215.348, critic loss: 0.771, actor loss: 2.686\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 7.205, mean reward: 1.238, reward rate: 0.172, rewarded fraction: 0.111, relative distance: 218.329, critic loss: 0.845, actor loss: 3.884\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 7.099, mean reward: 1.347, reward rate: 0.190, rewarded fraction: 0.121, relative distance: 223.573, critic loss: 0.951, actor loss: 5.141\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 6.924, mean reward: 1.714, reward rate: 0.248, rewarded fraction: 0.158, relative distance: 222.297, critic loss: 0.925, actor loss: 6.494\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 8.001, mean reward: 3.072, reward rate: 0.384, rewarded fraction: 0.293, relative distance: 195.498, critic loss: 0.936, actor loss: 7.860\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 7.966, mean reward: 3.161, reward rate: 0.397, rewarded fraction: 0.302, relative distance: 195.854, critic loss: 1.200, actor loss: 9.268\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 8.213, mean reward: 3.465, reward rate: 0.422, rewarded fraction: 0.330, relative distance: 188.884, critic loss: 1.337, actor loss: 10.627\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 8.559, mean reward: 3.662, reward rate: 0.428, rewarded fraction: 0.351, relative distance: 187.354, critic loss: 1.334, actor loss: 11.827\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 9.359, mean reward: 4.124, reward rate: 0.441, rewarded fraction: 0.395, relative distance: 173.455, critic loss: 1.314, actor loss: 12.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 9.663, mean reward: 4.437, reward rate: 0.459, rewarded fraction: 0.422, relative distance: 166.566, critic loss: 1.281, actor loss: 14.040\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 10.084, mean reward: 4.435, reward rate: 0.440, rewarded fraction: 0.423, relative distance: 166.857, critic loss: 1.269, actor loss: 15.046\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 10.295, mean reward: 4.565, reward rate: 0.443, rewarded fraction: 0.438, relative distance: 167.271, critic loss: 1.265, actor loss: 16.041\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 10.156, mean reward: 4.637, reward rate: 0.457, rewarded fraction: 0.446, relative distance: 166.904, critic loss: 1.258, actor loss: 17.023\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 11.757, mean reward: 5.750, reward rate: 0.489, rewarded fraction: 0.552, relative distance: 133.718, critic loss: 1.238, actor loss: 17.933\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 12.249, mean reward: 5.937, reward rate: 0.485, rewarded fraction: 0.567, relative distance: 126.272, critic loss: 1.217, actor loss: 18.746\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 11.916, mean reward: 5.577, reward rate: 0.468, rewarded fraction: 0.534, relative distance: 135.698, critic loss: 1.203, actor loss: 19.457\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 11.922, mean reward: 5.608, reward rate: 0.470, rewarded fraction: 0.537, relative distance: 137.584, critic loss: 1.206, actor loss: 20.088\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 12.201, mean reward: 6.052, reward rate: 0.496, rewarded fraction: 0.583, relative distance: 127.617, critic loss: 1.191, actor loss: 20.623\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 12.371, mean reward: 6.144, reward rate: 0.497, rewarded fraction: 0.592, relative distance: 125.269, critic loss: 1.195, actor loss: 20.987\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 11.856, mean reward: 5.690, reward rate: 0.480, rewarded fraction: 0.547, relative distance: 133.720, critic loss: 1.187, actor loss: 21.390\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 12.403, mean reward: 5.869, reward rate: 0.473, rewarded fraction: 0.562, relative distance: 127.687, critic loss: 1.182, actor loss: 21.764\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 12.262, mean reward: 5.808, reward rate: 0.474, rewarded fraction: 0.559, relative distance: 134.027, critic loss: 1.157, actor loss: 22.128\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 12.710, mean reward: 6.299, reward rate: 0.496, rewarded fraction: 0.611, relative distance: 119.901, critic loss: 1.158, actor loss: 22.476\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 13.235, mean reward: 6.784, reward rate: 0.513, rewarded fraction: 0.651, relative distance: 103.984, critic loss: 1.161, actor loss: 22.748\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 12.409, mean reward: 6.370, reward rate: 0.513, rewarded fraction: 0.614, relative distance: 118.324, critic loss: 1.160, actor loss: 23.028\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.526, mean reward: 6.082, reward rate: 0.485, rewarded fraction: 0.585, relative distance: 130.822, critic loss: 1.150, actor loss: 23.238\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.748, mean reward: 6.548, reward rate: 0.514, rewarded fraction: 0.633, relative distance: 113.676, critic loss: 1.152, actor loss: 23.515\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 12.549, mean reward: 6.197, reward rate: 0.494, rewarded fraction: 0.601, relative distance: 125.772, critic loss: 1.173, actor loss: 23.739\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 12.489, mean reward: 6.145, reward rate: 0.492, rewarded fraction: 0.589, relative distance: 124.489, critic loss: 1.165, actor loss: 23.980\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 12.014, mean reward: 5.893, reward rate: 0.490, rewarded fraction: 0.567, relative distance: 131.334, critic loss: 1.171, actor loss: 24.173\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.412, mean reward: 6.401, reward rate: 0.516, rewarded fraction: 0.622, relative distance: 119.324, critic loss: 1.150, actor loss: 24.291\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.194, mean reward: 5.930, reward rate: 0.486, rewarded fraction: 0.570, relative distance: 130.979, critic loss: 1.159, actor loss: 24.478\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 12.139, mean reward: 5.986, reward rate: 0.493, rewarded fraction: 0.577, relative distance: 133.306, critic loss: 1.189, actor loss: 24.635\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.384, mean reward: 6.239, reward rate: 0.504, rewarded fraction: 0.605, relative distance: 122.689, critic loss: 1.165, actor loss: 24.727\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 12.271, mean reward: 5.897, reward rate: 0.481, rewarded fraction: 0.567, relative distance: 133.542, critic loss: 1.153, actor loss: 24.862\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 12.281, mean reward: 5.904, reward rate: 0.481, rewarded fraction: 0.568, relative distance: 128.870, critic loss: 1.184, actor loss: 24.948\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 12.131, mean reward: 5.906, reward rate: 0.487, rewarded fraction: 0.570, relative distance: 132.341, critic loss: 1.160, actor loss: 25.006\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 12.168, mean reward: 5.826, reward rate: 0.479, rewarded fraction: 0.557, relative distance: 131.061, critic loss: 1.158, actor loss: 25.066\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 11.729, mean reward: 5.772, reward rate: 0.492, rewarded fraction: 0.558, relative distance: 136.185, critic loss: 1.186, actor loss: 25.139\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 12.525, mean reward: 6.378, reward rate: 0.509, rewarded fraction: 0.617, relative distance: 118.083, critic loss: 1.182, actor loss: 25.169\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 12.093, mean reward: 6.108, reward rate: 0.505, rewarded fraction: 0.591, relative distance: 127.739, critic loss: 1.174, actor loss: 25.207\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 12.611, mean reward: 6.456, reward rate: 0.512, rewarded fraction: 0.624, relative distance: 116.639, critic loss: 1.161, actor loss: 25.240\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.189, mean reward: 5.962, reward rate: 0.489, rewarded fraction: 0.573, relative distance: 129.597, critic loss: 1.174, actor loss: 25.260\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 11.734, mean reward: 5.845, reward rate: 0.498, rewarded fraction: 0.561, relative distance: 133.654, critic loss: 1.175, actor loss: 25.252\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.100, mean reward: 6.095, reward rate: 0.504, rewarded fraction: 0.591, relative distance: 129.537, critic loss: 1.168, actor loss: 25.315\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.273, mean reward: 6.334, reward rate: 0.516, rewarded fraction: 0.617, relative distance: 123.100, critic loss: 1.159, actor loss: 25.326\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.336, mean reward: 6.384, reward rate: 0.518, rewarded fraction: 0.619, relative distance: 119.428, critic loss: 1.160, actor loss: 25.316\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.293, mean reward: 6.093, reward rate: 0.496, rewarded fraction: 0.587, relative distance: 126.355, critic loss: 1.159, actor loss: 25.365\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.350, mean reward: 6.288, reward rate: 0.509, rewarded fraction: 0.611, relative distance: 126.058, critic loss: 1.142, actor loss: 25.338\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.557, mean reward: 6.060, reward rate: 0.483, rewarded fraction: 0.582, relative distance: 127.390, critic loss: 1.149, actor loss: 25.339\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 12.982, mean reward: 6.567, reward rate: 0.506, rewarded fraction: 0.638, relative distance: 113.699, critic loss: 1.135, actor loss: 25.356\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 12.488, mean reward: 6.076, reward rate: 0.487, rewarded fraction: 0.587, relative distance: 127.514, critic loss: 1.136, actor loss: 25.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.168, mean reward: 6.006, reward rate: 0.494, rewarded fraction: 0.580, relative distance: 131.192, critic loss: 1.146, actor loss: 25.366\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.496, mean reward: 6.255, reward rate: 0.501, rewarded fraction: 0.602, relative distance: 122.401, critic loss: 1.138, actor loss: 25.379\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.616, mean reward: 6.470, reward rate: 0.513, rewarded fraction: 0.624, relative distance: 116.553, critic loss: 1.151, actor loss: 25.366\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.444, mean reward: 6.103, reward rate: 0.490, rewarded fraction: 0.590, relative distance: 127.458, critic loss: 1.142, actor loss: 25.363\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.659, mean reward: 6.450, reward rate: 0.509, rewarded fraction: 0.625, relative distance: 121.192, critic loss: 1.131, actor loss: 25.355\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.399, mean reward: 6.284, reward rate: 0.507, rewarded fraction: 0.611, relative distance: 122.832, critic loss: 1.125, actor loss: 25.400\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.380, mean reward: 6.160, reward rate: 0.498, rewarded fraction: 0.592, relative distance: 124.313, critic loss: 1.152, actor loss: 25.382\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.969, mean reward: 6.508, reward rate: 0.502, rewarded fraction: 0.629, relative distance: 116.803, critic loss: 1.144, actor loss: 25.369\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.578, mean reward: 6.180, reward rate: 0.491, rewarded fraction: 0.597, relative distance: 126.675, critic loss: 1.143, actor loss: 25.401\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.504, mean reward: 6.343, reward rate: 0.507, rewarded fraction: 0.615, relative distance: 122.076, critic loss: 1.151, actor loss: 25.410\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 11.891, mean reward: 6.023, reward rate: 0.507, rewarded fraction: 0.584, relative distance: 132.230, critic loss: 1.146, actor loss: 25.409\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.479, mean reward: 6.564, reward rate: 0.526, rewarded fraction: 0.636, relative distance: 116.838, critic loss: 1.160, actor loss: 25.393\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.532, mean reward: 6.074, reward rate: 0.485, rewarded fraction: 0.587, relative distance: 128.304, critic loss: 1.150, actor loss: 25.404\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 12.279, mean reward: 6.258, reward rate: 0.510, rewarded fraction: 0.602, relative distance: 122.183, critic loss: 1.165, actor loss: 25.409\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.577, mean reward: 6.279, reward rate: 0.499, rewarded fraction: 0.606, relative distance: 123.369, critic loss: 1.126, actor loss: 25.382\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.379, mean reward: 6.231, reward rate: 0.503, rewarded fraction: 0.603, relative distance: 124.382, critic loss: 1.141, actor loss: 25.387\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.592, mean reward: 6.432, reward rate: 0.511, rewarded fraction: 0.621, relative distance: 118.154, critic loss: 1.154, actor loss: 25.373\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.564, mean reward: 6.474, reward rate: 0.515, rewarded fraction: 0.625, relative distance: 115.944, critic loss: 1.145, actor loss: 25.362\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.287, mean reward: 6.080, reward rate: 0.495, rewarded fraction: 0.587, relative distance: 129.080, critic loss: 1.161, actor loss: 25.365\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 11.950, mean reward: 6.000, reward rate: 0.502, rewarded fraction: 0.580, relative distance: 130.598, critic loss: 1.151, actor loss: 25.368\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.226, mean reward: 6.292, reward rate: 0.515, rewarded fraction: 0.608, relative distance: 123.275, critic loss: 1.158, actor loss: 25.353\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.401, mean reward: 6.293, reward rate: 0.507, rewarded fraction: 0.611, relative distance: 122.779, critic loss: 1.144, actor loss: 25.388\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 12.502, mean reward: 6.217, reward rate: 0.497, rewarded fraction: 0.599, relative distance: 123.028, critic loss: 1.160, actor loss: 25.369\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 12.799, mean reward: 6.558, reward rate: 0.512, rewarded fraction: 0.634, relative distance: 112.864, critic loss: 1.173, actor loss: 25.381\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.464, mean reward: 6.570, reward rate: 0.527, rewarded fraction: 0.641, relative distance: 117.300, critic loss: 1.146, actor loss: 25.362\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.371, mean reward: 6.213, reward rate: 0.502, rewarded fraction: 0.601, relative distance: 123.266, critic loss: 1.155, actor loss: 25.395\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.885, mean reward: 6.417, reward rate: 0.498, rewarded fraction: 0.614, relative distance: 116.410, critic loss: 1.164, actor loss: 25.359\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.771, mean reward: 6.434, reward rate: 0.504, rewarded fraction: 0.620, relative distance: 118.507, critic loss: 1.143, actor loss: 25.384\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 12.530, mean reward: 6.250, reward rate: 0.499, rewarded fraction: 0.603, relative distance: 121.926, critic loss: 1.148, actor loss: 25.394\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.694, mean reward: 6.248, reward rate: 0.492, rewarded fraction: 0.600, relative distance: 121.669, critic loss: 1.164, actor loss: 25.390\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.018, mean reward: 5.981, reward rate: 0.498, rewarded fraction: 0.575, relative distance: 129.186, critic loss: 1.184, actor loss: 25.360\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 12.415, mean reward: 6.360, reward rate: 0.512, rewarded fraction: 0.615, relative distance: 121.148, critic loss: 1.171, actor loss: 25.373\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 12.731, mean reward: 6.278, reward rate: 0.493, rewarded fraction: 0.602, relative distance: 118.711, critic loss: 1.163, actor loss: 25.359\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.030, mean reward: 6.545, reward rate: 0.502, rewarded fraction: 0.633, relative distance: 114.892, critic loss: 1.145, actor loss: 25.366\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 12.588, mean reward: 6.405, reward rate: 0.509, rewarded fraction: 0.616, relative distance: 117.103, critic loss: 1.132, actor loss: 25.402\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.384, mean reward: 6.314, reward rate: 0.510, rewarded fraction: 0.610, relative distance: 121.491, critic loss: 1.151, actor loss: 25.383\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.556, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 380.532, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 4.750, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 384.635, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 5.333, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 371.281, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 373.271, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 14.326, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 353.629, critic loss: 0.000, actor loss: 0.095\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 7.923, mean reward: 0.023, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 288.700, critic loss: 0.000, actor loss: 0.098\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 9.706, mean reward: 0.184, reward rate: 0.019, rewarded fraction: 0.013, relative distance: 274.872, critic loss: 0.010, actor loss: 0.111\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 15.798, mean reward: 0.459, reward rate: 0.029, rewarded fraction: 0.036, relative distance: 244.592, critic loss: 0.051, actor loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.931, mean reward: 0.369, reward rate: 0.023, rewarded fraction: 0.026, relative distance: 250.754, critic loss: 0.085, actor loss: 0.124\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.321, mean reward: 0.696, reward rate: 0.043, rewarded fraction: 0.054, relative distance: 235.452, critic loss: 0.105, actor loss: 0.091\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 15.973, mean reward: 0.550, reward rate: 0.034, rewarded fraction: 0.042, relative distance: 265.826, critic loss: 0.126, actor loss: 0.051\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.279, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 370.084, critic loss: 0.094, actor loss: -0.000\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.377, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 398.453, critic loss: 0.066, actor loss: -0.030\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 14.989, mean reward: 0.038, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 344.128, critic loss: 0.048, actor loss: -0.050\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 15.897, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 311.141, critic loss: 0.038, actor loss: -0.073\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 12.545, mean reward: 0.023, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 363.748, critic loss: 0.127, actor loss: 0.031\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 10.136, mean reward: 0.165, reward rate: 0.016, rewarded fraction: 0.010, relative distance: 248.952, critic loss: 0.110, actor loss: 0.075\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 10.165, mean reward: 0.704, reward rate: 0.069, rewarded fraction: 0.057, relative distance: 215.376, critic loss: 0.142, actor loss: 0.168\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 10.358, mean reward: 0.742, reward rate: 0.072, rewarded fraction: 0.057, relative distance: 205.531, critic loss: 0.206, actor loss: 0.183\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 7.355, mean reward: 0.858, reward rate: 0.117, rewarded fraction: 0.071, relative distance: 219.259, critic loss: 0.467, actor loss: 0.759\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 6.546, mean reward: 1.121, reward rate: 0.171, rewarded fraction: 0.098, relative distance: 225.583, critic loss: 0.590, actor loss: 1.966\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 6.970, mean reward: 1.285, reward rate: 0.184, rewarded fraction: 0.116, relative distance: 221.212, critic loss: 0.686, actor loss: 3.196\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 6.766, mean reward: 1.275, reward rate: 0.188, rewarded fraction: 0.113, relative distance: 224.117, critic loss: 0.683, actor loss: 4.364\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 7.122, mean reward: 1.497, reward rate: 0.210, rewarded fraction: 0.136, relative distance: 217.568, critic loss: 0.686, actor loss: 5.594\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 9.153, mean reward: 3.618, reward rate: 0.395, rewarded fraction: 0.347, relative distance: 177.116, critic loss: 0.858, actor loss: 6.995\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 9.974, mean reward: 4.149, reward rate: 0.416, rewarded fraction: 0.396, relative distance: 166.611, critic loss: 0.987, actor loss: 8.541\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 10.200, mean reward: 4.580, reward rate: 0.449, rewarded fraction: 0.438, relative distance: 162.328, critic loss: 0.983, actor loss: 10.137\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 11.301, mean reward: 5.217, reward rate: 0.462, rewarded fraction: 0.499, relative distance: 142.233, critic loss: 0.979, actor loss: 11.692\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 11.893, mean reward: 5.553, reward rate: 0.467, rewarded fraction: 0.529, relative distance: 133.235, critic loss: 0.997, actor loss: 13.212\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 12.086, mean reward: 5.552, reward rate: 0.459, rewarded fraction: 0.534, relative distance: 137.879, critic loss: 0.994, actor loss: 14.632\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 12.110, mean reward: 5.667, reward rate: 0.468, rewarded fraction: 0.544, relative distance: 135.113, critic loss: 0.994, actor loss: 15.959\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 12.090, mean reward: 5.563, reward rate: 0.460, rewarded fraction: 0.535, relative distance: 138.738, critic loss: 0.993, actor loss: 17.144\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 12.711, mean reward: 6.201, reward rate: 0.488, rewarded fraction: 0.602, relative distance: 123.630, critic loss: 1.006, actor loss: 18.218\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 12.516, mean reward: 6.157, reward rate: 0.492, rewarded fraction: 0.595, relative distance: 126.254, critic loss: 0.978, actor loss: 19.189\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 13.162, mean reward: 6.639, reward rate: 0.504, rewarded fraction: 0.638, relative distance: 106.859, critic loss: 0.970, actor loss: 20.078\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 12.225, mean reward: 6.028, reward rate: 0.493, rewarded fraction: 0.575, relative distance: 123.841, critic loss: 0.962, actor loss: 20.872\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 12.290, mean reward: 5.930, reward rate: 0.482, rewarded fraction: 0.566, relative distance: 125.540, critic loss: 0.961, actor loss: 21.594\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.802, mean reward: 6.327, reward rate: 0.494, rewarded fraction: 0.610, relative distance: 117.546, critic loss: 0.930, actor loss: 22.226\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.545, mean reward: 6.321, reward rate: 0.504, rewarded fraction: 0.613, relative distance: 123.448, critic loss: 0.928, actor loss: 22.780\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 12.434, mean reward: 6.304, reward rate: 0.507, rewarded fraction: 0.611, relative distance: 122.054, critic loss: 0.948, actor loss: 23.213\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 12.448, mean reward: 6.057, reward rate: 0.487, rewarded fraction: 0.587, relative distance: 130.562, critic loss: 0.939, actor loss: 23.568\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 12.809, mean reward: 6.460, reward rate: 0.504, rewarded fraction: 0.621, relative distance: 115.131, critic loss: 0.942, actor loss: 23.932\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.744, mean reward: 6.524, reward rate: 0.512, rewarded fraction: 0.632, relative distance: 115.908, critic loss: 0.942, actor loss: 24.196\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.577, mean reward: 6.250, reward rate: 0.497, rewarded fraction: 0.600, relative distance: 125.108, critic loss: 0.944, actor loss: 24.505\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 11.647, mean reward: 5.767, reward rate: 0.495, rewarded fraction: 0.551, relative distance: 134.678, critic loss: 0.948, actor loss: 24.707\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 11.848, mean reward: 6.194, reward rate: 0.523, rewarded fraction: 0.603, relative distance: 130.558, critic loss: 0.923, actor loss: 24.885\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 11.318, mean reward: 5.593, reward rate: 0.494, rewarded fraction: 0.537, relative distance: 142.165, critic loss: 0.908, actor loss: 25.032\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 11.897, mean reward: 5.897, reward rate: 0.496, rewarded fraction: 0.568, relative distance: 135.276, critic loss: 0.922, actor loss: 25.215\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 11.954, mean reward: 5.887, reward rate: 0.493, rewarded fraction: 0.565, relative distance: 134.708, critic loss: 0.942, actor loss: 25.371\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 11.420, mean reward: 5.489, reward rate: 0.481, rewarded fraction: 0.526, relative distance: 149.172, critic loss: 0.932, actor loss: 25.461\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 11.682, mean reward: 5.831, reward rate: 0.499, rewarded fraction: 0.560, relative distance: 137.394, critic loss: 0.923, actor loss: 25.575\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 12.211, mean reward: 6.111, reward rate: 0.500, rewarded fraction: 0.585, relative distance: 129.633, critic loss: 0.934, actor loss: 25.636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 11.494, mean reward: 5.713, reward rate: 0.497, rewarded fraction: 0.549, relative distance: 147.144, critic loss: 0.939, actor loss: 25.656\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 11.429, mean reward: 5.684, reward rate: 0.497, rewarded fraction: 0.546, relative distance: 143.254, critic loss: 0.932, actor loss: 25.742\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 11.891, mean reward: 6.168, reward rate: 0.519, rewarded fraction: 0.596, relative distance: 131.203, critic loss: 0.930, actor loss: 25.815\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 11.553, mean reward: 5.889, reward rate: 0.510, rewarded fraction: 0.570, relative distance: 141.506, critic loss: 0.931, actor loss: 25.840\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 11.212, mean reward: 5.665, reward rate: 0.505, rewarded fraction: 0.548, relative distance: 147.306, critic loss: 0.921, actor loss: 25.859\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 11.007, mean reward: 5.674, reward rate: 0.515, rewarded fraction: 0.547, relative distance: 145.325, critic loss: 0.937, actor loss: 25.871\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 11.059, mean reward: 5.820, reward rate: 0.526, rewarded fraction: 0.566, relative distance: 146.113, critic loss: 0.940, actor loss: 25.904\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 11.648, mean reward: 5.969, reward rate: 0.512, rewarded fraction: 0.578, relative distance: 137.688, critic loss: 0.922, actor loss: 25.915\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 10.900, mean reward: 5.733, reward rate: 0.526, rewarded fraction: 0.555, relative distance: 146.411, critic loss: 0.943, actor loss: 25.935\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 10.617, mean reward: 5.666, reward rate: 0.534, rewarded fraction: 0.553, relative distance: 154.388, critic loss: 0.943, actor loss: 25.967\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 11.070, mean reward: 5.813, reward rate: 0.525, rewarded fraction: 0.559, relative distance: 144.089, critic loss: 0.947, actor loss: 25.975\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 11.536, mean reward: 6.167, reward rate: 0.535, rewarded fraction: 0.598, relative distance: 135.018, critic loss: 0.912, actor loss: 26.019\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 11.212, mean reward: 5.813, reward rate: 0.518, rewarded fraction: 0.562, relative distance: 145.201, critic loss: 0.942, actor loss: 25.979\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 10.622, mean reward: 5.370, reward rate: 0.506, rewarded fraction: 0.517, relative distance: 159.895, critic loss: 0.919, actor loss: 25.974\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 10.607, mean reward: 5.447, reward rate: 0.513, rewarded fraction: 0.523, relative distance: 153.754, critic loss: 0.970, actor loss: 25.973\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 10.720, mean reward: 5.620, reward rate: 0.524, rewarded fraction: 0.540, relative distance: 146.145, critic loss: 0.929, actor loss: 25.901\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 10.717, mean reward: 5.575, reward rate: 0.520, rewarded fraction: 0.535, relative distance: 149.111, critic loss: 0.978, actor loss: 25.895\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 10.486, mean reward: 5.402, reward rate: 0.515, rewarded fraction: 0.516, relative distance: 154.385, critic loss: 0.956, actor loss: 25.874\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 10.656, mean reward: 5.432, reward rate: 0.510, rewarded fraction: 0.520, relative distance: 147.320, critic loss: 1.013, actor loss: 25.844\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 10.588, mean reward: 5.208, reward rate: 0.492, rewarded fraction: 0.499, relative distance: 161.018, critic loss: 0.945, actor loss: 25.736\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 10.632, mean reward: 5.465, reward rate: 0.514, rewarded fraction: 0.528, relative distance: 155.105, critic loss: 0.966, actor loss: 25.733\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 10.503, mean reward: 5.405, reward rate: 0.515, rewarded fraction: 0.521, relative distance: 158.289, critic loss: 0.937, actor loss: 25.742\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 9.702, mean reward: 5.040, reward rate: 0.520, rewarded fraction: 0.486, relative distance: 171.005, critic loss: 0.949, actor loss: 25.760\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 9.586, mean reward: 4.871, reward rate: 0.508, rewarded fraction: 0.467, relative distance: 174.471, critic loss: 0.944, actor loss: 25.741\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 9.987, mean reward: 5.177, reward rate: 0.518, rewarded fraction: 0.496, relative distance: 161.341, critic loss: 0.942, actor loss: 25.734\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 10.157, mean reward: 5.342, reward rate: 0.526, rewarded fraction: 0.518, relative distance: 161.433, critic loss: 0.917, actor loss: 25.752\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 10.019, mean reward: 5.366, reward rate: 0.536, rewarded fraction: 0.521, relative distance: 162.130, critic loss: 0.937, actor loss: 25.702\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 9.509, mean reward: 4.985, reward rate: 0.524, rewarded fraction: 0.479, relative distance: 172.702, critic loss: 0.936, actor loss: 25.691\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 10.035, mean reward: 5.251, reward rate: 0.523, rewarded fraction: 0.503, relative distance: 160.209, critic loss: 0.985, actor loss: 25.634\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 9.678, mean reward: 5.061, reward rate: 0.523, rewarded fraction: 0.490, relative distance: 169.376, critic loss: 0.920, actor loss: 25.574\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 9.877, mean reward: 5.140, reward rate: 0.520, rewarded fraction: 0.494, relative distance: 164.832, critic loss: 0.970, actor loss: 25.589\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 8.741, mean reward: 4.539, reward rate: 0.519, rewarded fraction: 0.437, relative distance: 186.053, critic loss: 0.954, actor loss: 25.537\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 9.499, mean reward: 5.068, reward rate: 0.534, rewarded fraction: 0.490, relative distance: 173.507, critic loss: 0.942, actor loss: 25.561\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 9.169, mean reward: 4.781, reward rate: 0.521, rewarded fraction: 0.464, relative distance: 180.861, critic loss: 0.957, actor loss: 25.517\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 9.201, mean reward: 4.822, reward rate: 0.524, rewarded fraction: 0.466, relative distance: 178.102, critic loss: 0.943, actor loss: 25.564\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 8.977, mean reward: 4.681, reward rate: 0.521, rewarded fraction: 0.451, relative distance: 182.414, critic loss: 0.932, actor loss: 25.536\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 9.210, mean reward: 4.811, reward rate: 0.522, rewarded fraction: 0.464, relative distance: 177.192, critic loss: 0.994, actor loss: 25.552\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 9.198, mean reward: 4.709, reward rate: 0.512, rewarded fraction: 0.452, relative distance: 179.310, critic loss: 0.952, actor loss: 25.517\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 9.377, mean reward: 4.963, reward rate: 0.529, rewarded fraction: 0.479, relative distance: 173.554, critic loss: 0.972, actor loss: 25.518\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 9.957, mean reward: 5.071, reward rate: 0.509, rewarded fraction: 0.489, relative distance: 166.038, critic loss: 0.966, actor loss: 25.520\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 9.287, mean reward: 4.764, reward rate: 0.513, rewarded fraction: 0.459, relative distance: 176.911, critic loss: 1.016, actor loss: 25.468\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 9.300, mean reward: 4.972, reward rate: 0.535, rewarded fraction: 0.481, relative distance: 172.200, critic loss: 0.941, actor loss: 25.472\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 8.915, mean reward: 4.698, reward rate: 0.527, rewarded fraction: 0.451, relative distance: 179.599, critic loss: 1.059, actor loss: 25.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 9.609, mean reward: 5.026, reward rate: 0.523, rewarded fraction: 0.484, relative distance: 169.523, critic loss: 0.955, actor loss: 25.436\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 8.930, mean reward: 4.789, reward rate: 0.536, rewarded fraction: 0.460, relative distance: 174.755, critic loss: 1.042, actor loss: 25.424\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 9.042, mean reward: 4.693, reward rate: 0.519, rewarded fraction: 0.449, relative distance: 177.834, critic loss: 1.010, actor loss: 25.399\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 8.321, mean reward: 4.470, reward rate: 0.537, rewarded fraction: 0.434, relative distance: 192.706, critic loss: 0.953, actor loss: 25.404\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 8.643, mean reward: 4.542, reward rate: 0.526, rewarded fraction: 0.433, relative distance: 180.510, critic loss: 1.061, actor loss: 25.371\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 358.513, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 398.848, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 4.600, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 389.894, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 14.465, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 338.057, critic loss: 0.000, actor loss: 0.005\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.128, mean reward: 0.027, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 341.375, critic loss: 0.000, actor loss: 0.046\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 15.454, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 334.295, critic loss: 0.002, actor loss: 0.052\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.537, mean reward: 0.057, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 341.937, critic loss: 0.006, actor loss: 0.044\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 17.111, mean reward: 0.044, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 339.685, critic loss: 0.008, actor loss: 0.037\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.345, mean reward: 0.038, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 342.010, critic loss: 0.009, actor loss: 0.028\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.994, mean reward: 0.039, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 342.555, critic loss: 0.012, actor loss: 0.022\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.421, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 339.349, critic loss: 0.011, actor loss: 0.021\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.666, mean reward: 0.045, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 347.554, critic loss: 0.013, actor loss: 0.019\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.185, mean reward: 0.034, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 345.388, critic loss: 0.015, actor loss: 0.017\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 16.834, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 351.533, critic loss: 0.012, actor loss: 0.013\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.552, mean reward: 0.026, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 358.327, critic loss: 0.012, actor loss: 0.007\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.839, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 350.145, critic loss: 0.013, actor loss: 0.004\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 18.206, mean reward: 0.023, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 357.240, critic loss: 0.011, actor loss: 0.001\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 17.617, mean reward: 0.103, reward rate: 0.006, rewarded fraction: 0.008, relative distance: 341.145, critic loss: 0.013, actor loss: -0.005\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 10.857, mean reward: 0.023, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 316.101, critic loss: 0.014, actor loss: 0.013\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 10.462, mean reward: 0.024, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 304.824, critic loss: 0.016, actor loss: 0.132\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 13.797, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 357.964, critic loss: 0.017, actor loss: 0.215\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 12.013, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 350.943, critic loss: 0.014, actor loss: 0.348\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 12.243, mean reward: 0.060, reward rate: 0.005, rewarded fraction: 0.003, relative distance: 304.156, critic loss: 0.014, actor loss: 0.448\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 14.495, mean reward: 0.149, reward rate: 0.010, rewarded fraction: 0.007, relative distance: 270.089, critic loss: 0.016, actor loss: 0.533\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 15.246, mean reward: 0.406, reward rate: 0.027, rewarded fraction: 0.025, relative distance: 232.584, critic loss: 0.026, actor loss: 0.633\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.902, mean reward: 0.691, reward rate: 0.041, rewarded fraction: 0.056, relative distance: 240.130, critic loss: 0.040, actor loss: 1.112\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.969, mean reward: 0.516, reward rate: 0.030, rewarded fraction: 0.033, relative distance: 233.511, critic loss: 0.049, actor loss: 1.379\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.442, mean reward: 0.532, reward rate: 0.032, rewarded fraction: 0.043, relative distance: 254.785, critic loss: 0.062, actor loss: 1.268\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.208, mean reward: 0.651, reward rate: 0.040, rewarded fraction: 0.050, relative distance: 235.291, critic loss: 0.065, actor loss: 1.029\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.526, mean reward: 0.777, reward rate: 0.047, rewarded fraction: 0.058, relative distance: 222.609, critic loss: 0.070, actor loss: 0.833\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.282, mean reward: 0.911, reward rate: 0.056, rewarded fraction: 0.074, relative distance: 216.331, critic loss: 0.077, actor loss: 0.666\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 16.980, mean reward: 1.037, reward rate: 0.061, rewarded fraction: 0.083, relative distance: 213.762, critic loss: 0.082, actor loss: 0.542\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 16.375, mean reward: 0.869, reward rate: 0.053, rewarded fraction: 0.072, relative distance: 223.895, critic loss: 0.081, actor loss: 0.439\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 16.631, mean reward: 0.977, reward rate: 0.059, rewarded fraction: 0.078, relative distance: 214.365, critic loss: 0.074, actor loss: 0.342\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.848, mean reward: 0.797, reward rate: 0.047, rewarded fraction: 0.064, relative distance: 220.427, critic loss: 0.069, actor loss: 0.266\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 10.492, mean reward: 0.298, reward rate: 0.028, rewarded fraction: 0.020, relative distance: 235.939, critic loss: 0.135, actor loss: 0.336\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 8.452, mean reward: 0.609, reward rate: 0.072, rewarded fraction: 0.046, relative distance: 220.841, critic loss: 0.153, actor loss: 0.542\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 5.808, mean reward: 0.743, reward rate: 0.128, rewarded fraction: 0.060, relative distance: 235.661, critic loss: 0.460, actor loss: 1.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 6.457, mean reward: 1.120, reward rate: 0.173, rewarded fraction: 0.099, relative distance: 225.847, critic loss: 0.610, actor loss: 2.222\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 6.645, mean reward: 1.265, reward rate: 0.190, rewarded fraction: 0.112, relative distance: 222.644, critic loss: 0.677, actor loss: 3.460\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 6.660, mean reward: 1.140, reward rate: 0.171, rewarded fraction: 0.098, relative distance: 222.775, critic loss: 0.674, actor loss: 4.769\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 6.614, mean reward: 1.243, reward rate: 0.188, rewarded fraction: 0.109, relative distance: 223.310, critic loss: 0.759, actor loss: 6.104\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 6.732, mean reward: 1.423, reward rate: 0.211, rewarded fraction: 0.126, relative distance: 219.282, critic loss: 0.899, actor loss: 7.230\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 8.080, mean reward: 2.739, reward rate: 0.339, rewarded fraction: 0.258, relative distance: 192.957, critic loss: 1.041, actor loss: 8.279\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 8.353, mean reward: 3.250, reward rate: 0.389, rewarded fraction: 0.310, relative distance: 184.942, critic loss: 1.080, actor loss: 9.326\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 8.671, mean reward: 3.602, reward rate: 0.415, rewarded fraction: 0.347, relative distance: 177.778, critic loss: 1.116, actor loss: 10.348\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 9.043, mean reward: 3.733, reward rate: 0.413, rewarded fraction: 0.360, relative distance: 181.026, critic loss: 1.158, actor loss: 11.437\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 10.120, mean reward: 4.593, reward rate: 0.454, rewarded fraction: 0.446, relative distance: 161.387, critic loss: 1.192, actor loss: 12.542\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 10.957, mean reward: 5.075, reward rate: 0.463, rewarded fraction: 0.494, relative distance: 149.890, critic loss: 1.182, actor loss: 13.712\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 12.395, mean reward: 5.841, reward rate: 0.471, rewarded fraction: 0.565, relative distance: 130.075, critic loss: 1.126, actor loss: 14.857\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 13.455, mean reward: 6.454, reward rate: 0.480, rewarded fraction: 0.626, relative distance: 115.789, critic loss: 1.040, actor loss: 15.938\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 14.188, mean reward: 6.568, reward rate: 0.463, rewarded fraction: 0.627, relative distance: 104.105, critic loss: 1.004, actor loss: 16.992\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 15.601, mean reward: 7.276, reward rate: 0.466, rewarded fraction: 0.699, relative distance: 86.225, critic loss: 0.963, actor loss: 17.895\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 14.998, mean reward: 7.595, reward rate: 0.506, rewarded fraction: 0.735, relative distance: 83.550, critic loss: 0.939, actor loss: 18.692\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 15.084, mean reward: 7.632, reward rate: 0.506, rewarded fraction: 0.736, relative distance: 78.260, critic loss: 0.953, actor loss: 19.367\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 15.120, mean reward: 7.619, reward rate: 0.504, rewarded fraction: 0.738, relative distance: 81.253, critic loss: 0.950, actor loss: 19.972\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 15.170, mean reward: 7.608, reward rate: 0.502, rewarded fraction: 0.738, relative distance: 82.274, critic loss: 0.971, actor loss: 20.550\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 14.469, mean reward: 7.400, reward rate: 0.511, rewarded fraction: 0.722, relative distance: 91.820, critic loss: 0.965, actor loss: 21.096\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 13.216, mean reward: 6.783, reward rate: 0.513, rewarded fraction: 0.662, relative distance: 113.713, critic loss: 0.940, actor loss: 21.562\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.922, mean reward: 6.500, reward rate: 0.503, rewarded fraction: 0.629, relative distance: 118.928, critic loss: 0.928, actor loss: 22.010\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 13.001, mean reward: 6.430, reward rate: 0.495, rewarded fraction: 0.619, relative distance: 116.183, critic loss: 0.938, actor loss: 22.447\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 13.733, mean reward: 7.169, reward rate: 0.522, rewarded fraction: 0.700, relative distance: 103.100, critic loss: 0.935, actor loss: 22.813\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.694, mean reward: 6.948, reward rate: 0.507, rewarded fraction: 0.670, relative distance: 102.130, critic loss: 0.929, actor loss: 23.180\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 13.979, mean reward: 7.327, reward rate: 0.524, rewarded fraction: 0.714, relative distance: 97.069, critic loss: 0.939, actor loss: 23.546\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 14.462, mean reward: 7.143, reward rate: 0.494, rewarded fraction: 0.688, relative distance: 95.424, critic loss: 0.929, actor loss: 23.856\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 14.555, mean reward: 7.619, reward rate: 0.523, rewarded fraction: 0.739, relative distance: 85.791, critic loss: 0.927, actor loss: 24.204\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 14.100, mean reward: 7.354, reward rate: 0.522, rewarded fraction: 0.716, relative distance: 95.563, critic loss: 0.928, actor loss: 24.427\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 14.856, mean reward: 7.679, reward rate: 0.517, rewarded fraction: 0.747, relative distance: 84.981, critic loss: 0.917, actor loss: 24.659\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 14.574, mean reward: 7.397, reward rate: 0.508, rewarded fraction: 0.718, relative distance: 92.137, critic loss: 0.920, actor loss: 24.879\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 14.167, mean reward: 7.073, reward rate: 0.499, rewarded fraction: 0.686, relative distance: 102.230, critic loss: 0.915, actor loss: 25.112\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 14.362, mean reward: 7.445, reward rate: 0.518, rewarded fraction: 0.722, relative distance: 89.150, critic loss: 0.922, actor loss: 25.341\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 14.057, mean reward: 7.008, reward rate: 0.499, rewarded fraction: 0.676, relative distance: 101.575, critic loss: 0.911, actor loss: 25.473\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 13.997, mean reward: 7.156, reward rate: 0.511, rewarded fraction: 0.694, relative distance: 99.439, critic loss: 0.915, actor loss: 25.625\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 14.243, mean reward: 7.165, reward rate: 0.503, rewarded fraction: 0.697, relative distance: 101.164, critic loss: 0.910, actor loss: 25.786\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 14.487, mean reward: 7.033, reward rate: 0.485, rewarded fraction: 0.678, relative distance: 100.179, critic loss: 0.906, actor loss: 25.931\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 13.663, mean reward: 7.096, reward rate: 0.519, rewarded fraction: 0.688, relative distance: 100.912, critic loss: 0.912, actor loss: 25.992\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.390, mean reward: 7.490, reward rate: 0.521, rewarded fraction: 0.727, relative distance: 90.039, critic loss: 0.900, actor loss: 26.092\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 14.515, mean reward: 7.342, reward rate: 0.506, rewarded fraction: 0.714, relative distance: 95.740, critic loss: 0.912, actor loss: 26.226\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 14.537, mean reward: 7.382, reward rate: 0.508, rewarded fraction: 0.714, relative distance: 92.924, critic loss: 0.900, actor loss: 26.222\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 14.691, mean reward: 7.697, reward rate: 0.524, rewarded fraction: 0.749, relative distance: 85.541, critic loss: 0.908, actor loss: 26.338\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 13.979, mean reward: 7.043, reward rate: 0.504, rewarded fraction: 0.679, relative distance: 101.388, critic loss: 0.902, actor loss: 26.366\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 13.856, mean reward: 7.137, reward rate: 0.515, rewarded fraction: 0.690, relative distance: 99.134, critic loss: 0.896, actor loss: 26.411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 14.736, mean reward: 7.478, reward rate: 0.507, rewarded fraction: 0.723, relative distance: 86.159, critic loss: 0.890, actor loss: 26.431\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 14.247, mean reward: 7.233, reward rate: 0.508, rewarded fraction: 0.699, relative distance: 95.749, critic loss: 0.883, actor loss: 26.448\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 14.371, mean reward: 7.557, reward rate: 0.526, rewarded fraction: 0.736, relative distance: 91.089, critic loss: 0.895, actor loss: 26.463\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 13.703, mean reward: 7.044, reward rate: 0.514, rewarded fraction: 0.684, relative distance: 105.175, critic loss: 0.904, actor loss: 26.477\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 14.432, mean reward: 7.303, reward rate: 0.506, rewarded fraction: 0.710, relative distance: 95.798, critic loss: 0.895, actor loss: 26.528\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 13.970, mean reward: 7.437, reward rate: 0.532, rewarded fraction: 0.724, relative distance: 93.236, critic loss: 0.907, actor loss: 26.509\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 13.535, mean reward: 7.175, reward rate: 0.530, rewarded fraction: 0.699, relative distance: 102.263, critic loss: 0.916, actor loss: 26.526\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 14.163, mean reward: 7.365, reward rate: 0.520, rewarded fraction: 0.712, relative distance: 92.048, critic loss: 0.910, actor loss: 26.544\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 13.982, mean reward: 7.172, reward rate: 0.513, rewarded fraction: 0.700, relative distance: 102.910, critic loss: 0.916, actor loss: 26.568\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 14.139, mean reward: 7.183, reward rate: 0.508, rewarded fraction: 0.695, relative distance: 98.604, critic loss: 0.906, actor loss: 26.606\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 13.637, mean reward: 7.141, reward rate: 0.524, rewarded fraction: 0.695, relative distance: 102.876, critic loss: 0.926, actor loss: 26.624\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.895, mean reward: 7.547, reward rate: 0.543, rewarded fraction: 0.732, relative distance: 89.319, critic loss: 0.901, actor loss: 26.592\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 14.749, mean reward: 7.601, reward rate: 0.515, rewarded fraction: 0.739, relative distance: 88.787, critic loss: 0.869, actor loss: 26.587\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 14.131, mean reward: 7.166, reward rate: 0.507, rewarded fraction: 0.693, relative distance: 97.712, critic loss: 0.882, actor loss: 26.634\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.917, mean reward: 6.981, reward rate: 0.502, rewarded fraction: 0.674, relative distance: 104.252, critic loss: 0.929, actor loss: 26.638\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 14.589, mean reward: 7.434, reward rate: 0.510, rewarded fraction: 0.720, relative distance: 89.512, critic loss: 0.886, actor loss: 26.594\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 14.541, mean reward: 7.443, reward rate: 0.512, rewarded fraction: 0.722, relative distance: 91.881, critic loss: 0.909, actor loss: 26.640\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 36.500, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 191.710, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 26.333, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 353.202, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 8.000, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 353.579, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 335.249, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.959, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 403.982, critic loss: 0.000, actor loss: 0.009\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.950, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.023, critic loss: 0.000, actor loss: -0.009\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.488, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 420.853, critic loss: 0.000, actor loss: -0.018\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.016, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 427.452, critic loss: 0.000, actor loss: -0.026\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.112, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 421.267, critic loss: 0.000, actor loss: -0.030\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 15.568, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.900, critic loss: 0.000, actor loss: -0.032\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.547, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.674, critic loss: 0.000, actor loss: -0.031\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.731, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 430.267, critic loss: 0.000, actor loss: -0.031\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 17.232, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.918, critic loss: 0.000, actor loss: -0.029\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 15.239, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 416.480, critic loss: 0.000, actor loss: -0.028\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 16.044, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 431.985, critic loss: 0.001, actor loss: -0.026\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.331, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 430.201, critic loss: 0.002, actor loss: -0.025\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 15.723, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 424.105, critic loss: 0.002, actor loss: -0.024\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 15.329, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 414.549, critic loss: 0.001, actor loss: -0.024\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.434, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.223, critic loss: 0.001, actor loss: -0.025\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.554, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.745, critic loss: 0.001, actor loss: -0.025\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 15.858, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 416.792, critic loss: 0.001, actor loss: -0.025\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.869, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 427.477, critic loss: 0.001, actor loss: -0.025\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 15.560, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 416.843, critic loss: 0.001, actor loss: -0.025\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 15.564, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 415.541, critic loss: 0.001, actor loss: -0.025\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 15.609, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 410.827, critic loss: 0.001, actor loss: -0.025\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.476, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 414.815, critic loss: 0.001, actor loss: -0.024\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 15.949, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 416.825, critic loss: 0.001, actor loss: -0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.872, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.786, critic loss: 0.001, actor loss: -0.023\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.220, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 412.450, critic loss: 0.000, actor loss: -0.022\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.162, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 417.691, critic loss: 0.001, actor loss: -0.021\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.520, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 413.131, critic loss: 0.001, actor loss: -0.020\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 15.499, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 403.869, critic loss: 0.001, actor loss: -0.019\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 15.625, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 409.404, critic loss: 0.001, actor loss: -0.018\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 16.441, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 412.024, critic loss: 0.001, actor loss: -0.017\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 16.950, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 414.286, critic loss: 0.002, actor loss: -0.016\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.163, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 409.906, critic loss: 0.001, actor loss: -0.015\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 17.412, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 417.782, critic loss: 0.002, actor loss: -0.014\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 16.535, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 410.950, critic loss: 0.002, actor loss: -0.013\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 18.503, mean reward: 0.040, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 430.191, critic loss: 0.002, actor loss: -0.012\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 17.659, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 420.643, critic loss: 0.003, actor loss: -0.011\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 17.061, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 411.088, critic loss: 0.002, actor loss: -0.011\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 16.017, mean reward: 0.049, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 414.561, critic loss: 0.003, actor loss: -0.011\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 17.176, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.440, critic loss: 0.003, actor loss: -0.010\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 16.305, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 400.698, critic loss: 0.004, actor loss: -0.010\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 16.786, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 404.715, critic loss: 0.004, actor loss: -0.009\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 17.019, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 413.208, critic loss: 0.003, actor loss: -0.008\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 16.988, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 398.604, critic loss: 0.004, actor loss: -0.007\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 17.111, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 409.381, critic loss: 0.003, actor loss: -0.007\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.493, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 402.857, critic loss: 0.003, actor loss: -0.007\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 15.393, mean reward: 0.034, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 395.989, critic loss: 0.003, actor loss: -0.007\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 16.416, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 398.053, critic loss: 0.004, actor loss: -0.007\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 15.975, mean reward: 0.030, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 403.578, critic loss: 0.005, actor loss: -0.006\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 16.474, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 403.477, critic loss: 0.004, actor loss: -0.006\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 16.567, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 403.308, critic loss: 0.004, actor loss: -0.005\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 17.135, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 403.361, critic loss: 0.004, actor loss: -0.005\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 16.992, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 407.096, critic loss: 0.004, actor loss: -0.006\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 18.470, mean reward: 0.057, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 402.611, critic loss: 0.004, actor loss: -0.007\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 16.596, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 406.844, critic loss: 0.004, actor loss: -0.007\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 17.296, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 415.611, critic loss: 0.005, actor loss: -0.008\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 15.925, mean reward: 0.031, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 395.741, critic loss: 0.004, actor loss: -0.008\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 16.763, mean reward: 0.053, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 401.748, critic loss: 0.006, actor loss: -0.009\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 15.674, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 399.876, critic loss: 0.005, actor loss: -0.010\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 17.438, mean reward: 0.045, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 404.810, critic loss: 0.005, actor loss: -0.011\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 16.262, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 407.900, critic loss: 0.005, actor loss: -0.012\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 16.123, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 404.247, critic loss: 0.005, actor loss: -0.012\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 17.126, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 401.842, critic loss: 0.007, actor loss: -0.012\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 15.745, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 398.142, critic loss: 0.006, actor loss: -0.012\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 16.793, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 410.268, critic loss: 0.005, actor loss: -0.012\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 16.638, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 392.771, critic loss: 0.005, actor loss: -0.013\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 16.924, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 395.805, critic loss: 0.005, actor loss: -0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 16.731, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 383.405, critic loss: 0.005, actor loss: -0.011\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 16.025, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 387.335, critic loss: 0.004, actor loss: -0.011\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 16.379, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 384.424, critic loss: 0.005, actor loss: -0.011\n",
      "t: 740000, Ep: 7399, action std: 0.80\n",
      "mean steps: 16.642, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 394.216, critic loss: 0.004, actor loss: -0.010\n",
      "t: 750000, Ep: 7499, action std: 0.80\n",
      "mean steps: 16.167, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 392.842, critic loss: 0.005, actor loss: -0.009\n",
      "t: 760000, Ep: 7599, action std: 0.80\n",
      "mean steps: 15.528, mean reward: 0.033, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 386.390, critic loss: 0.004, actor loss: -0.008\n",
      "t: 770000, Ep: 7699, action std: 0.80\n",
      "mean steps: 16.428, mean reward: 0.056, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 384.501, critic loss: 0.005, actor loss: -0.008\n",
      "t: 780000, Ep: 7799, action std: 0.80\n",
      "mean steps: 16.916, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 388.317, critic loss: 0.005, actor loss: -0.006\n",
      "t: 790000, Ep: 7899, action std: 0.80\n",
      "mean steps: 16.399, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 390.946, critic loss: 0.005, actor loss: -0.005\n",
      "t: 800000, Ep: 7999, action std: 0.80\n",
      "mean steps: 15.562, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 396.162, critic loss: 0.005, actor loss: -0.005\n",
      "t: 810000, Ep: 8099, action std: 0.80\n",
      "mean steps: 15.256, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 386.849, critic loss: 0.006, actor loss: -0.005\n",
      "t: 820000, Ep: 8199, action std: 0.80\n",
      "mean steps: 16.374, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 387.249, critic loss: 0.005, actor loss: -0.004\n",
      "t: 830000, Ep: 8299, action std: 0.80\n",
      "mean steps: 16.534, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 383.443, critic loss: 0.006, actor loss: -0.003\n",
      "t: 840000, Ep: 8399, action std: 0.80\n",
      "mean steps: 17.369, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 389.856, critic loss: 0.005, actor loss: -0.003\n",
      "t: 850000, Ep: 8499, action std: 0.80\n",
      "mean steps: 17.632, mean reward: 0.041, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 379.994, critic loss: 0.006, actor loss: -0.004\n",
      "t: 860000, Ep: 8599, action std: 0.80\n",
      "mean steps: 16.240, mean reward: 0.033, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 387.901, critic loss: 0.007, actor loss: -0.004\n",
      "t: 870000, Ep: 8699, action std: 0.80\n",
      "mean steps: 11.027, mean reward: 0.018, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 323.054, critic loss: 0.006, actor loss: 0.003\n",
      "t: 880000, Ep: 8799, action std: 0.80\n",
      "mean steps: 6.616, mean reward: 0.028, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 285.604, critic loss: 0.007, actor loss: 0.029\n",
      "t: 890000, Ep: 8899, action std: 0.80\n",
      "mean steps: 5.482, mean reward: 0.030, reward rate: 0.005, rewarded fraction: 0.000, relative distance: 275.065, critic loss: 0.007, actor loss: 0.037\n",
      "t: 900000, Ep: 8999, action std: 0.80\n",
      "mean steps: 4.885, mean reward: 0.033, reward rate: 0.007, rewarded fraction: 0.000, relative distance: 278.065, critic loss: 0.007, actor loss: 0.044\n",
      "t: 910000, Ep: 9099, action std: 0.80\n",
      "mean steps: 4.806, mean reward: 0.028, reward rate: 0.006, rewarded fraction: 0.000, relative distance: 277.252, critic loss: 0.007, actor loss: 0.059\n",
      "t: 920000, Ep: 9199, action std: 0.80\n",
      "mean steps: 5.418, mean reward: 0.048, reward rate: 0.009, rewarded fraction: 0.001, relative distance: 271.984, critic loss: 0.006, actor loss: 0.074\n",
      "t: 930000, Ep: 9299, action std: 0.80\n",
      "mean steps: 9.187, mean reward: 0.146, reward rate: 0.016, rewarded fraction: 0.007, relative distance: 245.920, critic loss: 0.007, actor loss: 0.118\n",
      "t: 940000, Ep: 9399, action std: 0.80\n",
      "mean steps: 6.788, mean reward: 0.103, reward rate: 0.015, rewarded fraction: 0.004, relative distance: 249.794, critic loss: 0.009, actor loss: 0.187\n",
      "t: 950000, Ep: 9499, action std: 0.80\n",
      "mean steps: 10.578, mean reward: 0.368, reward rate: 0.035, rewarded fraction: 0.027, relative distance: 235.474, critic loss: 0.011, actor loss: 0.328\n",
      "t: 960000, Ep: 9599, action std: 0.80\n",
      "mean steps: 9.513, mean reward: 0.393, reward rate: 0.041, rewarded fraction: 0.029, relative distance: 227.925, critic loss: 0.017, actor loss: 0.515\n",
      "t: 970000, Ep: 9699, action std: 0.80\n",
      "mean steps: 11.044, mean reward: 0.585, reward rate: 0.053, rewarded fraction: 0.042, relative distance: 220.033, critic loss: 0.024, actor loss: 0.813\n",
      "t: 980000, Ep: 9799, action std: 0.80\n",
      "mean steps: 13.893, mean reward: 0.713, reward rate: 0.051, rewarded fraction: 0.056, relative distance: 217.220, critic loss: 0.035, actor loss: 1.233\n",
      "t: 990000, Ep: 9899, action std: 0.80\n",
      "mean steps: 16.525, mean reward: 0.758, reward rate: 0.046, rewarded fraction: 0.060, relative distance: 224.117, critic loss: 0.041, actor loss: 1.735\n",
      "t: 1000000, Ep: 9999, action std: 0.80\n",
      "mean steps: 15.705, mean reward: 0.740, reward rate: 0.047, rewarded fraction: 0.064, relative distance: 232.226, critic loss: 0.047, actor loss: 2.154\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 4.980, mean reward: 0.014, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 351.309, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 4.854, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 357.023, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 4.875, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 363.257, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 4.592, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 352.557, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 7.865, mean reward: 0.024, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 316.263, critic loss: 0.000, actor loss: 0.014\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 7.827, mean reward: 0.025, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 317.883, critic loss: 0.001, actor loss: 0.016\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 4.142, mean reward: 0.032, reward rate: 0.008, rewarded fraction: 0.000, relative distance: 276.752, critic loss: 0.001, actor loss: 0.055\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 3.879, mean reward: 0.038, reward rate: 0.010, rewarded fraction: 0.000, relative distance: 274.224, critic loss: 0.003, actor loss: 0.097\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 4.721, mean reward: 0.041, reward rate: 0.009, rewarded fraction: 0.000, relative distance: 272.373, critic loss: 0.005, actor loss: 0.142\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 12.241, mean reward: 0.164, reward rate: 0.013, rewarded fraction: 0.007, relative distance: 234.348, critic loss: 0.008, actor loss: 0.180\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 7.038, mean reward: 0.158, reward rate: 0.022, rewarded fraction: 0.007, relative distance: 231.005, critic loss: 0.018, actor loss: 0.259\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 8.132, mean reward: 0.296, reward rate: 0.036, rewarded fraction: 0.019, relative distance: 223.968, critic loss: 0.049, actor loss: 0.423\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 12.264, mean reward: 0.515, reward rate: 0.042, rewarded fraction: 0.033, relative distance: 208.156, critic loss: 0.076, actor loss: 0.775\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 12.902, mean reward: 0.836, reward rate: 0.065, rewarded fraction: 0.065, relative distance: 204.322, critic loss: 0.101, actor loss: 1.124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 11.096, mean reward: 0.954, reward rate: 0.086, rewarded fraction: 0.076, relative distance: 195.604, critic loss: 0.159, actor loss: 1.306\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 8.458, mean reward: 0.886, reward rate: 0.105, rewarded fraction: 0.073, relative distance: 213.057, critic loss: 0.577, actor loss: 1.918\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 7.160, mean reward: 0.792, reward rate: 0.111, rewarded fraction: 0.064, relative distance: 227.627, critic loss: 1.091, actor loss: 2.831\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 8.017, mean reward: 0.924, reward rate: 0.115, rewarded fraction: 0.077, relative distance: 224.445, critic loss: 0.907, actor loss: 3.510\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 7.867, mean reward: 1.146, reward rate: 0.146, rewarded fraction: 0.100, relative distance: 222.050, critic loss: 0.897, actor loss: 4.084\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 7.774, mean reward: 1.025, reward rate: 0.132, rewarded fraction: 0.087, relative distance: 230.973, critic loss: 0.969, actor loss: 4.739\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 7.779, mean reward: 1.215, reward rate: 0.156, rewarded fraction: 0.106, relative distance: 226.876, critic loss: 1.082, actor loss: 5.482\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 7.293, mean reward: 0.977, reward rate: 0.134, rewarded fraction: 0.083, relative distance: 236.476, critic loss: 1.147, actor loss: 6.222\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 7.466, mean reward: 1.302, reward rate: 0.174, rewarded fraction: 0.113, relative distance: 224.289, critic loss: 1.159, actor loss: 6.870\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 7.473, mean reward: 1.451, reward rate: 0.194, rewarded fraction: 0.130, relative distance: 227.464, critic loss: 1.384, actor loss: 7.712\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 7.585, mean reward: 1.505, reward rate: 0.198, rewarded fraction: 0.138, relative distance: 225.448, critic loss: 1.605, actor loss: 8.778\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 7.734, mean reward: 1.690, reward rate: 0.219, rewarded fraction: 0.151, relative distance: 218.578, critic loss: 1.713, actor loss: 9.931\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 9.840, mean reward: 3.293, reward rate: 0.335, rewarded fraction: 0.304, relative distance: 182.918, critic loss: 1.675, actor loss: 11.050\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 9.429, mean reward: 3.270, reward rate: 0.347, rewarded fraction: 0.306, relative distance: 188.563, critic loss: 1.553, actor loss: 12.003\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 9.772, mean reward: 3.484, reward rate: 0.357, rewarded fraction: 0.333, relative distance: 184.121, critic loss: 1.481, actor loss: 12.640\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 10.084, mean reward: 3.695, reward rate: 0.366, rewarded fraction: 0.352, relative distance: 180.542, critic loss: 1.499, actor loss: 13.177\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 9.953, mean reward: 3.826, reward rate: 0.384, rewarded fraction: 0.367, relative distance: 179.142, critic loss: 1.539, actor loss: 13.702\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 10.313, mean reward: 4.287, reward rate: 0.416, rewarded fraction: 0.409, relative distance: 165.163, critic loss: 1.584, actor loss: 14.277\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 10.165, mean reward: 4.057, reward rate: 0.399, rewarded fraction: 0.387, relative distance: 168.153, critic loss: 1.595, actor loss: 14.838\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 10.059, mean reward: 4.151, reward rate: 0.413, rewarded fraction: 0.396, relative distance: 166.302, critic loss: 1.571, actor loss: 15.415\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 10.380, mean reward: 4.377, reward rate: 0.422, rewarded fraction: 0.415, relative distance: 163.090, critic loss: 1.504, actor loss: 15.958\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 10.952, mean reward: 4.784, reward rate: 0.437, rewarded fraction: 0.459, relative distance: 155.415, critic loss: 1.428, actor loss: 16.474\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 11.348, mean reward: 4.928, reward rate: 0.434, rewarded fraction: 0.474, relative distance: 151.402, critic loss: 1.394, actor loss: 16.978\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 11.823, mean reward: 5.401, reward rate: 0.457, rewarded fraction: 0.512, relative distance: 133.311, critic loss: 1.322, actor loss: 17.458\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.432, mean reward: 5.351, reward rate: 0.430, rewarded fraction: 0.509, relative distance: 132.707, critic loss: 1.286, actor loss: 17.958\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 12.449, mean reward: 5.426, reward rate: 0.436, rewarded fraction: 0.516, relative distance: 131.885, critic loss: 1.202, actor loss: 18.417\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 12.789, mean reward: 5.674, reward rate: 0.444, rewarded fraction: 0.539, relative distance: 127.484, critic loss: 1.190, actor loss: 18.819\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 13.186, mean reward: 6.086, reward rate: 0.462, rewarded fraction: 0.587, relative distance: 119.942, critic loss: 1.213, actor loss: 19.205\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.604, mean reward: 5.967, reward rate: 0.473, rewarded fraction: 0.572, relative distance: 123.092, critic loss: 1.128, actor loss: 19.617\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.839, mean reward: 5.982, reward rate: 0.466, rewarded fraction: 0.574, relative distance: 120.607, critic loss: 1.134, actor loss: 20.023\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 13.326, mean reward: 6.505, reward rate: 0.488, rewarded fraction: 0.630, relative distance: 110.900, critic loss: 1.158, actor loss: 20.369\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 13.424, mean reward: 6.091, reward rate: 0.454, rewarded fraction: 0.582, relative distance: 116.114, critic loss: 1.162, actor loss: 20.713\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 13.857, mean reward: 6.694, reward rate: 0.483, rewarded fraction: 0.641, relative distance: 101.409, critic loss: 1.156, actor loss: 21.004\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 13.754, mean reward: 6.700, reward rate: 0.487, rewarded fraction: 0.644, relative distance: 101.105, critic loss: 1.127, actor loss: 21.233\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 13.645, mean reward: 6.887, reward rate: 0.505, rewarded fraction: 0.663, relative distance: 97.866, critic loss: 1.131, actor loss: 21.419\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 14.303, mean reward: 6.863, reward rate: 0.480, rewarded fraction: 0.665, relative distance: 102.536, critic loss: 1.110, actor loss: 21.600\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 14.017, mean reward: 6.645, reward rate: 0.474, rewarded fraction: 0.636, relative distance: 102.882, critic loss: 1.124, actor loss: 21.758\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 14.078, mean reward: 6.542, reward rate: 0.465, rewarded fraction: 0.622, relative distance: 104.332, critic loss: 1.105, actor loss: 21.930\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 13.939, mean reward: 6.738, reward rate: 0.483, rewarded fraction: 0.647, relative distance: 101.727, critic loss: 1.109, actor loss: 22.034\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 13.931, mean reward: 6.872, reward rate: 0.493, rewarded fraction: 0.664, relative distance: 101.378, critic loss: 1.074, actor loss: 22.163\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 14.035, mean reward: 6.950, reward rate: 0.495, rewarded fraction: 0.671, relative distance: 99.067, critic loss: 1.074, actor loss: 22.316\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 14.185, mean reward: 7.023, reward rate: 0.495, rewarded fraction: 0.678, relative distance: 97.074, critic loss: 1.073, actor loss: 22.478\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 14.281, mean reward: 7.260, reward rate: 0.508, rewarded fraction: 0.705, relative distance: 92.309, critic loss: 1.060, actor loss: 22.635\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 14.145, mean reward: 7.037, reward rate: 0.497, rewarded fraction: 0.676, relative distance: 95.456, critic loss: 1.053, actor loss: 22.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 14.882, mean reward: 7.332, reward rate: 0.493, rewarded fraction: 0.710, relative distance: 90.296, critic loss: 1.043, actor loss: 22.888\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 14.709, mean reward: 7.136, reward rate: 0.485, rewarded fraction: 0.689, relative distance: 90.384, critic loss: 1.037, actor loss: 22.982\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 14.923, mean reward: 7.145, reward rate: 0.479, rewarded fraction: 0.685, relative distance: 88.258, critic loss: 1.029, actor loss: 23.123\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 14.497, mean reward: 7.021, reward rate: 0.484, rewarded fraction: 0.671, relative distance: 91.590, critic loss: 1.011, actor loss: 23.236\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 14.736, mean reward: 7.238, reward rate: 0.491, rewarded fraction: 0.698, relative distance: 88.804, critic loss: 1.034, actor loss: 23.346\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 15.375, mean reward: 7.547, reward rate: 0.491, rewarded fraction: 0.728, relative distance: 78.889, critic loss: 1.018, actor loss: 23.394\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 15.086, mean reward: 7.586, reward rate: 0.503, rewarded fraction: 0.733, relative distance: 77.379, critic loss: 1.001, actor loss: 23.474\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 15.052, mean reward: 7.371, reward rate: 0.490, rewarded fraction: 0.710, relative distance: 84.074, critic loss: 1.018, actor loss: 23.561\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 15.656, mean reward: 7.761, reward rate: 0.496, rewarded fraction: 0.750, relative distance: 74.147, critic loss: 1.003, actor loss: 23.634\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 15.913, mean reward: 7.745, reward rate: 0.487, rewarded fraction: 0.742, relative distance: 68.499, critic loss: 1.004, actor loss: 23.720\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 15.449, mean reward: 7.714, reward rate: 0.499, rewarded fraction: 0.745, relative distance: 77.566, critic loss: 0.994, actor loss: 23.814\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 15.230, mean reward: 7.540, reward rate: 0.495, rewarded fraction: 0.729, relative distance: 81.938, critic loss: 0.996, actor loss: 23.847\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 15.785, mean reward: 8.028, reward rate: 0.509, rewarded fraction: 0.773, relative distance: 65.602, critic loss: 1.004, actor loss: 23.892\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 15.321, mean reward: 7.717, reward rate: 0.504, rewarded fraction: 0.746, relative distance: 78.180, critic loss: 1.020, actor loss: 23.908\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 15.350, mean reward: 7.801, reward rate: 0.508, rewarded fraction: 0.753, relative distance: 71.649, critic loss: 1.017, actor loss: 23.935\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 15.424, mean reward: 7.806, reward rate: 0.506, rewarded fraction: 0.753, relative distance: 73.585, critic loss: 1.046, actor loss: 23.927\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 16.109, mean reward: 8.003, reward rate: 0.497, rewarded fraction: 0.772, relative distance: 66.931, critic loss: 1.011, actor loss: 23.905\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 15.641, mean reward: 7.786, reward rate: 0.498, rewarded fraction: 0.747, relative distance: 69.461, critic loss: 0.988, actor loss: 23.888\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 15.860, mean reward: 8.090, reward rate: 0.510, rewarded fraction: 0.784, relative distance: 67.650, critic loss: 0.990, actor loss: 23.866\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 15.823, mean reward: 7.770, reward rate: 0.491, rewarded fraction: 0.748, relative distance: 71.446, critic loss: 1.016, actor loss: 23.866\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 15.213, mean reward: 7.947, reward rate: 0.522, rewarded fraction: 0.771, relative distance: 72.343, critic loss: 0.994, actor loss: 23.893\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 16.458, mean reward: 8.166, reward rate: 0.496, rewarded fraction: 0.795, relative distance: 65.959, critic loss: 0.992, actor loss: 23.842\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 15.770, mean reward: 7.607, reward rate: 0.482, rewarded fraction: 0.728, relative distance: 74.787, critic loss: 0.984, actor loss: 23.788\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 15.663, mean reward: 7.625, reward rate: 0.487, rewarded fraction: 0.733, relative distance: 75.163, critic loss: 0.977, actor loss: 23.795\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 16.073, mean reward: 7.849, reward rate: 0.488, rewarded fraction: 0.755, relative distance: 69.460, critic loss: 1.009, actor loss: 23.769\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 16.170, mean reward: 8.213, reward rate: 0.508, rewarded fraction: 0.799, relative distance: 67.361, critic loss: 0.998, actor loss: 23.743\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 15.977, mean reward: 8.141, reward rate: 0.510, rewarded fraction: 0.790, relative distance: 65.181, critic loss: 0.959, actor loss: 23.759\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 16.023, mean reward: 8.059, reward rate: 0.503, rewarded fraction: 0.780, relative distance: 69.310, critic loss: 0.970, actor loss: 23.776\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 16.146, mean reward: 7.773, reward rate: 0.481, rewarded fraction: 0.748, relative distance: 70.529, critic loss: 0.956, actor loss: 23.762\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 15.854, mean reward: 7.974, reward rate: 0.503, rewarded fraction: 0.772, relative distance: 71.971, critic loss: 0.952, actor loss: 23.761\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 15.470, mean reward: 7.790, reward rate: 0.504, rewarded fraction: 0.754, relative distance: 77.504, critic loss: 0.964, actor loss: 23.705\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 16.093, mean reward: 7.947, reward rate: 0.494, rewarded fraction: 0.771, relative distance: 70.009, critic loss: 0.973, actor loss: 23.645\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 16.151, mean reward: 8.257, reward rate: 0.511, rewarded fraction: 0.801, relative distance: 64.022, critic loss: 0.938, actor loss: 23.584\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 16.518, mean reward: 8.258, reward rate: 0.500, rewarded fraction: 0.798, relative distance: 60.348, critic loss: 0.931, actor loss: 23.595\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 16.054, mean reward: 8.169, reward rate: 0.509, rewarded fraction: 0.791, relative distance: 62.826, critic loss: 0.953, actor loss: 23.594\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 16.113, mean reward: 8.066, reward rate: 0.501, rewarded fraction: 0.773, relative distance: 63.160, critic loss: 0.966, actor loss: 23.508\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 15.805, mean reward: 7.834, reward rate: 0.496, rewarded fraction: 0.758, relative distance: 73.066, critic loss: 0.933, actor loss: 23.499\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 16.256, mean reward: 8.292, reward rate: 0.510, rewarded fraction: 0.804, relative distance: 60.358, critic loss: 0.936, actor loss: 23.406\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 15.706, mean reward: 8.143, reward rate: 0.518, rewarded fraction: 0.792, relative distance: 69.592, critic loss: 0.929, actor loss: 23.395\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 16.129, mean reward: 8.241, reward rate: 0.511, rewarded fraction: 0.800, relative distance: 63.266, critic loss: 0.959, actor loss: 23.325\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 16.901, mean reward: 8.534, reward rate: 0.505, rewarded fraction: 0.827, relative distance: 55.032, critic loss: 0.927, actor loss: 23.306\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 16.645, mean reward: 8.149, reward rate: 0.490, rewarded fraction: 0.781, relative distance: 60.478, critic loss: 0.951, actor loss: 23.264\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 15.107, mean reward: 0.217, reward rate: 0.014, rewarded fraction: 0.015, relative distance: 266.286, critic loss: 0.014, actor loss: 0.064\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 15.820, mean reward: 0.101, reward rate: 0.006, rewarded fraction: 0.004, relative distance: 276.305, critic loss: 0.030, actor loss: 0.057\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 17.767, mean reward: 0.119, reward rate: 0.007, rewarded fraction: 0.006, relative distance: 272.936, critic loss: 0.034, actor loss: 0.039\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 17.388, mean reward: 0.283, reward rate: 0.016, rewarded fraction: 0.019, relative distance: 264.335, critic loss: 0.038, actor loss: 0.035\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.559, mean reward: 0.292, reward rate: 0.018, rewarded fraction: 0.019, relative distance: 264.347, critic loss: 0.065, actor loss: 0.029\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.655, mean reward: 0.403, reward rate: 0.024, rewarded fraction: 0.029, relative distance: 243.488, critic loss: 0.080, actor loss: 0.019\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 10.224, mean reward: 0.114, reward rate: 0.011, rewarded fraction: 0.006, relative distance: 259.518, critic loss: 0.088, actor loss: 0.047\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 10.582, mean reward: 0.576, reward rate: 0.054, rewarded fraction: 0.046, relative distance: 222.230, critic loss: 0.106, actor loss: 0.246\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 8.721, mean reward: 0.758, reward rate: 0.087, rewarded fraction: 0.062, relative distance: 214.769, critic loss: 0.280, actor loss: 1.156\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 7.189, mean reward: 0.973, reward rate: 0.135, rewarded fraction: 0.083, relative distance: 216.297, critic loss: 0.825, actor loss: 3.042\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 7.786, mean reward: 1.250, reward rate: 0.161, rewarded fraction: 0.112, relative distance: 210.835, critic loss: 0.904, actor loss: 4.564\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 7.763, mean reward: 1.386, reward rate: 0.178, rewarded fraction: 0.122, relative distance: 214.062, critic loss: 0.759, actor loss: 5.625\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 7.764, mean reward: 1.328, reward rate: 0.171, rewarded fraction: 0.116, relative distance: 218.150, critic loss: 0.852, actor loss: 6.625\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 7.677, mean reward: 1.386, reward rate: 0.181, rewarded fraction: 0.122, relative distance: 221.723, critic loss: 1.019, actor loss: 7.605\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 7.213, mean reward: 1.830, reward rate: 0.254, rewarded fraction: 0.169, relative distance: 219.484, critic loss: 1.099, actor loss: 8.649\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 9.278, mean reward: 4.011, reward rate: 0.432, rewarded fraction: 0.388, relative distance: 178.459, critic loss: 1.162, actor loss: 9.845\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 11.172, mean reward: 5.105, reward rate: 0.457, rewarded fraction: 0.489, relative distance: 146.047, critic loss: 1.246, actor loss: 11.324\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 14.023, mean reward: 6.097, reward rate: 0.435, rewarded fraction: 0.574, relative distance: 103.924, critic loss: 1.210, actor loss: 13.048\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 13.854, mean reward: 6.364, reward rate: 0.459, rewarded fraction: 0.603, relative distance: 106.856, critic loss: 1.087, actor loss: 14.642\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 13.021, mean reward: 5.997, reward rate: 0.461, rewarded fraction: 0.573, relative distance: 121.710, critic loss: 1.107, actor loss: 15.824\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 11.507, mean reward: 5.407, reward rate: 0.470, rewarded fraction: 0.520, relative distance: 143.940, critic loss: 1.176, actor loss: 16.834\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 12.281, mean reward: 5.607, reward rate: 0.457, rewarded fraction: 0.540, relative distance: 135.615, critic loss: 1.163, actor loss: 17.692\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 12.299, mean reward: 5.542, reward rate: 0.451, rewarded fraction: 0.530, relative distance: 137.007, critic loss: 1.146, actor loss: 18.505\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 12.728, mean reward: 5.759, reward rate: 0.453, rewarded fraction: 0.550, relative distance: 129.124, critic loss: 1.125, actor loss: 19.223\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 12.623, mean reward: 6.031, reward rate: 0.478, rewarded fraction: 0.581, relative distance: 126.530, critic loss: 1.090, actor loss: 19.873\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 13.496, mean reward: 5.916, reward rate: 0.438, rewarded fraction: 0.566, relative distance: 127.169, critic loss: 1.087, actor loss: 20.493\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 13.280, mean reward: 6.456, reward rate: 0.486, rewarded fraction: 0.619, relative distance: 112.307, critic loss: 1.065, actor loss: 21.050\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 12.990, mean reward: 6.365, reward rate: 0.490, rewarded fraction: 0.611, relative distance: 117.043, critic loss: 1.049, actor loss: 21.605\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 13.045, mean reward: 6.327, reward rate: 0.485, rewarded fraction: 0.609, relative distance: 116.079, critic loss: 1.041, actor loss: 22.029\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 12.407, mean reward: 5.960, reward rate: 0.480, rewarded fraction: 0.575, relative distance: 130.651, critic loss: 1.047, actor loss: 22.433\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 12.866, mean reward: 6.207, reward rate: 0.482, rewarded fraction: 0.600, relative distance: 124.019, critic loss: 1.064, actor loss: 22.813\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 12.309, mean reward: 6.130, reward rate: 0.498, rewarded fraction: 0.595, relative distance: 128.558, critic loss: 1.077, actor loss: 23.183\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 12.281, mean reward: 6.021, reward rate: 0.490, rewarded fraction: 0.581, relative distance: 129.704, critic loss: 1.076, actor loss: 23.577\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.210, mean reward: 5.995, reward rate: 0.491, rewarded fraction: 0.579, relative distance: 130.185, critic loss: 1.077, actor loss: 23.872\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.514, mean reward: 6.436, reward rate: 0.514, rewarded fraction: 0.623, relative distance: 117.276, critic loss: 1.081, actor loss: 24.165\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 11.988, mean reward: 5.731, reward rate: 0.478, rewarded fraction: 0.551, relative distance: 135.680, critic loss: 1.109, actor loss: 24.454\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 12.048, mean reward: 5.770, reward rate: 0.479, rewarded fraction: 0.552, relative distance: 132.597, critic loss: 1.101, actor loss: 24.668\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 12.398, mean reward: 6.026, reward rate: 0.486, rewarded fraction: 0.578, relative distance: 125.826, critic loss: 1.098, actor loss: 24.888\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.433, mean reward: 5.856, reward rate: 0.471, rewarded fraction: 0.561, relative distance: 131.613, critic loss: 1.097, actor loss: 25.089\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.159, mean reward: 6.199, reward rate: 0.510, rewarded fraction: 0.602, relative distance: 126.540, critic loss: 1.096, actor loss: 25.292\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 12.203, mean reward: 6.120, reward rate: 0.502, rewarded fraction: 0.592, relative distance: 127.497, critic loss: 1.111, actor loss: 25.426\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.265, mean reward: 6.255, reward rate: 0.510, rewarded fraction: 0.608, relative distance: 124.457, critic loss: 1.102, actor loss: 25.642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 12.072, mean reward: 5.996, reward rate: 0.497, rewarded fraction: 0.579, relative distance: 129.319, critic loss: 1.123, actor loss: 25.818\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 12.041, mean reward: 6.001, reward rate: 0.498, rewarded fraction: 0.577, relative distance: 128.770, critic loss: 1.146, actor loss: 25.950\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 12.249, mean reward: 5.994, reward rate: 0.489, rewarded fraction: 0.574, relative distance: 126.075, critic loss: 1.150, actor loss: 26.072\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 12.700, mean reward: 6.092, reward rate: 0.480, rewarded fraction: 0.584, relative distance: 121.819, critic loss: 1.155, actor loss: 26.185\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 12.306, mean reward: 6.205, reward rate: 0.504, rewarded fraction: 0.598, relative distance: 124.653, critic loss: 1.144, actor loss: 26.265\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 11.941, mean reward: 5.879, reward rate: 0.492, rewarded fraction: 0.562, relative distance: 127.801, critic loss: 1.141, actor loss: 26.297\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 11.986, mean reward: 6.065, reward rate: 0.506, rewarded fraction: 0.587, relative distance: 128.310, critic loss: 1.142, actor loss: 26.348\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 11.673, mean reward: 5.784, reward rate: 0.496, rewarded fraction: 0.558, relative distance: 134.452, critic loss: 1.137, actor loss: 26.398\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.177, mean reward: 6.129, reward rate: 0.503, rewarded fraction: 0.594, relative distance: 126.148, critic loss: 1.158, actor loss: 26.427\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 12.210, mean reward: 6.063, reward rate: 0.497, rewarded fraction: 0.584, relative distance: 125.281, critic loss: 1.158, actor loss: 26.409\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.154, mean reward: 6.121, reward rate: 0.504, rewarded fraction: 0.591, relative distance: 127.000, critic loss: 1.167, actor loss: 26.424\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.466, mean reward: 6.065, reward rate: 0.487, rewarded fraction: 0.585, relative distance: 128.417, critic loss: 1.164, actor loss: 26.449\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 11.659, mean reward: 5.606, reward rate: 0.481, rewarded fraction: 0.537, relative distance: 139.727, critic loss: 1.175, actor loss: 26.420\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 11.877, mean reward: 6.130, reward rate: 0.516, rewarded fraction: 0.595, relative distance: 128.682, critic loss: 1.165, actor loss: 26.418\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.548, mean reward: 6.389, reward rate: 0.509, rewarded fraction: 0.620, relative distance: 120.829, critic loss: 1.157, actor loss: 26.403\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.182, mean reward: 5.974, reward rate: 0.490, rewarded fraction: 0.576, relative distance: 132.629, critic loss: 1.147, actor loss: 26.375\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 12.075, mean reward: 6.161, reward rate: 0.510, rewarded fraction: 0.596, relative distance: 126.528, critic loss: 1.141, actor loss: 26.359\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.021, mean reward: 6.315, reward rate: 0.485, rewarded fraction: 0.607, relative distance: 120.673, critic loss: 1.166, actor loss: 26.350\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.025, mean reward: 6.152, reward rate: 0.512, rewarded fraction: 0.596, relative distance: 128.110, critic loss: 1.168, actor loss: 26.348\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.209, mean reward: 6.041, reward rate: 0.495, rewarded fraction: 0.583, relative distance: 131.679, critic loss: 1.148, actor loss: 26.314\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.374, mean reward: 6.054, reward rate: 0.489, rewarded fraction: 0.583, relative distance: 126.607, critic loss: 1.161, actor loss: 26.310\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.072, mean reward: 6.222, reward rate: 0.515, rewarded fraction: 0.601, relative distance: 126.062, critic loss: 1.178, actor loss: 26.334\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.552, mean reward: 6.375, reward rate: 0.508, rewarded fraction: 0.618, relative distance: 121.767, critic loss: 1.167, actor loss: 26.280\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.583, mean reward: 6.457, reward rate: 0.513, rewarded fraction: 0.627, relative distance: 118.698, critic loss: 1.153, actor loss: 26.254\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 11.600, mean reward: 5.866, reward rate: 0.506, rewarded fraction: 0.570, relative distance: 136.947, critic loss: 1.165, actor loss: 26.233\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.431, mean reward: 6.209, reward rate: 0.499, rewarded fraction: 0.599, relative distance: 125.949, critic loss: 1.164, actor loss: 26.199\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.257, mean reward: 6.335, reward rate: 0.517, rewarded fraction: 0.614, relative distance: 120.682, critic loss: 1.164, actor loss: 26.208\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.116, mean reward: 6.206, reward rate: 0.512, rewarded fraction: 0.602, relative distance: 127.372, critic loss: 1.136, actor loss: 26.196\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 13.116, mean reward: 6.806, reward rate: 0.519, rewarded fraction: 0.659, relative distance: 106.743, critic loss: 1.124, actor loss: 26.205\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.021, mean reward: 5.899, reward rate: 0.491, rewarded fraction: 0.569, relative distance: 134.626, critic loss: 1.145, actor loss: 26.139\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.310, mean reward: 6.254, reward rate: 0.508, rewarded fraction: 0.601, relative distance: 122.861, critic loss: 1.134, actor loss: 26.151\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 12.849, mean reward: 6.627, reward rate: 0.516, rewarded fraction: 0.644, relative distance: 115.746, critic loss: 1.150, actor loss: 26.112\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.344, mean reward: 6.177, reward rate: 0.500, rewarded fraction: 0.598, relative distance: 127.127, critic loss: 1.163, actor loss: 26.091\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.740, mean reward: 6.466, reward rate: 0.508, rewarded fraction: 0.625, relative distance: 119.750, critic loss: 1.167, actor loss: 26.081\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.314, mean reward: 6.320, reward rate: 0.513, rewarded fraction: 0.613, relative distance: 125.111, critic loss: 1.196, actor loss: 26.100\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.876, mean reward: 6.468, reward rate: 0.502, rewarded fraction: 0.627, relative distance: 118.096, critic loss: 1.166, actor loss: 26.102\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.768, mean reward: 6.364, reward rate: 0.498, rewarded fraction: 0.617, relative distance: 122.500, critic loss: 1.178, actor loss: 26.153\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 12.904, mean reward: 6.442, reward rate: 0.499, rewarded fraction: 0.628, relative distance: 121.028, critic loss: 1.160, actor loss: 26.206\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.507, mean reward: 6.347, reward rate: 0.507, rewarded fraction: 0.615, relative distance: 124.411, critic loss: 1.127, actor loss: 26.175\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.618, mean reward: 6.311, reward rate: 0.500, rewarded fraction: 0.608, relative distance: 120.041, critic loss: 1.154, actor loss: 26.205\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 12.359, mean reward: 6.148, reward rate: 0.497, rewarded fraction: 0.596, relative distance: 126.825, critic loss: 1.157, actor loss: 26.187\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 12.441, mean reward: 6.219, reward rate: 0.500, rewarded fraction: 0.597, relative distance: 120.673, critic loss: 1.136, actor loss: 26.179\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.720, mean reward: 6.389, reward rate: 0.502, rewarded fraction: 0.618, relative distance: 120.628, critic loss: 1.134, actor loss: 26.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.524, mean reward: 6.263, reward rate: 0.500, rewarded fraction: 0.609, relative distance: 124.427, critic loss: 1.144, actor loss: 26.136\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.256, mean reward: 6.235, reward rate: 0.509, rewarded fraction: 0.607, relative distance: 127.406, critic loss: 1.165, actor loss: 26.080\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 11.994, mean reward: 6.324, reward rate: 0.527, rewarded fraction: 0.614, relative distance: 125.175, critic loss: 1.162, actor loss: 26.100\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 12.174, mean reward: 6.107, reward rate: 0.502, rewarded fraction: 0.590, relative distance: 130.043, critic loss: 1.163, actor loss: 26.111\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.914, mean reward: 6.640, reward rate: 0.514, rewarded fraction: 0.643, relative distance: 113.528, critic loss: 1.169, actor loss: 26.099\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.111, mean reward: 6.654, reward rate: 0.507, rewarded fraction: 0.646, relative distance: 116.679, critic loss: 1.121, actor loss: 26.057\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 12.722, mean reward: 6.379, reward rate: 0.501, rewarded fraction: 0.616, relative distance: 117.851, critic loss: 1.125, actor loss: 26.063\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 12.397, mean reward: 6.296, reward rate: 0.508, rewarded fraction: 0.610, relative distance: 122.884, critic loss: 1.165, actor loss: 26.017\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.109, mean reward: 6.586, reward rate: 0.502, rewarded fraction: 0.639, relative distance: 114.993, critic loss: 1.135, actor loss: 25.981\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 12.161, mean reward: 6.059, reward rate: 0.498, rewarded fraction: 0.578, relative distance: 124.644, critic loss: 1.148, actor loss: 25.995\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.521, mean reward: 6.390, reward rate: 0.510, rewarded fraction: 0.620, relative distance: 122.983, critic loss: 1.144, actor loss: 25.988\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 11.167, mean reward: 0.028, reward rate: 0.003, rewarded fraction: 0.000, relative distance: 285.806, critic loss: 0.000, actor loss: -0.074\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 5.585, mean reward: 0.059, reward rate: 0.011, rewarded fraction: 0.002, relative distance: 278.107, critic loss: 0.002, actor loss: -0.040\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 14.629, mean reward: 0.216, reward rate: 0.015, rewarded fraction: 0.014, relative distance: 262.736, critic loss: 0.029, actor loss: 0.095\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 17.163, mean reward: 0.280, reward rate: 0.016, rewarded fraction: 0.021, relative distance: 264.145, critic loss: 0.045, actor loss: 0.198\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.685, mean reward: 0.395, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 256.060, critic loss: 0.064, actor loss: 0.177\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.579, mean reward: 0.387, reward rate: 0.023, rewarded fraction: 0.028, relative distance: 251.945, critic loss: 0.084, actor loss: 0.129\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 12.979, mean reward: 0.281, reward rate: 0.022, rewarded fraction: 0.019, relative distance: 254.942, critic loss: 0.084, actor loss: 0.091\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 11.978, mean reward: 0.907, reward rate: 0.076, rewarded fraction: 0.071, relative distance: 208.603, critic loss: 0.143, actor loss: 0.441\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 9.825, mean reward: 0.944, reward rate: 0.096, rewarded fraction: 0.074, relative distance: 195.330, critic loss: 0.554, actor loss: 1.371\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 8.347, mean reward: 1.081, reward rate: 0.129, rewarded fraction: 0.091, relative distance: 215.006, critic loss: 0.728, actor loss: 2.730\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 7.762, mean reward: 1.155, reward rate: 0.149, rewarded fraction: 0.100, relative distance: 220.268, critic loss: 0.649, actor loss: 3.820\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 7.077, mean reward: 1.276, reward rate: 0.180, rewarded fraction: 0.109, relative distance: 224.909, critic loss: 0.569, actor loss: 4.971\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 6.388, mean reward: 1.307, reward rate: 0.205, rewarded fraction: 0.117, relative distance: 234.798, critic loss: 0.733, actor loss: 6.380\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 7.508, mean reward: 2.659, reward rate: 0.354, rewarded fraction: 0.254, relative distance: 204.909, critic loss: 0.831, actor loss: 7.819\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 8.190, mean reward: 3.074, reward rate: 0.375, rewarded fraction: 0.293, relative distance: 197.154, critic loss: 0.970, actor loss: 9.199\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 8.701, mean reward: 3.385, reward rate: 0.389, rewarded fraction: 0.321, relative distance: 190.446, critic loss: 1.132, actor loss: 10.563\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 8.443, mean reward: 3.468, reward rate: 0.411, rewarded fraction: 0.328, relative distance: 187.330, critic loss: 1.185, actor loss: 11.830\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 8.403, mean reward: 3.476, reward rate: 0.414, rewarded fraction: 0.331, relative distance: 190.653, critic loss: 1.225, actor loss: 12.898\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 8.513, mean reward: 3.711, reward rate: 0.436, rewarded fraction: 0.355, relative distance: 185.930, critic loss: 1.297, actor loss: 14.046\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 8.970, mean reward: 3.918, reward rate: 0.437, rewarded fraction: 0.375, relative distance: 180.220, critic loss: 1.348, actor loss: 15.412\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 9.503, mean reward: 4.437, reward rate: 0.467, rewarded fraction: 0.428, relative distance: 170.752, critic loss: 1.387, actor loss: 16.635\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 10.473, mean reward: 4.858, reward rate: 0.464, rewarded fraction: 0.465, relative distance: 156.270, critic loss: 1.384, actor loss: 17.630\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 11.819, mean reward: 5.531, reward rate: 0.468, rewarded fraction: 0.526, relative distance: 134.998, critic loss: 1.342, actor loss: 18.694\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 13.754, mean reward: 6.074, reward rate: 0.442, rewarded fraction: 0.564, relative distance: 100.367, critic loss: 1.230, actor loss: 19.750\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 13.779, mean reward: 6.118, reward rate: 0.444, rewarded fraction: 0.580, relative distance: 106.519, critic loss: 1.120, actor loss: 20.670\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 12.155, mean reward: 5.738, reward rate: 0.472, rewarded fraction: 0.548, relative distance: 133.299, critic loss: 1.198, actor loss: 21.503\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 11.913, mean reward: 5.694, reward rate: 0.478, rewarded fraction: 0.547, relative distance: 136.119, critic loss: 1.284, actor loss: 22.208\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 12.163, mean reward: 5.793, reward rate: 0.476, rewarded fraction: 0.559, relative distance: 134.673, critic loss: 1.243, actor loss: 22.807\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 11.929, mean reward: 5.918, reward rate: 0.496, rewarded fraction: 0.569, relative distance: 132.755, critic loss: 1.227, actor loss: 23.270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 11.150, mean reward: 5.552, reward rate: 0.498, rewarded fraction: 0.535, relative distance: 144.940, critic loss: 1.231, actor loss: 23.673\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 11.360, mean reward: 5.584, reward rate: 0.492, rewarded fraction: 0.540, relative distance: 144.526, critic loss: 1.227, actor loss: 23.973\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 11.319, mean reward: 5.528, reward rate: 0.488, rewarded fraction: 0.533, relative distance: 144.223, critic loss: 1.218, actor loss: 24.230\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 11.583, mean reward: 5.699, reward rate: 0.492, rewarded fraction: 0.551, relative distance: 138.927, critic loss: 1.221, actor loss: 24.496\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 11.318, mean reward: 5.509, reward rate: 0.487, rewarded fraction: 0.530, relative distance: 144.975, critic loss: 1.224, actor loss: 24.742\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 11.567, mean reward: 5.673, reward rate: 0.490, rewarded fraction: 0.541, relative distance: 135.741, critic loss: 1.211, actor loss: 24.917\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 11.402, mean reward: 5.477, reward rate: 0.480, rewarded fraction: 0.526, relative distance: 142.786, critic loss: 1.221, actor loss: 25.109\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 11.408, mean reward: 5.486, reward rate: 0.481, rewarded fraction: 0.525, relative distance: 141.445, critic loss: 1.209, actor loss: 25.233\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 11.479, mean reward: 5.670, reward rate: 0.494, rewarded fraction: 0.546, relative distance: 137.774, critic loss: 1.191, actor loss: 25.287\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 11.723, mean reward: 5.717, reward rate: 0.488, rewarded fraction: 0.551, relative distance: 138.463, critic loss: 1.169, actor loss: 25.333\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 11.660, mean reward: 5.823, reward rate: 0.499, rewarded fraction: 0.555, relative distance: 131.895, critic loss: 1.166, actor loss: 25.392\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 11.970, mean reward: 5.628, reward rate: 0.470, rewarded fraction: 0.537, relative distance: 136.241, critic loss: 1.164, actor loss: 25.418\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 11.802, mean reward: 5.922, reward rate: 0.502, rewarded fraction: 0.570, relative distance: 130.257, critic loss: 1.168, actor loss: 25.488\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 10.998, mean reward: 5.468, reward rate: 0.497, rewarded fraction: 0.529, relative distance: 146.041, critic loss: 1.157, actor loss: 25.505\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 11.355, mean reward: 5.398, reward rate: 0.475, rewarded fraction: 0.515, relative distance: 146.236, critic loss: 1.170, actor loss: 25.530\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 11.778, mean reward: 6.064, reward rate: 0.515, rewarded fraction: 0.582, relative distance: 127.713, critic loss: 1.175, actor loss: 25.543\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 11.597, mean reward: 5.444, reward rate: 0.469, rewarded fraction: 0.515, relative distance: 140.541, critic loss: 1.174, actor loss: 25.518\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 11.504, mean reward: 5.587, reward rate: 0.486, rewarded fraction: 0.530, relative distance: 136.485, critic loss: 1.146, actor loss: 25.509\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 11.435, mean reward: 5.501, reward rate: 0.481, rewarded fraction: 0.526, relative distance: 140.246, critic loss: 1.132, actor loss: 25.466\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 11.456, mean reward: 5.280, reward rate: 0.461, rewarded fraction: 0.502, relative distance: 144.070, critic loss: 1.152, actor loss: 25.453\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 11.449, mean reward: 5.550, reward rate: 0.485, rewarded fraction: 0.533, relative distance: 141.194, critic loss: 1.160, actor loss: 25.467\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 11.728, mean reward: 5.846, reward rate: 0.498, rewarded fraction: 0.566, relative distance: 137.858, critic loss: 1.125, actor loss: 25.435\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 11.445, mean reward: 5.735, reward rate: 0.501, rewarded fraction: 0.553, relative distance: 138.888, critic loss: 1.127, actor loss: 25.396\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 11.442, mean reward: 5.899, reward rate: 0.516, rewarded fraction: 0.570, relative distance: 135.834, critic loss: 1.136, actor loss: 25.400\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 11.471, mean reward: 5.581, reward rate: 0.487, rewarded fraction: 0.532, relative distance: 135.921, critic loss: 1.152, actor loss: 25.386\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 11.707, mean reward: 5.817, reward rate: 0.497, rewarded fraction: 0.557, relative distance: 132.804, critic loss: 1.151, actor loss: 25.359\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 11.357, mean reward: 5.586, reward rate: 0.492, rewarded fraction: 0.534, relative distance: 140.103, critic loss: 1.136, actor loss: 25.349\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.376, mean reward: 6.068, reward rate: 0.490, rewarded fraction: 0.584, relative distance: 127.013, critic loss: 1.128, actor loss: 25.343\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.008, mean reward: 5.826, reward rate: 0.485, rewarded fraction: 0.557, relative distance: 133.666, critic loss: 1.136, actor loss: 25.313\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 11.445, mean reward: 5.952, reward rate: 0.520, rewarded fraction: 0.577, relative distance: 132.701, critic loss: 1.133, actor loss: 25.270\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 11.657, mean reward: 5.945, reward rate: 0.510, rewarded fraction: 0.574, relative distance: 134.609, critic loss: 1.111, actor loss: 25.272\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.260, mean reward: 6.338, reward rate: 0.517, rewarded fraction: 0.613, relative distance: 123.352, critic loss: 1.103, actor loss: 25.228\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.385, mean reward: 5.946, reward rate: 0.480, rewarded fraction: 0.567, relative distance: 127.599, critic loss: 1.101, actor loss: 25.179\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 11.802, mean reward: 6.172, reward rate: 0.523, rewarded fraction: 0.599, relative distance: 128.950, critic loss: 1.058, actor loss: 25.156\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.083, mean reward: 6.031, reward rate: 0.499, rewarded fraction: 0.578, relative distance: 127.430, critic loss: 1.105, actor loss: 25.164\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 11.714, mean reward: 5.656, reward rate: 0.483, rewarded fraction: 0.540, relative distance: 138.306, critic loss: 1.095, actor loss: 25.163\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.189, mean reward: 5.994, reward rate: 0.492, rewarded fraction: 0.575, relative distance: 127.409, critic loss: 1.118, actor loss: 25.148\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.138, mean reward: 6.082, reward rate: 0.501, rewarded fraction: 0.589, relative distance: 130.252, critic loss: 1.127, actor loss: 25.159\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.588, mean reward: 6.172, reward rate: 0.490, rewarded fraction: 0.590, relative distance: 121.427, critic loss: 1.159, actor loss: 25.109\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.306, mean reward: 6.151, reward rate: 0.500, rewarded fraction: 0.593, relative distance: 126.093, critic loss: 1.120, actor loss: 25.031\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.703, mean reward: 6.187, reward rate: 0.487, rewarded fraction: 0.595, relative distance: 123.365, critic loss: 1.077, actor loss: 25.043\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 12.331, mean reward: 6.061, reward rate: 0.492, rewarded fraction: 0.574, relative distance: 121.766, critic loss: 1.139, actor loss: 25.030\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.395, mean reward: 6.177, reward rate: 0.498, rewarded fraction: 0.597, relative distance: 124.959, critic loss: 1.123, actor loss: 25.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.264, mean reward: 6.011, reward rate: 0.490, rewarded fraction: 0.578, relative distance: 128.068, critic loss: 1.102, actor loss: 24.975\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 12.740, mean reward: 6.215, reward rate: 0.488, rewarded fraction: 0.602, relative distance: 121.574, critic loss: 1.099, actor loss: 24.968\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.122, mean reward: 6.056, reward rate: 0.500, rewarded fraction: 0.587, relative distance: 131.169, critic loss: 1.118, actor loss: 24.951\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.048, mean reward: 5.788, reward rate: 0.480, rewarded fraction: 0.551, relative distance: 129.600, critic loss: 1.167, actor loss: 24.913\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.411, mean reward: 6.316, reward rate: 0.509, rewarded fraction: 0.611, relative distance: 118.790, critic loss: 1.155, actor loss: 24.911\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.434, mean reward: 6.275, reward rate: 0.505, rewarded fraction: 0.602, relative distance: 119.592, critic loss: 1.118, actor loss: 24.934\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.463, mean reward: 6.137, reward rate: 0.492, rewarded fraction: 0.590, relative distance: 126.844, critic loss: 1.110, actor loss: 24.917\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 12.367, mean reward: 6.261, reward rate: 0.506, rewarded fraction: 0.608, relative distance: 125.794, critic loss: 1.101, actor loss: 24.928\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.803, mean reward: 6.070, reward rate: 0.474, rewarded fraction: 0.575, relative distance: 119.472, critic loss: 1.150, actor loss: 24.923\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.629, mean reward: 6.217, reward rate: 0.492, rewarded fraction: 0.594, relative distance: 118.968, critic loss: 1.168, actor loss: 24.934\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 12.212, mean reward: 6.303, reward rate: 0.516, rewarded fraction: 0.612, relative distance: 122.017, critic loss: 1.106, actor loss: 24.950\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 12.561, mean reward: 6.422, reward rate: 0.511, rewarded fraction: 0.622, relative distance: 118.829, critic loss: 1.145, actor loss: 24.923\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.588, mean reward: 6.332, reward rate: 0.503, rewarded fraction: 0.611, relative distance: 120.734, critic loss: 1.120, actor loss: 24.934\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.501, mean reward: 6.372, reward rate: 0.510, rewarded fraction: 0.615, relative distance: 120.181, critic loss: 1.110, actor loss: 24.912\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.883, mean reward: 6.571, reward rate: 0.510, rewarded fraction: 0.636, relative distance: 115.589, critic loss: 1.078, actor loss: 24.887\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.749, mean reward: 6.454, reward rate: 0.506, rewarded fraction: 0.628, relative distance: 121.291, critic loss: 1.078, actor loss: 24.893\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 13.024, mean reward: 6.343, reward rate: 0.487, rewarded fraction: 0.612, relative distance: 123.174, critic loss: 1.082, actor loss: 24.922\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 13.226, mean reward: 6.702, reward rate: 0.507, rewarded fraction: 0.647, relative distance: 111.842, critic loss: 1.120, actor loss: 24.906\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.950, mean reward: 6.752, reward rate: 0.521, rewarded fraction: 0.655, relative distance: 111.827, critic loss: 1.103, actor loss: 24.882\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.132, mean reward: 6.483, reward rate: 0.494, rewarded fraction: 0.624, relative distance: 117.473, critic loss: 1.044, actor loss: 24.868\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 12.219, mean reward: 6.127, reward rate: 0.501, rewarded fraction: 0.584, relative distance: 124.290, critic loss: 1.154, actor loss: 24.887\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 12.764, mean reward: 6.624, reward rate: 0.519, rewarded fraction: 0.638, relative distance: 114.189, critic loss: 1.123, actor loss: 24.889\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 12.462, mean reward: 6.178, reward rate: 0.496, rewarded fraction: 0.597, relative distance: 125.534, critic loss: 1.100, actor loss: 24.842\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.394, mean reward: 6.236, reward rate: 0.503, rewarded fraction: 0.602, relative distance: 123.495, critic loss: 1.101, actor loss: 24.859\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 44.000, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 314.372, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 37.667, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 261.418, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 41.500, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 349.585, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 53.500, mean reward: 0.010, reward rate: 0.000, rewarded fraction: 0.000, relative distance: 259.678, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 11.235, mean reward: 0.070, reward rate: 0.006, rewarded fraction: 0.001, relative distance: 280.746, critic loss: 0.004, actor loss: 0.016\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 12.697, mean reward: 0.187, reward rate: 0.015, rewarded fraction: 0.012, relative distance: 271.274, critic loss: 0.020, actor loss: 0.095\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 13.497, mean reward: 0.200, reward rate: 0.015, rewarded fraction: 0.011, relative distance: 262.801, critic loss: 0.036, actor loss: 0.249\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 11.158, mean reward: 0.312, reward rate: 0.028, rewarded fraction: 0.021, relative distance: 237.150, critic loss: 0.063, actor loss: 0.585\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 12.855, mean reward: 0.817, reward rate: 0.064, rewarded fraction: 0.063, relative distance: 208.235, critic loss: 0.115, actor loss: 1.338\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 12.209, mean reward: 1.086, reward rate: 0.089, rewarded fraction: 0.086, relative distance: 190.008, critic loss: 0.205, actor loss: 2.970\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 14.068, mean reward: 1.276, reward rate: 0.091, rewarded fraction: 0.106, relative distance: 189.364, critic loss: 0.249, actor loss: 4.309\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 10.946, mean reward: 1.776, reward rate: 0.162, rewarded fraction: 0.153, relative distance: 188.160, critic loss: 0.640, actor loss: 6.628\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 9.351, mean reward: 1.751, reward rate: 0.187, rewarded fraction: 0.156, relative distance: 207.741, critic loss: 1.353, actor loss: 7.787\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 9.199, mean reward: 1.948, reward rate: 0.212, rewarded fraction: 0.175, relative distance: 201.861, critic loss: 1.190, actor loss: 9.570\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 11.745, mean reward: 4.513, reward rate: 0.384, rewarded fraction: 0.437, relative distance: 154.909, critic loss: 1.162, actor loss: 11.179\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 12.422, mean reward: 4.827, reward rate: 0.389, rewarded fraction: 0.458, relative distance: 144.228, critic loss: 1.407, actor loss: 12.824\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 12.472, mean reward: 5.295, reward rate: 0.425, rewarded fraction: 0.513, relative distance: 137.157, critic loss: 1.526, actor loss: 14.249\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 12.813, mean reward: 5.598, reward rate: 0.437, rewarded fraction: 0.536, relative distance: 126.592, critic loss: 1.451, actor loss: 15.530\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 13.112, mean reward: 6.035, reward rate: 0.460, rewarded fraction: 0.584, relative distance: 119.794, critic loss: 1.359, actor loss: 16.697\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 12.993, mean reward: 5.815, reward rate: 0.448, rewarded fraction: 0.554, relative distance: 122.764, critic loss: 1.266, actor loss: 17.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 13.937, mean reward: 6.233, reward rate: 0.447, rewarded fraction: 0.595, relative distance: 109.729, critic loss: 1.208, actor loss: 18.766\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 13.848, mean reward: 6.032, reward rate: 0.436, rewarded fraction: 0.577, relative distance: 115.959, critic loss: 1.188, actor loss: 19.677\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 14.748, mean reward: 7.191, reward rate: 0.488, rewarded fraction: 0.696, relative distance: 90.521, critic loss: 1.146, actor loss: 20.579\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 14.433, mean reward: 7.047, reward rate: 0.488, rewarded fraction: 0.682, relative distance: 93.944, critic loss: 1.093, actor loss: 21.411\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 15.665, mean reward: 7.661, reward rate: 0.489, rewarded fraction: 0.744, relative distance: 78.354, critic loss: 1.037, actor loss: 22.160\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 15.600, mean reward: 7.581, reward rate: 0.486, rewarded fraction: 0.728, relative distance: 78.149, critic loss: 1.020, actor loss: 22.812\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 15.798, mean reward: 7.829, reward rate: 0.496, rewarded fraction: 0.751, relative distance: 69.131, critic loss: 1.017, actor loss: 23.438\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 16.293, mean reward: 8.151, reward rate: 0.500, rewarded fraction: 0.785, relative distance: 61.809, critic loss: 1.014, actor loss: 24.007\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 16.812, mean reward: 8.175, reward rate: 0.486, rewarded fraction: 0.790, relative distance: 63.684, critic loss: 1.011, actor loss: 24.479\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 16.529, mean reward: 8.286, reward rate: 0.501, rewarded fraction: 0.803, relative distance: 59.736, critic loss: 1.009, actor loss: 24.879\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 16.461, mean reward: 8.284, reward rate: 0.503, rewarded fraction: 0.805, relative distance: 60.686, critic loss: 1.006, actor loss: 25.236\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 17.011, mean reward: 8.520, reward rate: 0.501, rewarded fraction: 0.829, relative distance: 56.638, critic loss: 0.989, actor loss: 25.567\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 16.899, mean reward: 8.673, reward rate: 0.513, rewarded fraction: 0.842, relative distance: 51.641, critic loss: 0.990, actor loss: 25.881\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 17.021, mean reward: 8.785, reward rate: 0.516, rewarded fraction: 0.859, relative distance: 51.505, critic loss: 0.987, actor loss: 26.129\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 17.197, mean reward: 8.326, reward rate: 0.484, rewarded fraction: 0.807, relative distance: 56.602, critic loss: 0.990, actor loss: 26.377\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 17.300, mean reward: 8.573, reward rate: 0.496, rewarded fraction: 0.832, relative distance: 52.150, critic loss: 0.981, actor loss: 26.572\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 17.359, mean reward: 8.894, reward rate: 0.512, rewarded fraction: 0.868, relative distance: 47.593, critic loss: 0.963, actor loss: 26.743\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 17.706, mean reward: 8.771, reward rate: 0.495, rewarded fraction: 0.849, relative distance: 46.321, critic loss: 0.969, actor loss: 26.902\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 17.116, mean reward: 8.744, reward rate: 0.511, rewarded fraction: 0.850, relative distance: 48.806, critic loss: 0.954, actor loss: 27.032\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 17.439, mean reward: 8.559, reward rate: 0.491, rewarded fraction: 0.829, relative distance: 52.809, critic loss: 0.950, actor loss: 27.140\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 17.471, mean reward: 8.942, reward rate: 0.512, rewarded fraction: 0.870, relative distance: 46.018, critic loss: 0.955, actor loss: 27.262\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 17.516, mean reward: 8.606, reward rate: 0.491, rewarded fraction: 0.833, relative distance: 50.355, critic loss: 0.955, actor loss: 27.334\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 17.676, mean reward: 8.718, reward rate: 0.493, rewarded fraction: 0.841, relative distance: 44.869, critic loss: 0.943, actor loss: 27.367\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 17.079, mean reward: 8.937, reward rate: 0.523, rewarded fraction: 0.872, relative distance: 45.722, critic loss: 0.944, actor loss: 27.364\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 17.382, mean reward: 8.907, reward rate: 0.512, rewarded fraction: 0.869, relative distance: 45.715, critic loss: 0.944, actor loss: 27.391\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 17.100, mean reward: 8.760, reward rate: 0.512, rewarded fraction: 0.852, relative distance: 47.145, critic loss: 0.950, actor loss: 27.384\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 17.320, mean reward: 8.847, reward rate: 0.511, rewarded fraction: 0.864, relative distance: 49.336, critic loss: 0.960, actor loss: 27.407\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 16.912, mean reward: 8.752, reward rate: 0.518, rewarded fraction: 0.850, relative distance: 48.684, critic loss: 0.967, actor loss: 27.424\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 16.392, mean reward: 8.322, reward rate: 0.508, rewarded fraction: 0.796, relative distance: 53.964, critic loss: 0.972, actor loss: 27.422\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 17.203, mean reward: 8.561, reward rate: 0.498, rewarded fraction: 0.822, relative distance: 47.584, critic loss: 0.974, actor loss: 27.446\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 17.087, mean reward: 8.519, reward rate: 0.499, rewarded fraction: 0.822, relative distance: 49.963, critic loss: 0.966, actor loss: 27.436\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 16.734, mean reward: 8.626, reward rate: 0.515, rewarded fraction: 0.834, relative distance: 48.141, critic loss: 0.969, actor loss: 27.436\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 17.145, mean reward: 8.812, reward rate: 0.514, rewarded fraction: 0.855, relative distance: 44.913, critic loss: 0.965, actor loss: 27.407\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 16.929, mean reward: 8.980, reward rate: 0.530, rewarded fraction: 0.873, relative distance: 42.976, critic loss: 0.949, actor loss: 27.392\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 17.420, mean reward: 8.683, reward rate: 0.498, rewarded fraction: 0.839, relative distance: 47.375, critic loss: 0.950, actor loss: 27.388\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 16.610, mean reward: 8.591, reward rate: 0.517, rewarded fraction: 0.832, relative distance: 47.848, critic loss: 0.970, actor loss: 27.398\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 17.064, mean reward: 8.303, reward rate: 0.487, rewarded fraction: 0.797, relative distance: 54.595, critic loss: 0.976, actor loss: 27.370\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 17.037, mean reward: 8.597, reward rate: 0.505, rewarded fraction: 0.827, relative distance: 50.143, critic loss: 0.968, actor loss: 27.359\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 17.559, mean reward: 8.798, reward rate: 0.501, rewarded fraction: 0.855, relative distance: 45.345, critic loss: 0.966, actor loss: 27.357\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 17.198, mean reward: 8.604, reward rate: 0.500, rewarded fraction: 0.830, relative distance: 47.820, critic loss: 0.977, actor loss: 27.353\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 17.400, mean reward: 8.545, reward rate: 0.491, rewarded fraction: 0.822, relative distance: 48.449, critic loss: 0.989, actor loss: 27.367\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 17.036, mean reward: 8.694, reward rate: 0.510, rewarded fraction: 0.839, relative distance: 46.951, critic loss: 0.969, actor loss: 27.313\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 16.985, mean reward: 8.778, reward rate: 0.517, rewarded fraction: 0.852, relative distance: 47.549, critic loss: 0.968, actor loss: 27.314\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 17.209, mean reward: 8.677, reward rate: 0.504, rewarded fraction: 0.837, relative distance: 45.511, critic loss: 0.998, actor loss: 27.297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 17.333, mean reward: 8.643, reward rate: 0.499, rewarded fraction: 0.834, relative distance: 49.376, critic loss: 0.990, actor loss: 27.269\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 17.464, mean reward: 8.472, reward rate: 0.485, rewarded fraction: 0.816, relative distance: 48.669, critic loss: 0.974, actor loss: 27.272\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 17.006, mean reward: 8.371, reward rate: 0.492, rewarded fraction: 0.803, relative distance: 48.725, critic loss: 1.015, actor loss: 27.247\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 16.854, mean reward: 8.523, reward rate: 0.506, rewarded fraction: 0.820, relative distance: 47.734, critic loss: 0.986, actor loss: 27.251\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 16.944, mean reward: 8.524, reward rate: 0.503, rewarded fraction: 0.825, relative distance: 52.683, critic loss: 1.004, actor loss: 27.248\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 16.976, mean reward: 8.881, reward rate: 0.523, rewarded fraction: 0.860, relative distance: 45.395, critic loss: 0.987, actor loss: 27.297\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 17.051, mean reward: 8.717, reward rate: 0.511, rewarded fraction: 0.843, relative distance: 46.738, critic loss: 1.010, actor loss: 27.306\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 17.013, mean reward: 8.672, reward rate: 0.510, rewarded fraction: 0.841, relative distance: 47.638, critic loss: 1.068, actor loss: 27.306\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 17.374, mean reward: 8.608, reward rate: 0.495, rewarded fraction: 0.833, relative distance: 47.987, critic loss: 0.992, actor loss: 27.287\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 16.824, mean reward: 9.082, reward rate: 0.540, rewarded fraction: 0.886, relative distance: 44.538, critic loss: 1.045, actor loss: 27.306\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 17.282, mean reward: 8.818, reward rate: 0.510, rewarded fraction: 0.857, relative distance: 45.323, critic loss: 0.989, actor loss: 27.298\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 17.229, mean reward: 8.798, reward rate: 0.511, rewarded fraction: 0.855, relative distance: 46.368, critic loss: 1.003, actor loss: 27.327\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 16.855, mean reward: 8.746, reward rate: 0.519, rewarded fraction: 0.848, relative distance: 45.858, critic loss: 1.048, actor loss: 27.286\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 16.993, mean reward: 8.485, reward rate: 0.499, rewarded fraction: 0.820, relative distance: 51.551, critic loss: 1.073, actor loss: 27.310\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 17.350, mean reward: 8.689, reward rate: 0.501, rewarded fraction: 0.836, relative distance: 46.100, critic loss: 1.065, actor loss: 27.288\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 17.335, mean reward: 8.888, reward rate: 0.513, rewarded fraction: 0.862, relative distance: 44.115, critic loss: 1.306, actor loss: 27.288\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 17.000, mean reward: 8.901, reward rate: 0.524, rewarded fraction: 0.868, relative distance: 46.693, critic loss: 1.133, actor loss: 27.287\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 17.468, mean reward: 8.607, reward rate: 0.493, rewarded fraction: 0.830, relative distance: 48.810, critic loss: 1.013, actor loss: 27.333\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 17.045, mean reward: 8.743, reward rate: 0.513, rewarded fraction: 0.843, relative distance: 47.098, critic loss: 1.034, actor loss: 27.338\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 17.322, mean reward: 8.671, reward rate: 0.501, rewarded fraction: 0.842, relative distance: 48.150, critic loss: 1.056, actor loss: 27.357\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 17.432, mean reward: 8.651, reward rate: 0.496, rewarded fraction: 0.836, relative distance: 48.033, critic loss: 1.341, actor loss: 27.296\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 17.235, mean reward: 8.816, reward rate: 0.512, rewarded fraction: 0.856, relative distance: 47.670, critic loss: 1.016, actor loss: 27.312\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 17.649, mean reward: 8.841, reward rate: 0.501, rewarded fraction: 0.861, relative distance: 45.398, critic loss: 1.116, actor loss: 27.333\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 17.049, mean reward: 8.628, reward rate: 0.506, rewarded fraction: 0.831, relative distance: 48.473, critic loss: 1.077, actor loss: 27.334\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 17.075, mean reward: 8.603, reward rate: 0.504, rewarded fraction: 0.831, relative distance: 50.312, critic loss: 1.021, actor loss: 27.342\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 16.524, mean reward: 8.961, reward rate: 0.542, rewarded fraction: 0.879, relative distance: 47.552, critic loss: 1.058, actor loss: 27.367\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 16.608, mean reward: 8.854, reward rate: 0.533, rewarded fraction: 0.863, relative distance: 49.121, critic loss: 1.055, actor loss: 27.372\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 17.104, mean reward: 8.574, reward rate: 0.501, rewarded fraction: 0.829, relative distance: 49.843, critic loss: 1.063, actor loss: 27.371\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 16.896, mean reward: 8.540, reward rate: 0.505, rewarded fraction: 0.820, relative distance: 47.790, critic loss: 1.028, actor loss: 27.406\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 17.380, mean reward: 8.940, reward rate: 0.514, rewarded fraction: 0.868, relative distance: 45.145, critic loss: 1.092, actor loss: 27.411\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 17.039, mean reward: 8.933, reward rate: 0.524, rewarded fraction: 0.871, relative distance: 45.811, critic loss: 1.023, actor loss: 27.400\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 17.169, mean reward: 8.862, reward rate: 0.516, rewarded fraction: 0.860, relative distance: 44.731, critic loss: 1.022, actor loss: 27.418\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 16.755, mean reward: 8.611, reward rate: 0.514, rewarded fraction: 0.836, relative distance: 52.327, critic loss: 1.031, actor loss: 27.447\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 17.359, mean reward: 8.743, reward rate: 0.504, rewarded fraction: 0.847, relative distance: 46.066, critic loss: 1.075, actor loss: 27.449\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 17.090, mean reward: 8.575, reward rate: 0.502, rewarded fraction: 0.829, relative distance: 48.898, critic loss: 1.048, actor loss: 27.446\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 17.087, mean reward: 8.778, reward rate: 0.514, rewarded fraction: 0.853, relative distance: 46.899, critic loss: 1.086, actor loss: 27.458\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.349, mean reward: 0.022, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 357.363, critic loss: 0.000, actor loss: -0.118\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.945, mean reward: 0.023, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 369.344, critic loss: 0.000, actor loss: -0.091\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.587, mean reward: 0.039, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 354.551, critic loss: 0.002, actor loss: -0.083\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 17.429, mean reward: 0.058, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 354.773, critic loss: 0.008, actor loss: -0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.587, mean reward: 0.068, reward rate: 0.004, rewarded fraction: 0.004, relative distance: 364.542, critic loss: 0.009, actor loss: -0.078\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.389, mean reward: 0.078, reward rate: 0.005, rewarded fraction: 0.006, relative distance: 360.956, critic loss: 0.014, actor loss: -0.077\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.214, mean reward: 0.024, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 357.538, critic loss: 0.015, actor loss: -0.073\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.618, mean reward: 0.033, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 362.675, critic loss: 0.016, actor loss: -0.066\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.362, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 364.395, critic loss: 0.016, actor loss: -0.062\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 17.560, mean reward: 0.024, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 369.305, critic loss: 0.015, actor loss: -0.057\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 18.333, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 370.637, critic loss: 0.014, actor loss: -0.054\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.336, mean reward: 0.052, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 361.877, critic loss: 0.015, actor loss: -0.052\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.233, mean reward: 0.056, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 358.586, critic loss: 0.016, actor loss: -0.052\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 16.287, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 365.978, critic loss: 0.016, actor loss: -0.050\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 18.371, mean reward: 0.084, reward rate: 0.005, rewarded fraction: 0.007, relative distance: 357.212, critic loss: 0.018, actor loss: -0.048\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 15.854, mean reward: 0.033, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 354.411, critic loss: 0.020, actor loss: -0.046\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 17.091, mean reward: 0.054, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 356.224, critic loss: 0.020, actor loss: -0.049\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.043, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 359.497, critic loss: 0.020, actor loss: -0.051\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.483, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 366.636, critic loss: 0.020, actor loss: -0.055\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.946, mean reward: 0.047, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 362.960, critic loss: 0.019, actor loss: -0.064\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.254, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 360.961, critic loss: 0.019, actor loss: -0.073\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.317, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 357.007, critic loss: 0.017, actor loss: -0.082\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 17.619, mean reward: 0.094, reward rate: 0.005, rewarded fraction: 0.006, relative distance: 354.794, critic loss: 0.021, actor loss: -0.090\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 17.194, mean reward: 0.018, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 366.240, critic loss: 0.019, actor loss: -0.098\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 17.446, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 371.910, critic loss: 0.020, actor loss: -0.105\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.537, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 360.229, critic loss: 0.019, actor loss: -0.108\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.233, mean reward: 0.024, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 367.652, critic loss: 0.019, actor loss: -0.109\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 17.328, mean reward: 0.055, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 355.948, critic loss: 0.017, actor loss: -0.109\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 17.286, mean reward: 0.107, reward rate: 0.006, rewarded fraction: 0.009, relative distance: 358.761, critic loss: 0.019, actor loss: -0.109\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 17.135, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 357.795, critic loss: 0.020, actor loss: -0.111\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 17.374, mean reward: 0.103, reward rate: 0.006, rewarded fraction: 0.009, relative distance: 354.754, critic loss: 0.020, actor loss: -0.112\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.180, mean reward: 0.024, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 362.737, critic loss: 0.021, actor loss: -0.118\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 16.010, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 342.532, critic loss: 0.019, actor loss: -0.129\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 9.118, mean reward: 0.063, reward rate: 0.007, rewarded fraction: 0.002, relative distance: 297.139, critic loss: 0.022, actor loss: -0.112\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 11.203, mean reward: 0.118, reward rate: 0.011, rewarded fraction: 0.008, relative distance: 277.685, critic loss: 0.022, actor loss: -0.036\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 12.966, mean reward: 0.179, reward rate: 0.014, rewarded fraction: 0.010, relative distance: 250.265, critic loss: 0.029, actor loss: 0.133\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 15.389, mean reward: 0.672, reward rate: 0.044, rewarded fraction: 0.051, relative distance: 222.231, critic loss: 0.039, actor loss: 0.446\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 16.170, mean reward: 0.685, reward rate: 0.042, rewarded fraction: 0.056, relative distance: 233.203, critic loss: 0.054, actor loss: 1.121\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 17.262, mean reward: 0.644, reward rate: 0.037, rewarded fraction: 0.048, relative distance: 221.275, critic loss: 0.060, actor loss: 1.616\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 16.874, mean reward: 0.814, reward rate: 0.048, rewarded fraction: 0.065, relative distance: 216.125, critic loss: 0.063, actor loss: 1.477\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 16.802, mean reward: 0.895, reward rate: 0.053, rewarded fraction: 0.069, relative distance: 218.838, critic loss: 0.078, actor loss: 1.169\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 16.120, mean reward: 0.842, reward rate: 0.052, rewarded fraction: 0.062, relative distance: 216.757, critic loss: 0.078, actor loss: 0.946\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 17.010, mean reward: 0.889, reward rate: 0.052, rewarded fraction: 0.070, relative distance: 220.742, critic loss: 0.085, actor loss: 0.838\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 17.072, mean reward: 0.528, reward rate: 0.031, rewarded fraction: 0.036, relative distance: 225.265, critic loss: 0.085, actor loss: 0.742\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.277, mean reward: 0.849, reward rate: 0.052, rewarded fraction: 0.064, relative distance: 213.522, critic loss: 0.088, actor loss: 0.615\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 15.053, mean reward: 0.467, reward rate: 0.031, rewarded fraction: 0.030, relative distance: 235.949, critic loss: 0.106, actor loss: 0.507\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 13.204, mean reward: 0.782, reward rate: 0.059, rewarded fraction: 0.060, relative distance: 202.241, critic loss: 0.123, actor loss: 0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 7.951, mean reward: 0.631, reward rate: 0.079, rewarded fraction: 0.051, relative distance: 231.073, critic loss: 0.264, actor loss: 1.459\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 10.664, mean reward: 1.171, reward rate: 0.110, rewarded fraction: 0.097, relative distance: 200.575, critic loss: 0.355, actor loss: 2.101\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 9.563, mean reward: 1.313, reward rate: 0.137, rewarded fraction: 0.111, relative distance: 205.824, critic loss: 0.385, actor loss: 2.852\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 9.367, mean reward: 1.229, reward rate: 0.131, rewarded fraction: 0.102, relative distance: 206.399, critic loss: 0.522, actor loss: 4.206\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 8.746, mean reward: 1.174, reward rate: 0.134, rewarded fraction: 0.099, relative distance: 213.441, critic loss: 0.582, actor loss: 5.591\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 9.060, mean reward: 1.374, reward rate: 0.152, rewarded fraction: 0.118, relative distance: 208.961, critic loss: 0.617, actor loss: 6.672\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 9.529, mean reward: 1.687, reward rate: 0.177, rewarded fraction: 0.146, relative distance: 200.480, critic loss: 0.652, actor loss: 7.773\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 8.637, mean reward: 1.681, reward rate: 0.195, rewarded fraction: 0.148, relative distance: 205.005, critic loss: 0.639, actor loss: 8.562\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 8.746, mean reward: 1.864, reward rate: 0.213, rewarded fraction: 0.167, relative distance: 201.348, critic loss: 0.670, actor loss: 9.094\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.880, mean reward: 5.089, reward rate: 0.395, rewarded fraction: 0.472, relative distance: 122.423, critic loss: 0.756, actor loss: 9.726\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 13.117, mean reward: 4.987, reward rate: 0.380, rewarded fraction: 0.460, relative distance: 122.895, critic loss: 0.843, actor loss: 10.555\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 15.388, mean reward: 3.992, reward rate: 0.259, rewarded fraction: 0.359, relative distance: 138.418, critic loss: 0.926, actor loss: 11.535\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 16.232, mean reward: 4.876, reward rate: 0.300, rewarded fraction: 0.465, relative distance: 141.850, critic loss: 0.963, actor loss: 12.922\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 20.064, mean reward: 4.340, reward rate: 0.216, rewarded fraction: 0.392, relative distance: 125.283, critic loss: 1.027, actor loss: 14.845\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 31.817, mean reward: 0.944, reward rate: 0.030, rewarded fraction: 0.075, relative distance: 226.031, critic loss: 0.685, actor loss: 17.066\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 14.911, mean reward: 5.353, reward rate: 0.359, rewarded fraction: 0.508, relative distance: 110.283, critic loss: 0.379, actor loss: 17.243\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.694, mean reward: 5.657, reward rate: 0.446, rewarded fraction: 0.542, relative distance: 125.593, critic loss: 0.740, actor loss: 17.107\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 13.293, mean reward: 6.369, reward rate: 0.479, rewarded fraction: 0.615, relative distance: 109.335, critic loss: 0.920, actor loss: 17.738\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 13.440, mean reward: 6.432, reward rate: 0.479, rewarded fraction: 0.617, relative distance: 106.625, critic loss: 0.902, actor loss: 18.581\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.753, mean reward: 6.024, reward rate: 0.472, rewarded fraction: 0.583, relative distance: 119.179, critic loss: 0.893, actor loss: 19.285\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 13.641, mean reward: 6.431, reward rate: 0.471, rewarded fraction: 0.619, relative distance: 108.636, critic loss: 0.908, actor loss: 19.938\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.823, mean reward: 6.089, reward rate: 0.475, rewarded fraction: 0.586, relative distance: 118.118, critic loss: 0.909, actor loss: 20.585\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.299, mean reward: 5.993, reward rate: 0.487, rewarded fraction: 0.579, relative distance: 126.979, critic loss: 0.887, actor loss: 21.172\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 12.585, mean reward: 6.115, reward rate: 0.486, rewarded fraction: 0.589, relative distance: 119.950, critic loss: 0.875, actor loss: 21.702\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.353, mean reward: 6.121, reward rate: 0.496, rewarded fraction: 0.595, relative distance: 128.785, critic loss: 0.882, actor loss: 22.201\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.190, mean reward: 5.756, reward rate: 0.472, rewarded fraction: 0.554, relative distance: 133.634, critic loss: 0.834, actor loss: 22.585\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 11.735, mean reward: 6.053, reward rate: 0.516, rewarded fraction: 0.590, relative distance: 131.784, critic loss: 0.816, actor loss: 22.922\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.655, mean reward: 6.392, reward rate: 0.505, rewarded fraction: 0.623, relative distance: 121.872, critic loss: 0.815, actor loss: 23.261\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 11.851, mean reward: 5.936, reward rate: 0.501, rewarded fraction: 0.575, relative distance: 134.400, critic loss: 0.813, actor loss: 23.575\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.690, mean reward: 6.218, reward rate: 0.490, rewarded fraction: 0.602, relative distance: 123.055, critic loss: 0.833, actor loss: 23.841\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.682, mean reward: 6.407, reward rate: 0.505, rewarded fraction: 0.621, relative distance: 119.161, critic loss: 0.852, actor loss: 24.077\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.367, mean reward: 6.263, reward rate: 0.506, rewarded fraction: 0.606, relative distance: 124.388, critic loss: 0.852, actor loss: 24.299\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 12.587, mean reward: 6.353, reward rate: 0.505, rewarded fraction: 0.610, relative distance: 117.675, critic loss: 0.863, actor loss: 24.524\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.252, mean reward: 6.173, reward rate: 0.504, rewarded fraction: 0.592, relative distance: 121.809, critic loss: 0.868, actor loss: 24.754\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.442, mean reward: 6.553, reward rate: 0.527, rewarded fraction: 0.635, relative distance: 116.243, critic loss: 0.876, actor loss: 24.990\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 12.554, mean reward: 6.163, reward rate: 0.491, rewarded fraction: 0.585, relative distance: 120.638, critic loss: 0.894, actor loss: 25.137\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 13.105, mean reward: 6.519, reward rate: 0.497, rewarded fraction: 0.627, relative distance: 113.986, critic loss: 0.888, actor loss: 25.302\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.653, mean reward: 6.415, reward rate: 0.507, rewarded fraction: 0.616, relative distance: 116.657, critic loss: 0.878, actor loss: 25.415\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.450, mean reward: 6.356, reward rate: 0.511, rewarded fraction: 0.614, relative distance: 121.390, critic loss: 0.881, actor loss: 25.511\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.094, mean reward: 5.967, reward rate: 0.493, rewarded fraction: 0.574, relative distance: 130.675, critic loss: 0.890, actor loss: 25.624\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.177, mean reward: 6.144, reward rate: 0.505, rewarded fraction: 0.594, relative distance: 127.750, critic loss: 0.895, actor loss: 25.643\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 12.116, mean reward: 6.159, reward rate: 0.508, rewarded fraction: 0.593, relative distance: 126.164, critic loss: 0.887, actor loss: 25.773\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.401, mean reward: 6.218, reward rate: 0.501, rewarded fraction: 0.600, relative distance: 123.927, critic loss: 0.895, actor loss: 25.819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.757, mean reward: 6.550, reward rate: 0.513, rewarded fraction: 0.630, relative distance: 113.194, critic loss: 0.888, actor loss: 25.888\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 12.619, mean reward: 6.160, reward rate: 0.488, rewarded fraction: 0.592, relative distance: 125.143, critic loss: 0.878, actor loss: 25.962\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 12.314, mean reward: 6.306, reward rate: 0.512, rewarded fraction: 0.607, relative distance: 120.563, critic loss: 0.882, actor loss: 25.993\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 12.999, mean reward: 6.819, reward rate: 0.525, rewarded fraction: 0.661, relative distance: 108.647, critic loss: 0.890, actor loss: 26.043\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 13.065, mean reward: 7.007, reward rate: 0.536, rewarded fraction: 0.684, relative distance: 108.487, critic loss: 0.894, actor loss: 26.131\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.539, mean reward: 6.092, reward rate: 0.486, rewarded fraction: 0.588, relative distance: 126.578, critic loss: 0.886, actor loss: 26.138\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 357.687, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 378.369, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 4.750, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 368.633, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 324.512, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 12.779, mean reward: 0.495, reward rate: 0.039, rewarded fraction: 0.040, relative distance: 240.145, critic loss: 0.057, actor loss: 0.168\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.151, mean reward: 0.456, reward rate: 0.028, rewarded fraction: 0.037, relative distance: 252.555, critic loss: 0.119, actor loss: 0.951\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 12.245, mean reward: 0.266, reward rate: 0.022, rewarded fraction: 0.019, relative distance: 259.562, critic loss: 0.136, actor loss: 1.018\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 7.478, mean reward: 0.120, reward rate: 0.016, rewarded fraction: 0.006, relative distance: 253.320, critic loss: 0.155, actor loss: 1.892\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.784, mean reward: 0.341, reward rate: 0.020, rewarded fraction: 0.027, relative distance: 262.466, critic loss: 0.181, actor loss: 3.104\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 15.908, mean reward: 0.398, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 252.689, critic loss: 0.122, actor loss: 0.529\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.955, mean reward: 0.400, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 255.348, critic loss: 0.058, actor loss: 0.484\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 17.029, mean reward: 0.355, reward rate: 0.021, rewarded fraction: 0.027, relative distance: 263.466, critic loss: 0.049, actor loss: 0.482\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 17.510, mean reward: 0.345, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 260.987, critic loss: 0.043, actor loss: 0.412\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.235, mean reward: 0.257, reward rate: 0.016, rewarded fraction: 0.016, relative distance: 260.276, critic loss: 0.033, actor loss: 0.330\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 17.182, mean reward: 0.238, reward rate: 0.014, rewarded fraction: 0.017, relative distance: 262.782, critic loss: 0.029, actor loss: 0.257\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.759, mean reward: 0.339, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 252.137, critic loss: 0.027, actor loss: 0.207\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.654, mean reward: 0.505, reward rate: 0.030, rewarded fraction: 0.043, relative distance: 256.836, critic loss: 0.027, actor loss: 0.167\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 16.802, mean reward: 0.353, reward rate: 0.021, rewarded fraction: 0.021, relative distance: 251.906, critic loss: 0.027, actor loss: 0.130\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.408, mean reward: 0.270, reward rate: 0.016, rewarded fraction: 0.020, relative distance: 262.136, critic loss: 0.026, actor loss: 0.091\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.545, mean reward: 0.279, reward rate: 0.017, rewarded fraction: 0.018, relative distance: 262.702, critic loss: 0.027, actor loss: 0.053\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.905, mean reward: 0.318, reward rate: 0.019, rewarded fraction: 0.021, relative distance: 252.173, critic loss: 0.028, actor loss: 0.019\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 17.008, mean reward: 0.486, reward rate: 0.029, rewarded fraction: 0.038, relative distance: 256.681, critic loss: 0.027, actor loss: -0.007\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 17.837, mean reward: 0.293, reward rate: 0.016, rewarded fraction: 0.019, relative distance: 261.325, critic loss: 0.028, actor loss: -0.024\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.484, mean reward: 0.356, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 253.951, critic loss: 0.027, actor loss: -0.036\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.757, mean reward: 0.343, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 260.471, critic loss: 0.027, actor loss: -0.046\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 14.817, mean reward: 0.339, reward rate: 0.023, rewarded fraction: 0.026, relative distance: 248.992, critic loss: 0.026, actor loss: -0.052\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 17.144, mean reward: 0.378, reward rate: 0.022, rewarded fraction: 0.029, relative distance: 263.061, critic loss: 0.023, actor loss: -0.055\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.717, mean reward: 0.327, reward rate: 0.020, rewarded fraction: 0.018, relative distance: 251.895, critic loss: 0.022, actor loss: -0.054\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 15.977, mean reward: 0.359, reward rate: 0.022, rewarded fraction: 0.025, relative distance: 252.894, critic loss: 0.023, actor loss: -0.051\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.877, mean reward: 0.495, reward rate: 0.029, rewarded fraction: 0.042, relative distance: 251.557, critic loss: 0.022, actor loss: -0.048\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.304, mean reward: 0.361, reward rate: 0.022, rewarded fraction: 0.024, relative distance: 255.840, critic loss: 0.021, actor loss: -0.045\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 15.994, mean reward: 0.382, reward rate: 0.024, rewarded fraction: 0.027, relative distance: 250.252, critic loss: 0.020, actor loss: -0.045\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 16.095, mean reward: 0.323, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 260.416, critic loss: 0.020, actor loss: -0.044\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 17.046, mean reward: 0.184, reward rate: 0.011, rewarded fraction: 0.008, relative distance: 263.521, critic loss: 0.018, actor loss: -0.041\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 16.174, mean reward: 0.414, reward rate: 0.026, rewarded fraction: 0.031, relative distance: 252.432, critic loss: 0.017, actor loss: -0.039\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.362, mean reward: 0.408, reward rate: 0.025, rewarded fraction: 0.032, relative distance: 254.558, critic loss: 0.017, actor loss: -0.038\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 17.848, mean reward: 0.337, reward rate: 0.019, rewarded fraction: 0.023, relative distance: 263.716, critic loss: 0.018, actor loss: -0.038\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 16.242, mean reward: 0.382, reward rate: 0.024, rewarded fraction: 0.024, relative distance: 243.155, critic loss: 0.016, actor loss: -0.038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 15.639, mean reward: 0.300, reward rate: 0.019, rewarded fraction: 0.021, relative distance: 253.920, critic loss: 0.016, actor loss: -0.039\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 17.103, mean reward: 0.444, reward rate: 0.026, rewarded fraction: 0.034, relative distance: 252.648, critic loss: 0.016, actor loss: -0.040\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 16.440, mean reward: 0.490, reward rate: 0.030, rewarded fraction: 0.040, relative distance: 254.675, critic loss: 0.016, actor loss: -0.041\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 17.444, mean reward: 0.332, reward rate: 0.019, rewarded fraction: 0.024, relative distance: 256.768, critic loss: 0.015, actor loss: -0.043\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 16.463, mean reward: 0.279, reward rate: 0.017, rewarded fraction: 0.018, relative distance: 260.825, critic loss: 0.014, actor loss: -0.046\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 17.125, mean reward: 0.442, reward rate: 0.026, rewarded fraction: 0.032, relative distance: 245.220, critic loss: 0.014, actor loss: -0.047\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 17.371, mean reward: 0.412, reward rate: 0.024, rewarded fraction: 0.026, relative distance: 250.658, critic loss: 0.014, actor loss: -0.049\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 16.967, mean reward: 0.375, reward rate: 0.022, rewarded fraction: 0.029, relative distance: 258.421, critic loss: 0.014, actor loss: -0.049\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 17.371, mean reward: 0.333, reward rate: 0.019, rewarded fraction: 0.025, relative distance: 263.273, critic loss: 0.013, actor loss: -0.048\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 16.398, mean reward: 0.317, reward rate: 0.019, rewarded fraction: 0.022, relative distance: 261.129, critic loss: 0.014, actor loss: -0.048\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.669, mean reward: 0.247, reward rate: 0.015, rewarded fraction: 0.019, relative distance: 268.734, critic loss: 0.014, actor loss: -0.048\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 17.613, mean reward: 0.249, reward rate: 0.014, rewarded fraction: 0.017, relative distance: 262.331, critic loss: 0.014, actor loss: -0.048\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 16.477, mean reward: 0.353, reward rate: 0.021, rewarded fraction: 0.023, relative distance: 258.979, critic loss: 0.014, actor loss: -0.047\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 17.023, mean reward: 0.432, reward rate: 0.025, rewarded fraction: 0.032, relative distance: 260.489, critic loss: 0.013, actor loss: -0.047\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 16.248, mean reward: 0.365, reward rate: 0.022, rewarded fraction: 0.025, relative distance: 252.787, critic loss: 0.013, actor loss: -0.046\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 16.667, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 256.556, critic loss: 0.013, actor loss: -0.045\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 16.057, mean reward: 0.539, reward rate: 0.034, rewarded fraction: 0.045, relative distance: 254.822, critic loss: 0.013, actor loss: -0.044\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 15.818, mean reward: 0.396, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 256.719, critic loss: 0.013, actor loss: -0.043\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 16.988, mean reward: 0.428, reward rate: 0.025, rewarded fraction: 0.033, relative distance: 254.844, critic loss: 0.012, actor loss: -0.042\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 16.715, mean reward: 0.399, reward rate: 0.024, rewarded fraction: 0.028, relative distance: 257.781, critic loss: 0.014, actor loss: -0.040\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 16.227, mean reward: 0.507, reward rate: 0.031, rewarded fraction: 0.042, relative distance: 255.941, critic loss: 0.013, actor loss: -0.038\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 17.686, mean reward: 0.411, reward rate: 0.023, rewarded fraction: 0.033, relative distance: 257.060, critic loss: 0.013, actor loss: -0.038\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 17.251, mean reward: 0.432, reward rate: 0.025, rewarded fraction: 0.035, relative distance: 257.769, critic loss: 0.011, actor loss: -0.037\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 16.451, mean reward: 0.443, reward rate: 0.027, rewarded fraction: 0.033, relative distance: 252.252, critic loss: 0.013, actor loss: -0.037\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 16.698, mean reward: 0.190, reward rate: 0.011, rewarded fraction: 0.012, relative distance: 265.828, critic loss: 0.013, actor loss: -0.036\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 17.174, mean reward: 0.278, reward rate: 0.016, rewarded fraction: 0.017, relative distance: 258.979, critic loss: 0.011, actor loss: -0.036\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 17.982, mean reward: 0.356, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 249.369, critic loss: 0.013, actor loss: -0.037\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 17.431, mean reward: 0.196, reward rate: 0.011, rewarded fraction: 0.013, relative distance: 264.895, critic loss: 0.011, actor loss: -0.037\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 16.460, mean reward: 0.321, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 253.156, critic loss: 0.010, actor loss: -0.037\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 17.367, mean reward: 0.285, reward rate: 0.016, rewarded fraction: 0.019, relative distance: 254.095, critic loss: 0.012, actor loss: -0.036\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 16.597, mean reward: 0.456, reward rate: 0.028, rewarded fraction: 0.038, relative distance: 259.970, critic loss: 0.012, actor loss: -0.036\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 17.340, mean reward: 0.353, reward rate: 0.020, rewarded fraction: 0.023, relative distance: 254.399, critic loss: 0.011, actor loss: -0.036\n",
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 16.548, mean reward: 0.232, reward rate: 0.014, rewarded fraction: 0.010, relative distance: 258.492, critic loss: 0.011, actor loss: -0.036\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 17.223, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 259.621, critic loss: 0.010, actor loss: -0.036\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 15.902, mean reward: 0.286, reward rate: 0.018, rewarded fraction: 0.020, relative distance: 262.873, critic loss: 0.010, actor loss: -0.036\n",
      "t: 740000, Ep: 7399, action std: 0.80\n",
      "mean steps: 15.901, mean reward: 0.391, reward rate: 0.025, rewarded fraction: 0.028, relative distance: 250.587, critic loss: 0.011, actor loss: -0.035\n",
      "t: 750000, Ep: 7499, action std: 0.80\n",
      "mean steps: 16.640, mean reward: 0.363, reward rate: 0.022, rewarded fraction: 0.024, relative distance: 257.841, critic loss: 0.011, actor loss: -0.035\n",
      "t: 760000, Ep: 7599, action std: 0.80\n",
      "mean steps: 16.535, mean reward: 0.355, reward rate: 0.021, rewarded fraction: 0.028, relative distance: 263.111, critic loss: 0.011, actor loss: -0.035\n",
      "t: 770000, Ep: 7699, action std: 0.80\n",
      "mean steps: 17.033, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.027, relative distance: 258.739, critic loss: 0.010, actor loss: -0.035\n",
      "t: 780000, Ep: 7799, action std: 0.80\n",
      "mean steps: 16.715, mean reward: 0.345, reward rate: 0.021, rewarded fraction: 0.024, relative distance: 269.082, critic loss: 0.010, actor loss: -0.034\n",
      "t: 790000, Ep: 7899, action std: 0.80\n",
      "mean steps: 16.789, mean reward: 0.341, reward rate: 0.020, rewarded fraction: 0.026, relative distance: 258.080, critic loss: 0.010, actor loss: -0.033\n",
      "t: 800000, Ep: 7999, action std: 0.80\n",
      "mean steps: 16.482, mean reward: 0.356, reward rate: 0.022, rewarded fraction: 0.024, relative distance: 252.587, critic loss: 0.009, actor loss: -0.032\n",
      "t: 810000, Ep: 8099, action std: 0.80\n",
      "mean steps: 16.926, mean reward: 0.261, reward rate: 0.015, rewarded fraction: 0.014, relative distance: 255.698, critic loss: 0.010, actor loss: -0.032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 820000, Ep: 8199, action std: 0.80\n",
      "mean steps: 17.280, mean reward: 0.465, reward rate: 0.027, rewarded fraction: 0.036, relative distance: 249.075, critic loss: 0.010, actor loss: -0.031\n",
      "t: 830000, Ep: 8299, action std: 0.80\n",
      "mean steps: 17.002, mean reward: 0.313, reward rate: 0.018, rewarded fraction: 0.023, relative distance: 262.044, critic loss: 0.010, actor loss: -0.031\n",
      "t: 840000, Ep: 8399, action std: 0.80\n",
      "mean steps: 17.976, mean reward: 0.274, reward rate: 0.015, rewarded fraction: 0.019, relative distance: 260.547, critic loss: 0.010, actor loss: -0.030\n",
      "t: 850000, Ep: 8499, action std: 0.80\n",
      "mean steps: 15.741, mean reward: 0.362, reward rate: 0.023, rewarded fraction: 0.026, relative distance: 258.518, critic loss: 0.010, actor loss: -0.029\n",
      "t: 860000, Ep: 8599, action std: 0.80\n",
      "mean steps: 16.476, mean reward: 0.436, reward rate: 0.026, rewarded fraction: 0.034, relative distance: 258.792, critic loss: 0.011, actor loss: -0.028\n",
      "t: 870000, Ep: 8699, action std: 0.80\n",
      "mean steps: 18.016, mean reward: 0.281, reward rate: 0.016, rewarded fraction: 0.020, relative distance: 263.188, critic loss: 0.010, actor loss: -0.028\n",
      "t: 880000, Ep: 8799, action std: 0.80\n",
      "mean steps: 16.271, mean reward: 0.245, reward rate: 0.015, rewarded fraction: 0.016, relative distance: 264.119, critic loss: 0.009, actor loss: -0.028\n",
      "t: 890000, Ep: 8899, action std: 0.80\n",
      "mean steps: 16.731, mean reward: 0.329, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 255.749, critic loss: 0.010, actor loss: -0.028\n",
      "t: 900000, Ep: 8999, action std: 0.80\n",
      "mean steps: 16.989, mean reward: 0.281, reward rate: 0.017, rewarded fraction: 0.017, relative distance: 256.963, critic loss: 0.009, actor loss: -0.028\n",
      "t: 910000, Ep: 9099, action std: 0.80\n",
      "mean steps: 16.439, mean reward: 0.365, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 258.386, critic loss: 0.010, actor loss: -0.027\n",
      "t: 920000, Ep: 9199, action std: 0.80\n",
      "mean steps: 16.753, mean reward: 0.269, reward rate: 0.016, rewarded fraction: 0.017, relative distance: 252.409, critic loss: 0.009, actor loss: -0.026\n",
      "t: 930000, Ep: 9299, action std: 0.80\n",
      "mean steps: 16.042, mean reward: 0.303, reward rate: 0.019, rewarded fraction: 0.017, relative distance: 245.863, critic loss: 0.010, actor loss: -0.027\n",
      "t: 940000, Ep: 9399, action std: 0.80\n",
      "mean steps: 18.141, mean reward: 0.459, reward rate: 0.025, rewarded fraction: 0.034, relative distance: 264.517, critic loss: 0.010, actor loss: -0.026\n",
      "t: 950000, Ep: 9499, action std: 0.80\n",
      "mean steps: 17.730, mean reward: 0.403, reward rate: 0.023, rewarded fraction: 0.029, relative distance: 258.156, critic loss: 0.010, actor loss: -0.026\n",
      "t: 960000, Ep: 9599, action std: 0.80\n",
      "mean steps: 16.809, mean reward: 0.475, reward rate: 0.028, rewarded fraction: 0.040, relative distance: 260.250, critic loss: 0.010, actor loss: -0.025\n",
      "t: 970000, Ep: 9699, action std: 0.80\n",
      "mean steps: 15.868, mean reward: 0.409, reward rate: 0.026, rewarded fraction: 0.032, relative distance: 256.617, critic loss: 0.011, actor loss: -0.025\n",
      "t: 980000, Ep: 9799, action std: 0.80\n",
      "mean steps: 16.718, mean reward: 0.416, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 247.993, critic loss: 0.010, actor loss: -0.024\n",
      "t: 990000, Ep: 9899, action std: 0.80\n",
      "mean steps: 16.650, mean reward: 0.432, reward rate: 0.026, rewarded fraction: 0.031, relative distance: 248.741, critic loss: 0.011, actor loss: -0.024\n",
      "t: 1000000, Ep: 9999, action std: 0.80\n",
      "mean steps: 16.202, mean reward: 0.329, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 257.947, critic loss: 0.011, actor loss: -0.024\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 9.381, mean reward: 0.437, reward rate: 0.047, rewarded fraction: 0.033, relative distance: 231.959, critic loss: 0.055, actor loss: 0.320\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 15.196, mean reward: 0.528, reward rate: 0.035, rewarded fraction: 0.040, relative distance: 240.279, critic loss: 0.150, actor loss: 1.530\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 17.620, mean reward: 0.369, reward rate: 0.021, rewarded fraction: 0.026, relative distance: 255.221, critic loss: 0.151, actor loss: 2.243\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.885, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.025, relative distance: 252.706, critic loss: 0.113, actor loss: 2.186\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 17.260, mean reward: 0.318, reward rate: 0.018, rewarded fraction: 0.021, relative distance: 258.813, critic loss: 0.069, actor loss: 1.792\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.671, mean reward: 0.349, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 255.870, critic loss: 0.049, actor loss: 1.392\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 15.321, mean reward: 0.320, reward rate: 0.021, rewarded fraction: 0.023, relative distance: 250.140, critic loss: 0.035, actor loss: 1.141\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 17.243, mean reward: 0.396, reward rate: 0.023, rewarded fraction: 0.030, relative distance: 256.756, critic loss: 0.033, actor loss: 0.984\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.082, mean reward: 0.444, reward rate: 0.028, rewarded fraction: 0.036, relative distance: 254.345, critic loss: 0.029, actor loss: 0.849\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 15.399, mean reward: 0.305, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 263.376, critic loss: 0.028, actor loss: 0.715\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 17.738, mean reward: 0.369, reward rate: 0.021, rewarded fraction: 0.029, relative distance: 264.408, critic loss: 0.022, actor loss: 0.595\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.920, mean reward: 0.314, reward rate: 0.019, rewarded fraction: 0.020, relative distance: 256.846, critic loss: 0.022, actor loss: 0.491\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.896, mean reward: 0.286, reward rate: 0.017, rewarded fraction: 0.022, relative distance: 270.307, critic loss: 0.018, actor loss: 0.399\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 16.858, mean reward: 0.315, reward rate: 0.019, rewarded fraction: 0.023, relative distance: 259.029, critic loss: 0.019, actor loss: 0.318\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.685, mean reward: 0.346, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 259.035, critic loss: 0.016, actor loss: 0.253\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 17.405, mean reward: 0.293, reward rate: 0.017, rewarded fraction: 0.021, relative distance: 256.686, critic loss: 0.016, actor loss: 0.202\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.811, mean reward: 0.341, reward rate: 0.020, rewarded fraction: 0.023, relative distance: 262.153, critic loss: 0.016, actor loss: 0.155\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.769, mean reward: 0.358, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 248.671, critic loss: 0.018, actor loss: 0.110\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 15.856, mean reward: 0.319, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 260.504, critic loss: 0.017, actor loss: 0.068\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 16.682, mean reward: 0.340, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 254.909, critic loss: 0.016, actor loss: 0.034\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.768, mean reward: 0.396, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 262.001, critic loss: 0.019, actor loss: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 17.038, mean reward: 0.376, reward rate: 0.022, rewarded fraction: 0.023, relative distance: 252.676, critic loss: 0.019, actor loss: -0.015\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 17.430, mean reward: 0.318, reward rate: 0.018, rewarded fraction: 0.021, relative distance: 256.097, critic loss: 0.019, actor loss: -0.031\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 17.462, mean reward: 0.377, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 260.119, critic loss: 0.018, actor loss: -0.046\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.888, mean reward: 0.268, reward rate: 0.016, rewarded fraction: 0.017, relative distance: 261.576, critic loss: 0.017, actor loss: -0.056\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.446, mean reward: 0.429, reward rate: 0.026, rewarded fraction: 0.032, relative distance: 254.573, critic loss: 0.017, actor loss: -0.063\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 17.145, mean reward: 0.502, reward rate: 0.029, rewarded fraction: 0.042, relative distance: 258.045, critic loss: 0.016, actor loss: -0.067\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.709, mean reward: 0.418, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 254.633, critic loss: 0.018, actor loss: -0.068\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 16.735, mean reward: 0.308, reward rate: 0.018, rewarded fraction: 0.020, relative distance: 254.531, critic loss: 0.017, actor loss: -0.068\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 16.677, mean reward: 0.394, reward rate: 0.024, rewarded fraction: 0.028, relative distance: 255.137, critic loss: 0.017, actor loss: -0.068\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 16.744, mean reward: 0.387, reward rate: 0.023, rewarded fraction: 0.027, relative distance: 254.319, critic loss: 0.018, actor loss: -0.066\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.847, mean reward: 0.418, reward rate: 0.025, rewarded fraction: 0.029, relative distance: 236.545, critic loss: 0.017, actor loss: -0.064\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 16.647, mean reward: 0.690, reward rate: 0.041, rewarded fraction: 0.052, relative distance: 233.553, critic loss: 0.019, actor loss: -0.063\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 15.631, mean reward: 0.699, reward rate: 0.045, rewarded fraction: 0.056, relative distance: 238.775, critic loss: 0.023, actor loss: -0.068\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.445, mean reward: 0.579, reward rate: 0.035, rewarded fraction: 0.046, relative distance: 245.415, critic loss: 0.022, actor loss: -0.082\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 15.986, mean reward: 0.783, reward rate: 0.049, rewarded fraction: 0.061, relative distance: 227.081, critic loss: 0.024, actor loss: -0.094\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 15.782, mean reward: 0.451, reward rate: 0.029, rewarded fraction: 0.029, relative distance: 238.873, critic loss: 0.025, actor loss: -0.106\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 9.182, mean reward: 0.163, reward rate: 0.018, rewarded fraction: 0.011, relative distance: 269.603, critic loss: 0.205, actor loss: 0.106\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 10.652, mean reward: 0.087, reward rate: 0.008, rewarded fraction: 0.004, relative distance: 276.478, critic loss: 0.228, actor loss: 0.177\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 10.588, mean reward: 0.111, reward rate: 0.010, rewarded fraction: 0.006, relative distance: 284.444, critic loss: 0.193, actor loss: 0.114\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 10.693, mean reward: 0.129, reward rate: 0.012, rewarded fraction: 0.006, relative distance: 268.786, critic loss: 0.186, actor loss: 0.122\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 10.070, mean reward: 0.190, reward rate: 0.019, rewarded fraction: 0.009, relative distance: 251.637, critic loss: 0.181, actor loss: 0.111\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 9.598, mean reward: 0.419, reward rate: 0.044, rewarded fraction: 0.029, relative distance: 240.221, critic loss: 0.208, actor loss: 0.210\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 6.681, mean reward: 0.281, reward rate: 0.042, rewarded fraction: 0.017, relative distance: 288.840, critic loss: 0.237, actor loss: 0.393\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 7.001, mean reward: 0.492, reward rate: 0.070, rewarded fraction: 0.037, relative distance: 289.846, critic loss: 0.261, actor loss: 0.656\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 6.324, mean reward: 0.512, reward rate: 0.081, rewarded fraction: 0.038, relative distance: 266.275, critic loss: 0.331, actor loss: 0.984\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 6.483, mean reward: 0.891, reward rate: 0.137, rewarded fraction: 0.075, relative distance: 232.089, critic loss: 0.429, actor loss: 1.449\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 6.592, mean reward: 1.129, reward rate: 0.171, rewarded fraction: 0.100, relative distance: 230.249, critic loss: 0.638, actor loss: 2.211\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 6.276, mean reward: 1.305, reward rate: 0.208, rewarded fraction: 0.117, relative distance: 228.812, critic loss: 0.722, actor loss: 3.146\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 7.408, mean reward: 2.548, reward rate: 0.344, rewarded fraction: 0.243, relative distance: 206.163, critic loss: 0.723, actor loss: 4.094\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 8.137, mean reward: 3.274, reward rate: 0.402, rewarded fraction: 0.314, relative distance: 193.708, critic loss: 0.781, actor loss: 5.138\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 8.568, mean reward: 3.566, reward rate: 0.416, rewarded fraction: 0.338, relative distance: 185.113, critic loss: 0.800, actor loss: 6.405\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 9.138, mean reward: 4.081, reward rate: 0.447, rewarded fraction: 0.390, relative distance: 175.472, critic loss: 0.777, actor loss: 7.687\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 8.827, mean reward: 3.814, reward rate: 0.432, rewarded fraction: 0.360, relative distance: 179.419, critic loss: 0.788, actor loss: 8.939\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 9.106, mean reward: 4.073, reward rate: 0.447, rewarded fraction: 0.389, relative distance: 178.139, critic loss: 0.811, actor loss: 10.101\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 9.566, mean reward: 4.331, reward rate: 0.453, rewarded fraction: 0.412, relative distance: 166.584, critic loss: 0.814, actor loss: 11.274\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 10.044, mean reward: 4.639, reward rate: 0.462, rewarded fraction: 0.438, relative distance: 160.003, critic loss: 0.818, actor loss: 12.416\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 9.968, mean reward: 4.618, reward rate: 0.463, rewarded fraction: 0.438, relative distance: 159.877, critic loss: 0.835, actor loss: 13.475\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 9.730, mean reward: 4.295, reward rate: 0.441, rewarded fraction: 0.410, relative distance: 173.389, critic loss: 0.866, actor loss: 14.449\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 9.640, mean reward: 4.419, reward rate: 0.458, rewarded fraction: 0.420, relative distance: 169.625, critic loss: 0.869, actor loss: 15.300\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 10.473, mean reward: 4.918, reward rate: 0.470, rewarded fraction: 0.465, relative distance: 149.938, critic loss: 0.863, actor loss: 16.148\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 10.411, mean reward: 4.962, reward rate: 0.477, rewarded fraction: 0.472, relative distance: 153.189, critic loss: 0.856, actor loss: 16.946\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 10.400, mean reward: 4.674, reward rate: 0.449, rewarded fraction: 0.443, relative distance: 158.591, critic loss: 0.860, actor loss: 17.667\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 10.895, mean reward: 5.335, reward rate: 0.490, rewarded fraction: 0.515, relative distance: 148.822, critic loss: 0.844, actor loss: 18.368\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 10.480, mean reward: 5.025, reward rate: 0.480, rewarded fraction: 0.484, relative distance: 155.601, critic loss: 0.864, actor loss: 18.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 10.888, mean reward: 5.059, reward rate: 0.465, rewarded fraction: 0.482, relative distance: 148.980, critic loss: 0.882, actor loss: 19.575\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 10.619, mean reward: 5.104, reward rate: 0.481, rewarded fraction: 0.487, relative distance: 152.544, critic loss: 0.900, actor loss: 20.090\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 10.866, mean reward: 5.220, reward rate: 0.480, rewarded fraction: 0.501, relative distance: 149.714, critic loss: 0.884, actor loss: 20.511\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 10.438, mean reward: 5.070, reward rate: 0.486, rewarded fraction: 0.490, relative distance: 157.380, critic loss: 0.895, actor loss: 20.945\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 10.820, mean reward: 5.178, reward rate: 0.479, rewarded fraction: 0.499, relative distance: 149.694, critic loss: 0.910, actor loss: 21.285\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 10.645, mean reward: 5.283, reward rate: 0.496, rewarded fraction: 0.507, relative distance: 146.398, critic loss: 0.928, actor loss: 21.564\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 10.588, mean reward: 5.006, reward rate: 0.473, rewarded fraction: 0.479, relative distance: 154.277, critic loss: 0.905, actor loss: 21.759\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 10.800, mean reward: 5.224, reward rate: 0.484, rewarded fraction: 0.502, relative distance: 150.042, critic loss: 0.919, actor loss: 21.990\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 10.925, mean reward: 5.447, reward rate: 0.499, rewarded fraction: 0.523, relative distance: 143.267, critic loss: 0.926, actor loss: 22.279\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 10.703, mean reward: 5.402, reward rate: 0.505, rewarded fraction: 0.526, relative distance: 151.723, critic loss: 0.901, actor loss: 22.512\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 10.745, mean reward: 5.337, reward rate: 0.497, rewarded fraction: 0.515, relative distance: 150.542, critic loss: 0.928, actor loss: 22.714\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 10.736, mean reward: 5.146, reward rate: 0.479, rewarded fraction: 0.497, relative distance: 153.893, critic loss: 0.931, actor loss: 22.867\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 11.136, mean reward: 5.470, reward rate: 0.491, rewarded fraction: 0.527, relative distance: 145.185, critic loss: 0.924, actor loss: 22.990\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 11.225, mean reward: 5.724, reward rate: 0.510, rewarded fraction: 0.550, relative distance: 135.128, critic loss: 0.931, actor loss: 23.167\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 10.926, mean reward: 5.309, reward rate: 0.486, rewarded fraction: 0.513, relative distance: 148.778, critic loss: 0.944, actor loss: 23.273\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 11.186, mean reward: 5.412, reward rate: 0.484, rewarded fraction: 0.519, relative distance: 143.642, critic loss: 0.915, actor loss: 23.342\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 11.122, mean reward: 5.609, reward rate: 0.504, rewarded fraction: 0.546, relative distance: 144.097, critic loss: 0.887, actor loss: 23.423\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 11.014, mean reward: 5.441, reward rate: 0.494, rewarded fraction: 0.524, relative distance: 143.787, critic loss: 0.954, actor loss: 23.573\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 11.202, mean reward: 5.484, reward rate: 0.490, rewarded fraction: 0.525, relative distance: 140.647, critic loss: 0.962, actor loss: 23.662\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 11.637, mean reward: 5.593, reward rate: 0.481, rewarded fraction: 0.537, relative distance: 137.918, critic loss: 0.911, actor loss: 23.742\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 11.853, mean reward: 5.710, reward rate: 0.482, rewarded fraction: 0.547, relative distance: 134.882, critic loss: 0.920, actor loss: 23.781\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 11.627, mean reward: 5.838, reward rate: 0.502, rewarded fraction: 0.563, relative distance: 133.183, critic loss: 0.888, actor loss: 23.821\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 11.477, mean reward: 5.695, reward rate: 0.496, rewarded fraction: 0.555, relative distance: 142.639, critic loss: 0.911, actor loss: 23.917\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 11.346, mean reward: 5.553, reward rate: 0.489, rewarded fraction: 0.533, relative distance: 140.707, critic loss: 0.944, actor loss: 24.012\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.180, mean reward: 5.856, reward rate: 0.481, rewarded fraction: 0.561, relative distance: 129.945, critic loss: 0.915, actor loss: 24.116\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 11.882, mean reward: 5.778, reward rate: 0.486, rewarded fraction: 0.556, relative distance: 133.404, critic loss: 0.939, actor loss: 24.206\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 11.605, mean reward: 5.664, reward rate: 0.488, rewarded fraction: 0.543, relative distance: 135.923, critic loss: 0.947, actor loss: 24.300\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 11.245, mean reward: 5.527, reward rate: 0.492, rewarded fraction: 0.530, relative distance: 141.206, critic loss: 0.955, actor loss: 24.297\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 11.675, mean reward: 5.552, reward rate: 0.476, rewarded fraction: 0.540, relative distance: 143.939, critic loss: 0.926, actor loss: 24.305\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 11.668, mean reward: 5.633, reward rate: 0.483, rewarded fraction: 0.541, relative distance: 137.043, critic loss: 0.989, actor loss: 24.349\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 11.751, mean reward: 5.803, reward rate: 0.494, rewarded fraction: 0.556, relative distance: 132.349, critic loss: 0.928, actor loss: 24.358\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 7.969, mean reward: 0.261, reward rate: 0.033, rewarded fraction: 0.017, relative distance: 245.106, critic loss: 0.025, actor loss: 0.094\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 13.620, mean reward: 0.781, reward rate: 0.057, rewarded fraction: 0.059, relative distance: 205.174, critic loss: 0.137, actor loss: 0.988\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.927, mean reward: 0.586, reward rate: 0.035, rewarded fraction: 0.045, relative distance: 234.498, critic loss: 0.164, actor loss: 1.901\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 15.148, mean reward: 0.317, reward rate: 0.021, rewarded fraction: 0.022, relative distance: 256.722, critic loss: 0.140, actor loss: 1.898\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 17.779, mean reward: 0.458, reward rate: 0.026, rewarded fraction: 0.035, relative distance: 256.898, critic loss: 0.107, actor loss: 1.612\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 15.990, mean reward: 0.279, reward rate: 0.017, rewarded fraction: 0.017, relative distance: 254.553, critic loss: 0.071, actor loss: 1.347\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 17.833, mean reward: 0.474, reward rate: 0.027, rewarded fraction: 0.038, relative distance: 253.492, critic loss: 0.052, actor loss: 1.125\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 17.058, mean reward: 0.380, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 259.193, critic loss: 0.047, actor loss: 0.948\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 17.450, mean reward: 0.434, reward rate: 0.025, rewarded fraction: 0.033, relative distance: 253.476, critic loss: 0.044, actor loss: 0.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.975, mean reward: 0.501, reward rate: 0.030, rewarded fraction: 0.038, relative distance: 245.726, critic loss: 0.046, actor loss: 0.683\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 17.101, mean reward: 0.456, reward rate: 0.027, rewarded fraction: 0.036, relative distance: 264.548, critic loss: 0.040, actor loss: 0.573\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.476, mean reward: 0.357, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 259.261, critic loss: 0.032, actor loss: 0.476\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 17.175, mean reward: 0.399, reward rate: 0.023, rewarded fraction: 0.028, relative distance: 255.226, critic loss: 0.030, actor loss: 0.392\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 17.448, mean reward: 0.359, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 254.167, critic loss: 0.027, actor loss: 0.320\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.966, mean reward: 0.436, reward rate: 0.026, rewarded fraction: 0.034, relative distance: 255.395, critic loss: 0.023, actor loss: 0.257\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 15.102, mean reward: 0.515, reward rate: 0.034, rewarded fraction: 0.040, relative distance: 250.234, critic loss: 0.023, actor loss: 0.208\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.599, mean reward: 0.462, reward rate: 0.028, rewarded fraction: 0.038, relative distance: 252.667, critic loss: 0.023, actor loss: 0.164\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 17.101, mean reward: 0.296, reward rate: 0.017, rewarded fraction: 0.019, relative distance: 257.325, critic loss: 0.022, actor loss: 0.121\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 17.070, mean reward: 0.263, reward rate: 0.015, rewarded fraction: 0.017, relative distance: 254.841, critic loss: 0.018, actor loss: 0.074\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.284, mean reward: 0.295, reward rate: 0.017, rewarded fraction: 0.017, relative distance: 252.269, critic loss: 0.018, actor loss: 0.032\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 17.094, mean reward: 0.214, reward rate: 0.012, rewarded fraction: 0.012, relative distance: 261.435, critic loss: 0.016, actor loss: -0.009\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 17.143, mean reward: 0.361, reward rate: 0.021, rewarded fraction: 0.024, relative distance: 252.449, critic loss: 0.017, actor loss: -0.045\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.326, mean reward: 0.381, reward rate: 0.023, rewarded fraction: 0.030, relative distance: 254.859, critic loss: 0.017, actor loss: -0.075\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.232, mean reward: 0.240, reward rate: 0.015, rewarded fraction: 0.014, relative distance: 260.410, critic loss: 0.016, actor loss: -0.101\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 15.935, mean reward: 0.417, reward rate: 0.026, rewarded fraction: 0.032, relative distance: 248.147, critic loss: 0.017, actor loss: -0.125\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 17.544, mean reward: 0.297, reward rate: 0.017, rewarded fraction: 0.024, relative distance: 263.248, critic loss: 0.019, actor loss: -0.146\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 17.806, mean reward: 0.456, reward rate: 0.026, rewarded fraction: 0.036, relative distance: 251.168, critic loss: 0.016, actor loss: -0.161\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 17.015, mean reward: 0.439, reward rate: 0.026, rewarded fraction: 0.034, relative distance: 249.711, critic loss: 0.016, actor loss: -0.171\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 17.272, mean reward: 0.445, reward rate: 0.026, rewarded fraction: 0.036, relative distance: 255.838, critic loss: 0.016, actor loss: -0.176\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 16.879, mean reward: 0.465, reward rate: 0.028, rewarded fraction: 0.037, relative distance: 253.185, critic loss: 0.017, actor loss: -0.177\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 16.491, mean reward: 0.457, reward rate: 0.028, rewarded fraction: 0.034, relative distance: 247.977, critic loss: 0.016, actor loss: -0.176\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 17.158, mean reward: 0.358, reward rate: 0.021, rewarded fraction: 0.025, relative distance: 258.252, critic loss: 0.016, actor loss: -0.175\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 17.177, mean reward: 0.436, reward rate: 0.025, rewarded fraction: 0.027, relative distance: 259.349, critic loss: 0.014, actor loss: -0.174\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 16.612, mean reward: 0.387, reward rate: 0.023, rewarded fraction: 0.028, relative distance: 255.806, critic loss: 0.013, actor loss: -0.172\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.948, mean reward: 0.368, reward rate: 0.022, rewarded fraction: 0.029, relative distance: 257.114, critic loss: 0.012, actor loss: -0.168\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 16.509, mean reward: 0.407, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 248.634, critic loss: 0.013, actor loss: -0.163\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 16.860, mean reward: 0.392, reward rate: 0.023, rewarded fraction: 0.027, relative distance: 256.351, critic loss: 0.014, actor loss: -0.159\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 17.678, mean reward: 0.512, reward rate: 0.029, rewarded fraction: 0.040, relative distance: 252.271, critic loss: 0.014, actor loss: -0.155\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 16.365, mean reward: 0.579, reward rate: 0.035, rewarded fraction: 0.046, relative distance: 249.353, critic loss: 0.015, actor loss: -0.151\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 16.166, mean reward: 0.483, reward rate: 0.030, rewarded fraction: 0.037, relative distance: 253.325, critic loss: 0.015, actor loss: -0.147\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 16.883, mean reward: 0.323, reward rate: 0.019, rewarded fraction: 0.021, relative distance: 252.658, critic loss: 0.014, actor loss: -0.144\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 17.198, mean reward: 0.322, reward rate: 0.019, rewarded fraction: 0.022, relative distance: 255.999, critic loss: 0.015, actor loss: -0.141\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 17.859, mean reward: 0.243, reward rate: 0.014, rewarded fraction: 0.015, relative distance: 265.794, critic loss: 0.013, actor loss: -0.137\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 17.151, mean reward: 0.307, reward rate: 0.018, rewarded fraction: 0.021, relative distance: 251.503, critic loss: 0.012, actor loss: -0.133\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 16.364, mean reward: 0.520, reward rate: 0.032, rewarded fraction: 0.041, relative distance: 256.629, critic loss: 0.013, actor loss: -0.127\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 17.151, mean reward: 0.478, reward rate: 0.028, rewarded fraction: 0.039, relative distance: 250.739, critic loss: 0.012, actor loss: -0.121\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 16.810, mean reward: 0.315, reward rate: 0.019, rewarded fraction: 0.022, relative distance: 262.099, critic loss: 0.012, actor loss: -0.115\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 17.154, mean reward: 0.307, reward rate: 0.018, rewarded fraction: 0.019, relative distance: 253.053, critic loss: 0.012, actor loss: -0.109\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 16.318, mean reward: 0.366, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 254.300, critic loss: 0.011, actor loss: -0.104\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 17.953, mean reward: 0.178, reward rate: 0.010, rewarded fraction: 0.009, relative distance: 256.835, critic loss: 0.009, actor loss: -0.097\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 17.262, mean reward: 0.439, reward rate: 0.025, rewarded fraction: 0.033, relative distance: 259.746, critic loss: 0.011, actor loss: -0.091\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 16.094, mean reward: 0.445, reward rate: 0.028, rewarded fraction: 0.031, relative distance: 252.432, critic loss: 0.010, actor loss: -0.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 16.574, mean reward: 0.395, reward rate: 0.024, rewarded fraction: 0.030, relative distance: 255.189, critic loss: 0.010, actor loss: -0.080\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 16.341, mean reward: 0.374, reward rate: 0.023, rewarded fraction: 0.027, relative distance: 260.769, critic loss: 0.010, actor loss: -0.074\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 15.950, mean reward: 0.356, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 254.091, critic loss: 0.009, actor loss: -0.070\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 16.915, mean reward: 0.425, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 247.954, critic loss: 0.010, actor loss: -0.066\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 16.224, mean reward: 0.419, reward rate: 0.026, rewarded fraction: 0.030, relative distance: 247.328, critic loss: 0.010, actor loss: -0.064\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 16.503, mean reward: 0.492, reward rate: 0.030, rewarded fraction: 0.039, relative distance: 248.691, critic loss: 0.011, actor loss: -0.062\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 16.501, mean reward: 0.340, reward rate: 0.021, rewarded fraction: 0.022, relative distance: 250.402, critic loss: 0.011, actor loss: -0.060\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 16.793, mean reward: 0.352, reward rate: 0.021, rewarded fraction: 0.026, relative distance: 259.509, critic loss: 0.011, actor loss: -0.058\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 17.910, mean reward: 0.355, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 255.162, critic loss: 0.010, actor loss: -0.055\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 17.145, mean reward: 0.558, reward rate: 0.033, rewarded fraction: 0.045, relative distance: 256.200, critic loss: 0.010, actor loss: -0.052\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 16.403, mean reward: 0.584, reward rate: 0.036, rewarded fraction: 0.046, relative distance: 245.645, critic loss: 0.010, actor loss: -0.050\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 16.506, mean reward: 0.491, reward rate: 0.030, rewarded fraction: 0.038, relative distance: 249.480, critic loss: 0.010, actor loss: -0.048\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 15.060, mean reward: 0.409, reward rate: 0.027, rewarded fraction: 0.027, relative distance: 248.120, critic loss: 0.011, actor loss: -0.048\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 17.227, mean reward: 0.479, reward rate: 0.028, rewarded fraction: 0.041, relative distance: 256.810, critic loss: 0.010, actor loss: -0.047\n",
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 16.861, mean reward: 0.380, reward rate: 0.023, rewarded fraction: 0.025, relative distance: 254.705, critic loss: 0.011, actor loss: -0.046\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 16.975, mean reward: 0.496, reward rate: 0.029, rewarded fraction: 0.039, relative distance: 249.590, critic loss: 0.011, actor loss: -0.046\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 17.645, mean reward: 0.473, reward rate: 0.027, rewarded fraction: 0.033, relative distance: 249.003, critic loss: 0.009, actor loss: -0.045\n",
      "t: 740000, Ep: 7399, action std: 0.80\n",
      "mean steps: 16.600, mean reward: 0.351, reward rate: 0.021, rewarded fraction: 0.024, relative distance: 260.652, critic loss: 0.010, actor loss: -0.044\n",
      "t: 750000, Ep: 7499, action std: 0.80\n",
      "mean steps: 17.226, mean reward: 0.440, reward rate: 0.026, rewarded fraction: 0.033, relative distance: 253.188, critic loss: 0.010, actor loss: -0.043\n",
      "t: 760000, Ep: 7599, action std: 0.80\n",
      "mean steps: 16.723, mean reward: 0.339, reward rate: 0.020, rewarded fraction: 0.024, relative distance: 262.986, critic loss: 0.010, actor loss: -0.042\n",
      "t: 770000, Ep: 7699, action std: 0.80\n",
      "mean steps: 16.836, mean reward: 0.440, reward rate: 0.026, rewarded fraction: 0.032, relative distance: 254.822, critic loss: 0.010, actor loss: -0.041\n",
      "t: 780000, Ep: 7799, action std: 0.80\n",
      "mean steps: 16.570, mean reward: 0.337, reward rate: 0.020, rewarded fraction: 0.020, relative distance: 253.595, critic loss: 0.009, actor loss: -0.040\n",
      "t: 790000, Ep: 7899, action std: 0.80\n",
      "mean steps: 15.103, mean reward: 0.378, reward rate: 0.025, rewarded fraction: 0.025, relative distance: 253.450, critic loss: 0.009, actor loss: -0.039\n",
      "t: 800000, Ep: 7999, action std: 0.80\n",
      "mean steps: 16.252, mean reward: 0.530, reward rate: 0.033, rewarded fraction: 0.040, relative distance: 250.537, critic loss: 0.010, actor loss: -0.039\n",
      "t: 810000, Ep: 8099, action std: 0.80\n",
      "mean steps: 17.132, mean reward: 0.581, reward rate: 0.034, rewarded fraction: 0.045, relative distance: 250.571, critic loss: 0.011, actor loss: -0.038\n",
      "t: 820000, Ep: 8199, action std: 0.80\n",
      "mean steps: 16.757, mean reward: 0.423, reward rate: 0.025, rewarded fraction: 0.033, relative distance: 258.061, critic loss: 0.010, actor loss: -0.038\n",
      "t: 830000, Ep: 8299, action std: 0.80\n",
      "mean steps: 17.826, mean reward: 0.482, reward rate: 0.027, rewarded fraction: 0.038, relative distance: 252.459, critic loss: 0.010, actor loss: -0.036\n",
      "t: 840000, Ep: 8399, action std: 0.80\n",
      "mean steps: 16.756, mean reward: 0.392, reward rate: 0.023, rewarded fraction: 0.029, relative distance: 254.903, critic loss: 0.009, actor loss: -0.036\n",
      "t: 850000, Ep: 8499, action std: 0.80\n",
      "mean steps: 16.974, mean reward: 0.399, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 253.676, critic loss: 0.010, actor loss: -0.036\n",
      "t: 860000, Ep: 8599, action std: 0.80\n",
      "mean steps: 17.118, mean reward: 0.368, reward rate: 0.022, rewarded fraction: 0.026, relative distance: 263.527, critic loss: 0.010, actor loss: -0.036\n",
      "t: 870000, Ep: 8699, action std: 0.80\n",
      "mean steps: 17.574, mean reward: 0.386, reward rate: 0.022, rewarded fraction: 0.027, relative distance: 259.305, critic loss: 0.010, actor loss: -0.037\n",
      "t: 880000, Ep: 8799, action std: 0.80\n",
      "mean steps: 16.967, mean reward: 0.292, reward rate: 0.017, rewarded fraction: 0.020, relative distance: 259.866, critic loss: 0.009, actor loss: -0.037\n",
      "t: 890000, Ep: 8899, action std: 0.80\n",
      "mean steps: 17.010, mean reward: 0.411, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 253.431, critic loss: 0.008, actor loss: -0.038\n",
      "t: 900000, Ep: 8999, action std: 0.80\n",
      "mean steps: 16.037, mean reward: 0.426, reward rate: 0.027, rewarded fraction: 0.036, relative distance: 259.196, critic loss: 0.008, actor loss: -0.038\n",
      "t: 910000, Ep: 9099, action std: 0.80\n",
      "mean steps: 17.328, mean reward: 0.380, reward rate: 0.022, rewarded fraction: 0.028, relative distance: 257.685, critic loss: 0.009, actor loss: -0.039\n",
      "t: 920000, Ep: 9199, action std: 0.80\n",
      "mean steps: 17.209, mean reward: 0.315, reward rate: 0.018, rewarded fraction: 0.023, relative distance: 256.792, critic loss: 0.010, actor loss: -0.039\n",
      "t: 930000, Ep: 9299, action std: 0.80\n",
      "mean steps: 16.789, mean reward: 0.404, reward rate: 0.024, rewarded fraction: 0.030, relative distance: 253.737, critic loss: 0.009, actor loss: -0.039\n",
      "t: 940000, Ep: 9399, action std: 0.80\n",
      "mean steps: 17.722, mean reward: 0.451, reward rate: 0.025, rewarded fraction: 0.036, relative distance: 257.772, critic loss: 0.008, actor loss: -0.039\n",
      "t: 950000, Ep: 9499, action std: 0.80\n",
      "mean steps: 15.823, mean reward: 0.273, reward rate: 0.017, rewarded fraction: 0.015, relative distance: 254.184, critic loss: 0.009, actor loss: -0.039\n",
      "t: 960000, Ep: 9599, action std: 0.80\n",
      "mean steps: 17.696, mean reward: 0.512, reward rate: 0.029, rewarded fraction: 0.043, relative distance: 256.750, critic loss: 0.009, actor loss: -0.040\n",
      "t: 970000, Ep: 9699, action std: 0.80\n",
      "mean steps: 16.405, mean reward: 0.379, reward rate: 0.023, rewarded fraction: 0.030, relative distance: 258.246, critic loss: 0.009, actor loss: -0.040\n",
      "t: 980000, Ep: 9799, action std: 0.80\n",
      "mean steps: 17.156, mean reward: 0.439, reward rate: 0.026, rewarded fraction: 0.036, relative distance: 254.272, critic loss: 0.010, actor loss: -0.041\n",
      "t: 990000, Ep: 9899, action std: 0.80\n",
      "mean steps: 17.550, mean reward: 0.417, reward rate: 0.024, rewarded fraction: 0.033, relative distance: 261.015, critic loss: 0.009, actor loss: -0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 1000000, Ep: 9999, action std: 0.80\n",
      "mean steps: 16.552, mean reward: 0.351, reward rate: 0.021, rewarded fraction: 0.022, relative distance: 258.627, critic loss: 0.009, actor loss: -0.040\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 12.330, mean reward: 0.039, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 366.768, critic loss: 0.004, actor loss: -0.010\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 17.410, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 439.402, critic loss: 0.004, actor loss: -0.010\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 12.504, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 372.086, critic loss: 0.003, actor loss: 0.050\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 13.658, mean reward: 0.129, reward rate: 0.009, rewarded fraction: 0.006, relative distance: 285.541, critic loss: 0.005, actor loss: 0.265\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 16.684, mean reward: 0.547, reward rate: 0.033, rewarded fraction: 0.043, relative distance: 244.455, critic loss: 0.033, actor loss: 0.557\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 17.375, mean reward: 0.415, reward rate: 0.024, rewarded fraction: 0.031, relative distance: 255.730, critic loss: 0.079, actor loss: 0.584\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 17.527, mean reward: 0.437, reward rate: 0.025, rewarded fraction: 0.032, relative distance: 254.146, critic loss: 0.080, actor loss: 0.512\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.056, mean reward: 0.299, reward rate: 0.019, rewarded fraction: 0.021, relative distance: 259.398, critic loss: 0.081, actor loss: 0.421\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.068, mean reward: 0.332, reward rate: 0.021, rewarded fraction: 0.024, relative distance: 257.148, critic loss: 0.073, actor loss: 0.309\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.472, mean reward: 0.322, reward rate: 0.020, rewarded fraction: 0.021, relative distance: 247.681, critic loss: 0.060, actor loss: 0.235\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 17.688, mean reward: 0.442, reward rate: 0.025, rewarded fraction: 0.036, relative distance: 264.128, critic loss: 0.053, actor loss: 0.195\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.448, mean reward: 0.328, reward rate: 0.020, rewarded fraction: 0.027, relative distance: 266.746, critic loss: 0.050, actor loss: 0.149\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.165, mean reward: 0.510, reward rate: 0.032, rewarded fraction: 0.037, relative distance: 243.907, critic loss: 0.047, actor loss: 0.111\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 17.322, mean reward: 0.367, reward rate: 0.021, rewarded fraction: 0.027, relative distance: 254.833, critic loss: 0.044, actor loss: 0.092\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.171, mean reward: 0.305, reward rate: 0.019, rewarded fraction: 0.020, relative distance: 250.829, critic loss: 0.038, actor loss: 0.101\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 15.736, mean reward: 0.148, reward rate: 0.009, rewarded fraction: 0.006, relative distance: 253.167, critic loss: 0.033, actor loss: 0.102\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.604, mean reward: 0.302, reward rate: 0.018, rewarded fraction: 0.021, relative distance: 233.728, critic loss: 0.028, actor loss: 0.078\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 17.025, mean reward: 0.244, reward rate: 0.014, rewarded fraction: 0.010, relative distance: 234.176, critic loss: 0.027, actor loss: 0.028\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.844, mean reward: 0.269, reward rate: 0.016, rewarded fraction: 0.018, relative distance: 234.145, critic loss: 0.026, actor loss: -0.034\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.050, mean reward: 0.328, reward rate: 0.019, rewarded fraction: 0.023, relative distance: 260.882, critic loss: 0.031, actor loss: -0.056\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.410, mean reward: 0.418, reward rate: 0.025, rewarded fraction: 0.031, relative distance: 245.514, critic loss: 0.027, actor loss: -0.023\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.846, mean reward: 0.357, reward rate: 0.021, rewarded fraction: 0.026, relative distance: 252.361, critic loss: 0.028, actor loss: 0.000\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 15.602, mean reward: 0.396, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 255.487, critic loss: 0.024, actor loss: 0.001\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.925, mean reward: 0.239, reward rate: 0.014, rewarded fraction: 0.016, relative distance: 262.822, critic loss: 0.025, actor loss: -0.013\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 15.783, mean reward: 0.209, reward rate: 0.013, rewarded fraction: 0.011, relative distance: 263.222, critic loss: 0.028, actor loss: -0.035\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 16.316, mean reward: 0.454, reward rate: 0.028, rewarded fraction: 0.033, relative distance: 222.609, critic loss: 0.020, actor loss: -0.040\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 15.517, mean reward: 0.515, reward rate: 0.033, rewarded fraction: 0.044, relative distance: 280.100, critic loss: 0.022, actor loss: -0.044\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 8.106, mean reward: 0.217, reward rate: 0.027, rewarded fraction: 0.013, relative distance: 244.553, critic loss: 0.208, actor loss: 0.181\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 8.186, mean reward: 0.772, reward rate: 0.094, rewarded fraction: 0.060, relative distance: 205.265, critic loss: 0.261, actor loss: 0.379\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 8.005, mean reward: 0.767, reward rate: 0.096, rewarded fraction: 0.062, relative distance: 209.635, critic loss: 0.551, actor loss: 1.002\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 7.926, mean reward: 1.045, reward rate: 0.132, rewarded fraction: 0.086, relative distance: 203.884, critic loss: 0.696, actor loss: 1.779\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 7.226, mean reward: 1.087, reward rate: 0.150, rewarded fraction: 0.096, relative distance: 220.180, critic loss: 0.639, actor loss: 2.478\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 6.933, mean reward: 1.256, reward rate: 0.181, rewarded fraction: 0.111, relative distance: 221.784, critic loss: 0.673, actor loss: 3.252\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 7.023, mean reward: 1.348, reward rate: 0.192, rewarded fraction: 0.117, relative distance: 219.180, critic loss: 0.763, actor loss: 4.184\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 6.959, mean reward: 1.376, reward rate: 0.198, rewarded fraction: 0.121, relative distance: 221.761, critic loss: 0.769, actor loss: 5.190\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 7.015, mean reward: 1.319, reward rate: 0.188, rewarded fraction: 0.115, relative distance: 221.395, critic loss: 0.800, actor loss: 6.209\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 7.252, mean reward: 1.651, reward rate: 0.228, rewarded fraction: 0.148, relative distance: 213.386, critic loss: 0.864, actor loss: 7.284\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 8.858, mean reward: 3.248, reward rate: 0.367, rewarded fraction: 0.304, relative distance: 181.826, critic loss: 0.919, actor loss: 8.359\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 9.101, mean reward: 3.410, reward rate: 0.375, rewarded fraction: 0.319, relative distance: 175.950, critic loss: 0.949, actor loss: 9.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 9.582, mean reward: 3.553, reward rate: 0.371, rewarded fraction: 0.329, relative distance: 173.533, critic loss: 0.970, actor loss: 10.383\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 9.898, mean reward: 4.227, reward rate: 0.427, rewarded fraction: 0.402, relative distance: 164.507, critic loss: 0.990, actor loss: 11.346\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 10.293, mean reward: 4.448, reward rate: 0.432, rewarded fraction: 0.420, relative distance: 158.124, critic loss: 0.989, actor loss: 12.315\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 9.955, mean reward: 4.243, reward rate: 0.426, rewarded fraction: 0.397, relative distance: 165.353, critic loss: 0.972, actor loss: 13.265\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 10.530, mean reward: 5.132, reward rate: 0.487, rewarded fraction: 0.492, relative distance: 149.354, critic loss: 0.938, actor loss: 14.174\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 11.282, mean reward: 5.349, reward rate: 0.474, rewarded fraction: 0.505, relative distance: 136.287, critic loss: 0.888, actor loss: 15.014\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 10.981, mean reward: 4.947, reward rate: 0.450, rewarded fraction: 0.467, relative distance: 149.204, critic loss: 0.882, actor loss: 15.815\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 11.946, mean reward: 5.197, reward rate: 0.435, rewarded fraction: 0.484, relative distance: 138.424, critic loss: 0.899, actor loss: 16.581\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 11.587, mean reward: 5.340, reward rate: 0.461, rewarded fraction: 0.502, relative distance: 137.617, critic loss: 0.906, actor loss: 17.279\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 12.266, mean reward: 5.755, reward rate: 0.469, rewarded fraction: 0.545, relative distance: 126.126, critic loss: 0.920, actor loss: 17.931\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 11.977, mean reward: 5.544, reward rate: 0.463, rewarded fraction: 0.522, relative distance: 133.317, critic loss: 0.931, actor loss: 18.527\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.447, mean reward: 5.656, reward rate: 0.454, rewarded fraction: 0.533, relative distance: 124.015, critic loss: 0.945, actor loss: 19.009\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 12.532, mean reward: 5.904, reward rate: 0.471, rewarded fraction: 0.561, relative distance: 123.995, critic loss: 0.947, actor loss: 19.430\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.711, mean reward: 5.827, reward rate: 0.458, rewarded fraction: 0.549, relative distance: 121.490, critic loss: 0.967, actor loss: 19.876\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.503, mean reward: 5.566, reward rate: 0.445, rewarded fraction: 0.519, relative distance: 123.058, critic loss: 0.971, actor loss: 20.236\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.140, mean reward: 5.814, reward rate: 0.479, rewarded fraction: 0.556, relative distance: 130.943, critic loss: 0.975, actor loss: 20.528\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 11.767, mean reward: 5.676, reward rate: 0.482, rewarded fraction: 0.538, relative distance: 133.039, critic loss: 0.981, actor loss: 20.836\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 11.698, mean reward: 5.683, reward rate: 0.486, rewarded fraction: 0.540, relative distance: 132.007, critic loss: 0.987, actor loss: 21.152\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 11.387, mean reward: 5.614, reward rate: 0.493, rewarded fraction: 0.540, relative distance: 137.473, critic loss: 1.009, actor loss: 21.338\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 11.653, mean reward: 5.842, reward rate: 0.501, rewarded fraction: 0.565, relative distance: 133.266, critic loss: 1.006, actor loss: 21.562\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 12.019, mean reward: 6.080, reward rate: 0.506, rewarded fraction: 0.588, relative distance: 127.481, critic loss: 1.006, actor loss: 21.820\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.369, mean reward: 6.105, reward rate: 0.494, rewarded fraction: 0.587, relative distance: 127.305, critic loss: 1.002, actor loss: 22.050\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.031, mean reward: 5.812, reward rate: 0.483, rewarded fraction: 0.554, relative distance: 131.341, critic loss: 0.996, actor loss: 22.209\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.421, mean reward: 5.957, reward rate: 0.480, rewarded fraction: 0.575, relative distance: 130.079, critic loss: 1.007, actor loss: 22.425\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.196, mean reward: 5.882, reward rate: 0.482, rewarded fraction: 0.565, relative distance: 129.445, critic loss: 1.011, actor loss: 22.601\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.327, mean reward: 5.983, reward rate: 0.485, rewarded fraction: 0.577, relative distance: 127.307, critic loss: 0.982, actor loss: 22.771\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.650, mean reward: 6.202, reward rate: 0.490, rewarded fraction: 0.594, relative distance: 118.577, critic loss: 1.003, actor loss: 22.943\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.382, mean reward: 6.222, reward rate: 0.503, rewarded fraction: 0.602, relative distance: 122.582, critic loss: 1.020, actor loss: 23.098\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.868, mean reward: 6.229, reward rate: 0.484, rewarded fraction: 0.597, relative distance: 118.959, critic loss: 1.013, actor loss: 23.254\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.292, mean reward: 6.063, reward rate: 0.493, rewarded fraction: 0.578, relative distance: 125.074, critic loss: 1.021, actor loss: 23.350\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.397, mean reward: 6.437, reward rate: 0.519, rewarded fraction: 0.619, relative distance: 116.398, critic loss: 1.035, actor loss: 23.458\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 12.720, mean reward: 6.320, reward rate: 0.497, rewarded fraction: 0.609, relative distance: 119.459, critic loss: 1.020, actor loss: 23.590\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.718, mean reward: 6.178, reward rate: 0.486, rewarded fraction: 0.593, relative distance: 124.422, critic loss: 1.008, actor loss: 23.702\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.209, mean reward: 6.032, reward rate: 0.494, rewarded fraction: 0.574, relative distance: 123.771, critic loss: 1.066, actor loss: 23.792\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 12.095, mean reward: 5.895, reward rate: 0.487, rewarded fraction: 0.568, relative distance: 132.470, critic loss: 1.041, actor loss: 23.874\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.919, mean reward: 6.804, reward rate: 0.527, rewarded fraction: 0.660, relative distance: 109.132, critic loss: 1.044, actor loss: 23.942\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.662, mean reward: 6.221, reward rate: 0.491, rewarded fraction: 0.601, relative distance: 123.149, critic loss: 1.029, actor loss: 24.035\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.262, mean reward: 5.842, reward rate: 0.476, rewarded fraction: 0.557, relative distance: 130.731, critic loss: 1.037, actor loss: 24.143\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.588, mean reward: 6.260, reward rate: 0.497, rewarded fraction: 0.602, relative distance: 120.735, critic loss: 1.038, actor loss: 24.248\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.797, mean reward: 6.247, reward rate: 0.488, rewarded fraction: 0.603, relative distance: 123.008, critic loss: 1.035, actor loss: 24.246\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 13.229, mean reward: 6.540, reward rate: 0.494, rewarded fraction: 0.636, relative distance: 117.411, critic loss: 1.010, actor loss: 24.270\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.645, mean reward: 6.337, reward rate: 0.501, rewarded fraction: 0.612, relative distance: 121.773, critic loss: 1.009, actor loss: 24.341\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.503, mean reward: 6.357, reward rate: 0.508, rewarded fraction: 0.618, relative distance: 121.994, critic loss: 1.031, actor loss: 24.400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 13.287, mean reward: 6.503, reward rate: 0.489, rewarded fraction: 0.627, relative distance: 115.169, critic loss: 1.031, actor loss: 24.453\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 12.481, mean reward: 6.128, reward rate: 0.491, rewarded fraction: 0.590, relative distance: 125.673, critic loss: 1.033, actor loss: 24.509\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.419, mean reward: 6.211, reward rate: 0.500, rewarded fraction: 0.601, relative distance: 123.789, critic loss: 1.043, actor loss: 24.541\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.787, mean reward: 6.555, reward rate: 0.513, rewarded fraction: 0.626, relative distance: 110.818, critic loss: 1.062, actor loss: 24.593\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 13.194, mean reward: 6.457, reward rate: 0.489, rewarded fraction: 0.623, relative distance: 114.595, critic loss: 1.018, actor loss: 24.644\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.380, mean reward: 6.315, reward rate: 0.510, rewarded fraction: 0.609, relative distance: 121.821, critic loss: 1.040, actor loss: 24.718\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 12.962, mean reward: 6.592, reward rate: 0.509, rewarded fraction: 0.638, relative distance: 115.546, critic loss: 1.036, actor loss: 24.706\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 13.007, mean reward: 6.718, reward rate: 0.516, rewarded fraction: 0.650, relative distance: 106.898, critic loss: 1.014, actor loss: 24.782\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.174, mean reward: 6.415, reward rate: 0.487, rewarded fraction: 0.617, relative distance: 115.628, critic loss: 1.036, actor loss: 24.821\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.082, mean reward: 6.300, reward rate: 0.482, rewarded fraction: 0.597, relative distance: 111.523, critic loss: 1.063, actor loss: 24.832\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 13.032, mean reward: 6.600, reward rate: 0.506, rewarded fraction: 0.637, relative distance: 111.310, critic loss: 1.031, actor loss: 24.823\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 13.083, mean reward: 6.324, reward rate: 0.483, rewarded fraction: 0.609, relative distance: 118.429, critic loss: 1.013, actor loss: 24.834\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 13.263, mean reward: 6.720, reward rate: 0.507, rewarded fraction: 0.655, relative distance: 114.072, critic loss: 1.027, actor loss: 24.846\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.884, mean reward: 6.424, reward rate: 0.499, rewarded fraction: 0.622, relative distance: 120.498, critic loss: 1.015, actor loss: 24.916\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 10.000, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 327.874, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.135, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 437.003, critic loss: 0.000, actor loss: 0.019\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 15.969, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 445.026, critic loss: 0.000, actor loss: -0.001\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 17.562, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 444.020, critic loss: 0.000, actor loss: -0.002\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.164, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.022, critic loss: 0.000, actor loss: -0.002\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.039, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.020, critic loss: 0.000, actor loss: -0.004\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 15.630, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 421.581, critic loss: 0.000, actor loss: -0.006\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.114, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.677, critic loss: 0.000, actor loss: -0.007\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.506, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 420.944, critic loss: 0.000, actor loss: -0.007\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 15.902, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 417.332, critic loss: 0.000, actor loss: -0.006\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.131, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 412.030, critic loss: 0.000, actor loss: -0.005\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 15.735, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 404.581, critic loss: 0.000, actor loss: -0.006\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.955, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 414.490, critic loss: 0.000, actor loss: -0.007\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 17.046, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.533, critic loss: 0.000, actor loss: -0.008\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 15.828, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 419.110, critic loss: 0.000, actor loss: -0.009\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 15.241, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 407.258, critic loss: 0.000, actor loss: -0.009\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.121, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 411.711, critic loss: 0.000, actor loss: -0.009\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.027, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 423.162, critic loss: 0.000, actor loss: -0.009\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 17.164, mean reward: 0.035, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 418.107, critic loss: 0.001, actor loss: -0.009\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.307, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.971, critic loss: 0.002, actor loss: -0.009\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.099, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 436.410, critic loss: 0.001, actor loss: -0.009\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 16.205, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.438, critic loss: 0.001, actor loss: -0.010\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.165, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 412.170, critic loss: 0.002, actor loss: -0.010\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.744, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 430.643, critic loss: 0.002, actor loss: -0.011\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.832, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.062, critic loss: 0.001, actor loss: -0.011\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.475, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.348, critic loss: 0.002, actor loss: -0.011\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 17.295, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.522, critic loss: 0.001, actor loss: -0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.331, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.202, critic loss: 0.001, actor loss: -0.011\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.335, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.879, critic loss: 0.001, actor loss: -0.011\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 15.637, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 406.939, critic loss: 0.001, actor loss: -0.011\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 16.503, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.895, critic loss: 0.002, actor loss: -0.011\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 15.185, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 423.476, critic loss: 0.001, actor loss: -0.011\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 16.716, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.893, critic loss: 0.001, actor loss: -0.011\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 17.281, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 424.045, critic loss: 0.001, actor loss: -0.010\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 15.358, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.492, critic loss: 0.001, actor loss: -0.010\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.266, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.606, critic loss: 0.001, actor loss: -0.010\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 16.220, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.456, critic loss: 0.001, actor loss: -0.010\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 16.273, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.341, critic loss: 0.001, actor loss: -0.010\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 17.467, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 433.459, critic loss: 0.001, actor loss: -0.010\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 16.194, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 430.885, critic loss: 0.001, actor loss: -0.010\n",
      "t: 440000, Ep: 4399, action std: 0.80\n",
      "mean steps: 15.893, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.756, critic loss: 0.001, actor loss: -0.010\n",
      "t: 450000, Ep: 4499, action std: 0.80\n",
      "mean steps: 15.817, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.526, critic loss: 0.001, actor loss: -0.010\n",
      "t: 460000, Ep: 4599, action std: 0.80\n",
      "mean steps: 15.616, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 414.936, critic loss: 0.001, actor loss: -0.010\n",
      "t: 470000, Ep: 4699, action std: 0.80\n",
      "mean steps: 16.412, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.545, critic loss: 0.001, actor loss: -0.010\n",
      "t: 480000, Ep: 4799, action std: 0.80\n",
      "mean steps: 16.868, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 427.010, critic loss: 0.001, actor loss: -0.010\n",
      "t: 490000, Ep: 4899, action std: 0.80\n",
      "mean steps: 17.141, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 434.067, critic loss: 0.001, actor loss: -0.010\n",
      "t: 500000, Ep: 4999, action std: 0.80\n",
      "mean steps: 15.896, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 430.453, critic loss: 0.001, actor loss: -0.010\n",
      "t: 510000, Ep: 5099, action std: 0.80\n",
      "mean steps: 15.893, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 417.753, critic loss: 0.001, actor loss: -0.010\n",
      "t: 520000, Ep: 5199, action std: 0.80\n",
      "mean steps: 15.793, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 424.649, critic loss: 0.001, actor loss: -0.010\n",
      "t: 530000, Ep: 5299, action std: 0.80\n",
      "mean steps: 15.045, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 415.367, critic loss: 0.001, actor loss: -0.010\n",
      "t: 540000, Ep: 5399, action std: 0.80\n",
      "mean steps: 15.929, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 413.574, critic loss: 0.001, actor loss: -0.010\n",
      "t: 550000, Ep: 5499, action std: 0.80\n",
      "mean steps: 15.482, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 423.656, critic loss: 0.000, actor loss: -0.010\n",
      "t: 560000, Ep: 5599, action std: 0.80\n",
      "mean steps: 17.070, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 427.332, critic loss: 0.001, actor loss: -0.010\n",
      "t: 570000, Ep: 5699, action std: 0.80\n",
      "mean steps: 16.380, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.723, critic loss: 0.001, actor loss: -0.010\n",
      "t: 580000, Ep: 5799, action std: 0.80\n",
      "mean steps: 16.073, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 422.518, critic loss: 0.001, actor loss: -0.010\n",
      "t: 590000, Ep: 5899, action std: 0.80\n",
      "mean steps: 16.683, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 426.958, critic loss: 0.001, actor loss: -0.009\n",
      "t: 600000, Ep: 5999, action std: 0.80\n",
      "mean steps: 16.145, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 427.217, critic loss: 0.001, actor loss: -0.009\n",
      "t: 610000, Ep: 6099, action std: 0.80\n",
      "mean steps: 15.740, mean reward: 0.030, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 420.899, critic loss: 0.001, actor loss: -0.009\n",
      "t: 620000, Ep: 6199, action std: 0.80\n",
      "mean steps: 17.051, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.836, critic loss: 0.001, actor loss: -0.009\n",
      "t: 630000, Ep: 6299, action std: 0.80\n",
      "mean steps: 16.776, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 425.088, critic loss: 0.001, actor loss: -0.009\n",
      "t: 640000, Ep: 6399, action std: 0.80\n",
      "mean steps: 17.004, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 434.033, critic loss: 0.001, actor loss: -0.009\n",
      "t: 650000, Ep: 6499, action std: 0.80\n",
      "mean steps: 15.893, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 434.928, critic loss: 0.001, actor loss: -0.009\n",
      "t: 660000, Ep: 6599, action std: 0.80\n",
      "mean steps: 16.964, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 471.863, critic loss: 0.001, actor loss: -0.009\n",
      "t: 670000, Ep: 6699, action std: 0.80\n",
      "mean steps: 15.963, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 442.054, critic loss: 0.001, actor loss: -0.010\n",
      "t: 680000, Ep: 6799, action std: 0.80\n",
      "mean steps: 16.694, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 452.723, critic loss: 0.001, actor loss: -0.010\n",
      "t: 690000, Ep: 6899, action std: 0.80\n",
      "mean steps: 15.698, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 442.771, critic loss: 0.001, actor loss: -0.010\n",
      "t: 700000, Ep: 6999, action std: 0.80\n",
      "mean steps: 16.684, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 455.453, critic loss: 0.001, actor loss: -0.010\n",
      "t: 710000, Ep: 7099, action std: 0.80\n",
      "mean steps: 15.882, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 449.498, critic loss: 0.001, actor loss: -0.010\n",
      "t: 720000, Ep: 7199, action std: 0.80\n",
      "mean steps: 15.928, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 445.027, critic loss: 0.001, actor loss: -0.010\n",
      "t: 730000, Ep: 7299, action std: 0.80\n",
      "mean steps: 16.362, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 449.660, critic loss: 0.001, actor loss: -0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 740000, Ep: 7399, action std: 0.80\n",
      "mean steps: 16.598, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 459.148, critic loss: 0.001, actor loss: -0.010\n",
      "t: 750000, Ep: 7499, action std: 0.80\n",
      "mean steps: 15.143, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 436.518, critic loss: 0.001, actor loss: -0.010\n",
      "t: 760000, Ep: 7599, action std: 0.80\n",
      "mean steps: 16.094, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 452.768, critic loss: 0.001, actor loss: -0.010\n",
      "t: 770000, Ep: 7699, action std: 0.80\n",
      "mean steps: 16.535, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 454.890, critic loss: 0.001, actor loss: -0.010\n",
      "t: 780000, Ep: 7799, action std: 0.80\n",
      "mean steps: 15.373, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 445.270, critic loss: 0.001, actor loss: -0.010\n",
      "t: 790000, Ep: 7899, action std: 0.80\n",
      "mean steps: 15.329, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 442.929, critic loss: 0.001, actor loss: -0.010\n",
      "t: 800000, Ep: 7999, action std: 0.80\n",
      "mean steps: 16.145, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 449.045, critic loss: 0.000, actor loss: -0.010\n",
      "t: 810000, Ep: 8099, action std: 0.80\n",
      "mean steps: 16.173, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 444.412, critic loss: 0.001, actor loss: -0.010\n",
      "t: 820000, Ep: 8199, action std: 0.80\n",
      "mean steps: 16.722, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 452.427, critic loss: 0.001, actor loss: -0.011\n",
      "t: 830000, Ep: 8299, action std: 0.80\n",
      "mean steps: 16.226, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 443.017, critic loss: 0.001, actor loss: -0.011\n",
      "t: 840000, Ep: 8399, action std: 0.80\n",
      "mean steps: 16.095, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 451.481, critic loss: 0.001, actor loss: -0.011\n",
      "t: 850000, Ep: 8499, action std: 0.80\n",
      "mean steps: 17.094, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 464.087, critic loss: 0.001, actor loss: -0.011\n",
      "t: 860000, Ep: 8599, action std: 0.80\n",
      "mean steps: 16.436, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 448.246, critic loss: 0.001, actor loss: -0.011\n",
      "t: 870000, Ep: 8699, action std: 0.80\n",
      "mean steps: 16.165, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 445.191, critic loss: 0.001, actor loss: -0.011\n",
      "t: 880000, Ep: 8799, action std: 0.80\n",
      "mean steps: 15.713, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 443.961, critic loss: 0.001, actor loss: -0.011\n",
      "t: 890000, Ep: 8899, action std: 0.80\n",
      "mean steps: 16.058, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 449.143, critic loss: 0.001, actor loss: -0.011\n",
      "t: 900000, Ep: 8999, action std: 0.80\n",
      "mean steps: 16.229, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 444.645, critic loss: 0.001, actor loss: -0.012\n",
      "t: 910000, Ep: 9099, action std: 0.80\n",
      "mean steps: 16.074, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 445.080, critic loss: 0.001, actor loss: -0.012\n",
      "t: 920000, Ep: 9199, action std: 0.80\n",
      "mean steps: 17.293, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 456.369, critic loss: 0.001, actor loss: -0.012\n",
      "t: 930000, Ep: 9299, action std: 0.80\n",
      "mean steps: 16.160, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 439.606, critic loss: 0.001, actor loss: -0.012\n",
      "t: 940000, Ep: 9399, action std: 0.80\n",
      "mean steps: 15.701, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 447.165, critic loss: 0.001, actor loss: -0.012\n",
      "t: 950000, Ep: 9499, action std: 0.80\n",
      "mean steps: 16.516, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 452.439, critic loss: 0.001, actor loss: -0.012\n",
      "t: 960000, Ep: 9599, action std: 0.80\n",
      "mean steps: 16.469, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 452.268, critic loss: 0.001, actor loss: -0.011\n",
      "t: 970000, Ep: 9699, action std: 0.80\n",
      "mean steps: 16.206, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 449.272, critic loss: 0.001, actor loss: -0.012\n",
      "t: 980000, Ep: 9799, action std: 0.80\n",
      "mean steps: 15.678, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 442.696, critic loss: 0.001, actor loss: -0.012\n",
      "t: 990000, Ep: 9899, action std: 0.80\n",
      "mean steps: 18.468, mean reward: 0.010, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 475.103, critic loss: 0.001, actor loss: -0.012\n",
      "t: 1000000, Ep: 9999, action std: 0.80\n",
      "mean steps: 16.364, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 455.210, critic loss: 0.001, actor loss: -0.012\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 4.960, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 341.440, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 4.542, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 359.912, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 4.462, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 329.201, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 4.654, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 359.304, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 14.743, mean reward: 0.054, reward rate: 0.004, rewarded fraction: 0.002, relative distance: 326.434, critic loss: 0.005, actor loss: 0.030\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.329, mean reward: 0.036, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 322.024, critic loss: 0.005, actor loss: 0.038\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 17.262, mean reward: 0.051, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 316.430, critic loss: 0.007, actor loss: 0.030\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 8.084, mean reward: 0.030, reward rate: 0.004, rewarded fraction: 0.000, relative distance: 295.036, critic loss: 0.009, actor loss: 0.064\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.444, mean reward: 0.039, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 332.146, critic loss: 0.009, actor loss: 0.077\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.987, mean reward: 0.293, reward rate: 0.017, rewarded fraction: 0.019, relative distance: 272.451, critic loss: 0.019, actor loss: 0.148\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.013, mean reward: 0.277, reward rate: 0.017, rewarded fraction: 0.017, relative distance: 254.943, critic loss: 0.038, actor loss: 0.196\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 17.073, mean reward: 0.360, reward rate: 0.021, rewarded fraction: 0.029, relative distance: 264.971, critic loss: 0.049, actor loss: 0.143\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 16.703, mean reward: 0.450, reward rate: 0.027, rewarded fraction: 0.035, relative distance: 255.459, critic loss: 0.065, actor loss: 0.092\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.552, mean reward: 0.546, reward rate: 0.033, rewarded fraction: 0.040, relative distance: 247.229, critic loss: 0.062, actor loss: 0.038\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 16.254, mean reward: 0.738, reward rate: 0.045, rewarded fraction: 0.057, relative distance: 237.412, critic loss: 0.067, actor loss: -0.026\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.490, mean reward: 0.451, reward rate: 0.027, rewarded fraction: 0.033, relative distance: 250.463, critic loss: 0.064, actor loss: -0.033\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 16.432, mean reward: 0.321, reward rate: 0.020, rewarded fraction: 0.022, relative distance: 255.309, critic loss: 0.056, actor loss: -0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 16.057, mean reward: 0.533, reward rate: 0.033, rewarded fraction: 0.041, relative distance: 250.370, critic loss: 0.056, actor loss: -0.036\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 16.300, mean reward: 0.521, reward rate: 0.032, rewarded fraction: 0.038, relative distance: 242.686, critic loss: 0.055, actor loss: -0.054\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.124, mean reward: 0.401, reward rate: 0.025, rewarded fraction: 0.027, relative distance: 248.370, critic loss: 0.052, actor loss: -0.067\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 15.083, mean reward: 0.383, reward rate: 0.025, rewarded fraction: 0.024, relative distance: 227.704, critic loss: 0.052, actor loss: -0.074\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.128, mean reward: 0.180, reward rate: 0.011, rewarded fraction: 0.010, relative distance: 255.460, critic loss: 0.048, actor loss: -0.105\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 13.050, mean reward: 0.192, reward rate: 0.015, rewarded fraction: 0.013, relative distance: 263.760, critic loss: 0.098, actor loss: -0.084\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 9.823, mean reward: 0.535, reward rate: 0.054, rewarded fraction: 0.040, relative distance: 226.239, critic loss: 0.184, actor loss: 0.177\n",
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 8.543, mean reward: 1.139, reward rate: 0.133, rewarded fraction: 0.094, relative distance: 204.942, critic loss: 0.462, actor loss: 0.863\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 6.310, mean reward: 0.930, reward rate: 0.147, rewarded fraction: 0.078, relative distance: 229.878, critic loss: 0.968, actor loss: 2.658\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 7.224, mean reward: 1.291, reward rate: 0.179, rewarded fraction: 0.112, relative distance: 218.018, critic loss: 0.603, actor loss: 3.741\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 6.855, mean reward: 1.787, reward rate: 0.261, rewarded fraction: 0.162, relative distance: 220.107, critic loss: 0.671, actor loss: 4.470\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 7.470, mean reward: 2.536, reward rate: 0.340, rewarded fraction: 0.238, relative distance: 206.546, critic loss: 0.915, actor loss: 5.870\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 7.646, mean reward: 2.964, reward rate: 0.388, rewarded fraction: 0.283, relative distance: 200.373, critic loss: 0.915, actor loss: 7.319\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 7.782, mean reward: 3.123, reward rate: 0.401, rewarded fraction: 0.294, relative distance: 197.319, critic loss: 0.955, actor loss: 8.653\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 8.181, mean reward: 3.247, reward rate: 0.397, rewarded fraction: 0.306, relative distance: 194.546, critic loss: 1.038, actor loss: 9.951\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 8.836, mean reward: 3.946, reward rate: 0.447, rewarded fraction: 0.378, relative distance: 178.995, critic loss: 1.077, actor loss: 11.194\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 9.025, mean reward: 3.833, reward rate: 0.425, rewarded fraction: 0.363, relative distance: 180.880, critic loss: 1.059, actor loss: 12.434\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 9.356, mean reward: 4.372, reward rate: 0.467, rewarded fraction: 0.420, relative distance: 171.271, critic loss: 1.047, actor loss: 13.604\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 9.478, mean reward: 4.450, reward rate: 0.470, rewarded fraction: 0.428, relative distance: 168.157, critic loss: 1.040, actor loss: 14.607\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 9.709, mean reward: 4.426, reward rate: 0.456, rewarded fraction: 0.423, relative distance: 168.900, critic loss: 1.064, actor loss: 15.602\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 9.917, mean reward: 4.702, reward rate: 0.474, rewarded fraction: 0.451, relative distance: 164.215, critic loss: 1.075, actor loss: 16.483\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 10.373, mean reward: 4.928, reward rate: 0.475, rewarded fraction: 0.467, relative distance: 150.983, critic loss: 1.096, actor loss: 17.398\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 10.650, mean reward: 5.033, reward rate: 0.473, rewarded fraction: 0.481, relative distance: 152.987, critic loss: 1.097, actor loss: 18.223\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 10.751, mean reward: 5.184, reward rate: 0.482, rewarded fraction: 0.497, relative distance: 151.434, critic loss: 1.092, actor loss: 18.930\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 10.733, mean reward: 5.152, reward rate: 0.480, rewarded fraction: 0.498, relative distance: 153.677, critic loss: 1.084, actor loss: 19.588\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 10.792, mean reward: 5.194, reward rate: 0.481, rewarded fraction: 0.499, relative distance: 151.240, critic loss: 1.079, actor loss: 20.222\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 10.539, mean reward: 5.390, reward rate: 0.511, rewarded fraction: 0.519, relative distance: 147.096, critic loss: 1.073, actor loss: 20.737\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 10.835, mean reward: 5.440, reward rate: 0.502, rewarded fraction: 0.525, relative distance: 145.955, critic loss: 1.077, actor loss: 21.235\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 10.851, mean reward: 5.421, reward rate: 0.500, rewarded fraction: 0.522, relative distance: 147.129, critic loss: 1.055, actor loss: 21.698\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 11.740, mean reward: 5.828, reward rate: 0.496, rewarded fraction: 0.558, relative distance: 130.599, critic loss: 1.048, actor loss: 22.114\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 11.249, mean reward: 5.600, reward rate: 0.498, rewarded fraction: 0.541, relative distance: 143.322, critic loss: 1.038, actor loss: 22.492\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 11.600, mean reward: 5.827, reward rate: 0.502, rewarded fraction: 0.558, relative distance: 133.502, critic loss: 1.024, actor loss: 22.850\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 11.816, mean reward: 5.647, reward rate: 0.478, rewarded fraction: 0.542, relative distance: 141.034, critic loss: 1.025, actor loss: 23.109\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 11.985, mean reward: 6.087, reward rate: 0.508, rewarded fraction: 0.588, relative distance: 128.642, critic loss: 1.024, actor loss: 23.367\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 11.725, mean reward: 5.818, reward rate: 0.496, rewarded fraction: 0.563, relative distance: 136.989, critic loss: 1.012, actor loss: 23.558\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 12.501, mean reward: 6.350, reward rate: 0.508, rewarded fraction: 0.613, relative distance: 122.137, critic loss: 1.004, actor loss: 23.726\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 12.710, mean reward: 6.525, reward rate: 0.513, rewarded fraction: 0.629, relative distance: 115.398, critic loss: 1.002, actor loss: 23.861\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 12.097, mean reward: 6.047, reward rate: 0.500, rewarded fraction: 0.578, relative distance: 127.873, critic loss: 1.010, actor loss: 23.999\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 12.381, mean reward: 6.338, reward rate: 0.512, rewarded fraction: 0.612, relative distance: 121.511, critic loss: 1.007, actor loss: 24.112\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.304, mean reward: 5.935, reward rate: 0.482, rewarded fraction: 0.567, relative distance: 129.759, critic loss: 1.002, actor loss: 24.240\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.925, mean reward: 6.313, reward rate: 0.488, rewarded fraction: 0.608, relative distance: 119.993, critic loss: 0.987, actor loss: 24.337\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.527, mean reward: 6.590, reward rate: 0.526, rewarded fraction: 0.639, relative distance: 116.132, critic loss: 0.973, actor loss: 24.451\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.319, mean reward: 6.186, reward rate: 0.502, rewarded fraction: 0.596, relative distance: 125.600, critic loss: 1.003, actor loss: 24.498\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.663, mean reward: 6.322, reward rate: 0.499, rewarded fraction: 0.609, relative distance: 119.637, critic loss: 1.008, actor loss: 24.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 13.102, mean reward: 6.395, reward rate: 0.488, rewarded fraction: 0.612, relative distance: 115.283, critic loss: 0.982, actor loss: 24.579\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 12.942, mean reward: 6.243, reward rate: 0.482, rewarded fraction: 0.600, relative distance: 118.955, critic loss: 1.004, actor loss: 24.652\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 12.779, mean reward: 6.630, reward rate: 0.519, rewarded fraction: 0.639, relative distance: 112.054, critic loss: 1.004, actor loss: 24.696\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.482, mean reward: 6.245, reward rate: 0.500, rewarded fraction: 0.604, relative distance: 124.095, critic loss: 0.990, actor loss: 24.718\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 13.246, mean reward: 6.563, reward rate: 0.496, rewarded fraction: 0.634, relative distance: 112.139, critic loss: 0.985, actor loss: 24.748\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.983, mean reward: 6.388, reward rate: 0.492, rewarded fraction: 0.615, relative distance: 115.628, critic loss: 0.975, actor loss: 24.807\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 13.525, mean reward: 6.687, reward rate: 0.494, rewarded fraction: 0.645, relative distance: 109.925, critic loss: 0.988, actor loss: 24.849\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 14.139, mean reward: 6.861, reward rate: 0.485, rewarded fraction: 0.661, relative distance: 100.961, critic loss: 0.962, actor loss: 24.862\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.676, mean reward: 6.364, reward rate: 0.502, rewarded fraction: 0.612, relative distance: 117.398, critic loss: 0.977, actor loss: 24.876\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 13.185, mean reward: 6.629, reward rate: 0.503, rewarded fraction: 0.639, relative distance: 112.102, critic loss: 0.971, actor loss: 24.923\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 13.877, mean reward: 6.990, reward rate: 0.504, rewarded fraction: 0.674, relative distance: 102.015, critic loss: 0.965, actor loss: 24.953\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 13.897, mean reward: 7.165, reward rate: 0.516, rewarded fraction: 0.695, relative distance: 98.552, critic loss: 0.961, actor loss: 24.960\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 13.264, mean reward: 6.522, reward rate: 0.492, rewarded fraction: 0.625, relative distance: 110.841, critic loss: 0.983, actor loss: 24.970\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 13.912, mean reward: 7.057, reward rate: 0.507, rewarded fraction: 0.684, relative distance: 99.142, critic loss: 0.966, actor loss: 24.972\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 14.009, mean reward: 7.126, reward rate: 0.509, rewarded fraction: 0.691, relative distance: 98.147, critic loss: 0.947, actor loss: 24.999\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 14.575, mean reward: 7.369, reward rate: 0.506, rewarded fraction: 0.710, relative distance: 86.428, critic loss: 0.949, actor loss: 24.994\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 14.718, mean reward: 7.223, reward rate: 0.491, rewarded fraction: 0.686, relative distance: 82.187, critic loss: 0.958, actor loss: 25.009\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 14.636, mean reward: 7.436, reward rate: 0.508, rewarded fraction: 0.716, relative distance: 87.209, critic loss: 0.940, actor loss: 25.041\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 14.903, mean reward: 7.665, reward rate: 0.514, rewarded fraction: 0.745, relative distance: 82.894, critic loss: 0.940, actor loss: 25.022\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 14.884, mean reward: 7.371, reward rate: 0.495, rewarded fraction: 0.711, relative distance: 87.838, critic loss: 0.912, actor loss: 25.022\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 14.438, mean reward: 7.280, reward rate: 0.504, rewarded fraction: 0.700, relative distance: 89.863, critic loss: 0.927, actor loss: 25.051\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 14.671, mean reward: 7.217, reward rate: 0.492, rewarded fraction: 0.690, relative distance: 92.251, critic loss: 0.954, actor loss: 25.004\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 14.531, mean reward: 7.450, reward rate: 0.513, rewarded fraction: 0.725, relative distance: 89.852, critic loss: 0.952, actor loss: 24.988\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 15.062, mean reward: 7.583, reward rate: 0.503, rewarded fraction: 0.729, relative distance: 79.806, critic loss: 0.945, actor loss: 25.004\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 13.599, mean reward: 6.921, reward rate: 0.509, rewarded fraction: 0.668, relative distance: 104.116, critic loss: 0.989, actor loss: 24.999\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 14.448, mean reward: 7.541, reward rate: 0.522, rewarded fraction: 0.731, relative distance: 86.616, critic loss: 0.966, actor loss: 24.999\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 13.904, mean reward: 7.017, reward rate: 0.505, rewarded fraction: 0.675, relative distance: 98.119, critic loss: 0.949, actor loss: 25.004\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 14.356, mean reward: 7.390, reward rate: 0.515, rewarded fraction: 0.713, relative distance: 88.910, critic loss: 0.985, actor loss: 25.025\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 14.365, mean reward: 7.588, reward rate: 0.528, rewarded fraction: 0.737, relative distance: 85.319, critic loss: 0.967, actor loss: 25.026\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 13.930, mean reward: 7.132, reward rate: 0.512, rewarded fraction: 0.689, relative distance: 95.490, critic loss: 0.979, actor loss: 24.996\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 14.451, mean reward: 7.562, reward rate: 0.523, rewarded fraction: 0.735, relative distance: 88.297, critic loss: 0.922, actor loss: 24.979\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 14.995, mean reward: 7.749, reward rate: 0.517, rewarded fraction: 0.752, relative distance: 79.542, critic loss: 0.893, actor loss: 24.988\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 15.090, mean reward: 7.621, reward rate: 0.505, rewarded fraction: 0.739, relative distance: 83.787, critic loss: 0.897, actor loss: 25.008\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 13.927, mean reward: 7.054, reward rate: 0.506, rewarded fraction: 0.680, relative distance: 99.346, critic loss: 0.962, actor loss: 24.999\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 13.798, mean reward: 7.323, reward rate: 0.531, rewarded fraction: 0.713, relative distance: 96.438, critic loss: 0.965, actor loss: 25.020\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 14.239, mean reward: 7.148, reward rate: 0.502, rewarded fraction: 0.696, relative distance: 102.144, critic loss: 0.934, actor loss: 24.960\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 14.294, mean reward: 7.140, reward rate: 0.500, rewarded fraction: 0.690, relative distance: 95.538, critic loss: 0.919, actor loss: 25.001\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 14.138, mean reward: 7.005, reward rate: 0.495, rewarded fraction: 0.673, relative distance: 97.495, critic loss: 0.941, actor loss: 24.985\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 14.231, mean reward: 7.247, reward rate: 0.509, rewarded fraction: 0.701, relative distance: 94.577, critic loss: 0.953, actor loss: 25.028\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: nan, mean reward: nan, reward rate: nan, rewarded fraction: nan, relative distance: nan, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 9.765, mean reward: 0.222, reward rate: 0.023, rewarded fraction: 0.016, relative distance: 255.262, critic loss: 0.032, actor loss: 0.068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 12.900, mean reward: 0.353, reward rate: 0.027, rewarded fraction: 0.025, relative distance: 247.205, critic loss: 0.086, actor loss: 0.595\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 16.998, mean reward: 0.339, reward rate: 0.020, rewarded fraction: 0.020, relative distance: 256.300, critic loss: 0.093, actor loss: 0.823\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 17.065, mean reward: 0.300, reward rate: 0.018, rewarded fraction: 0.020, relative distance: 264.610, critic loss: 0.091, actor loss: 0.560\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 17.658, mean reward: 0.300, reward rate: 0.017, rewarded fraction: 0.019, relative distance: 252.168, critic loss: 0.076, actor loss: 0.369\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 17.199, mean reward: 0.425, reward rate: 0.025, rewarded fraction: 0.030, relative distance: 255.590, critic loss: 0.065, actor loss: 0.252\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.403, mean reward: 0.284, reward rate: 0.017, rewarded fraction: 0.018, relative distance: 259.088, critic loss: 0.051, actor loss: 0.171\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.285, mean reward: 0.451, reward rate: 0.028, rewarded fraction: 0.033, relative distance: 257.496, critic loss: 0.048, actor loss: 0.126\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 7.156, mean reward: 0.110, reward rate: 0.015, rewarded fraction: 0.006, relative distance: 267.316, critic loss: 0.130, actor loss: 0.229\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 9.610, mean reward: 0.730, reward rate: 0.076, rewarded fraction: 0.057, relative distance: 206.981, critic loss: 0.201, actor loss: 0.481\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 8.929, mean reward: 0.851, reward rate: 0.095, rewarded fraction: 0.067, relative distance: 200.243, critic loss: 0.446, actor loss: 1.040\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 5.106, mean reward: 0.119, reward rate: 0.023, rewarded fraction: 0.004, relative distance: 248.576, critic loss: 0.374, actor loss: 1.854\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 6.944, mean reward: 0.709, reward rate: 0.102, rewarded fraction: 0.055, relative distance: 221.520, critic loss: 0.211, actor loss: 2.215\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 8.836, mean reward: 1.626, reward rate: 0.184, rewarded fraction: 0.144, relative distance: 204.268, critic loss: 0.691, actor loss: 3.031\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 7.846, mean reward: 1.744, reward rate: 0.222, rewarded fraction: 0.156, relative distance: 214.371, critic loss: 0.951, actor loss: 4.468\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 9.053, mean reward: 3.427, reward rate: 0.379, rewarded fraction: 0.323, relative distance: 183.464, critic loss: 0.786, actor loss: 6.024\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 9.188, mean reward: 3.784, reward rate: 0.412, rewarded fraction: 0.361, relative distance: 182.452, critic loss: 0.869, actor loss: 7.504\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 9.719, mean reward: 4.086, reward rate: 0.420, rewarded fraction: 0.390, relative distance: 175.953, critic loss: 0.963, actor loss: 9.018\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 10.333, mean reward: 4.612, reward rate: 0.446, rewarded fraction: 0.440, relative distance: 162.537, critic loss: 1.003, actor loss: 10.561\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 10.782, mean reward: 4.732, reward rate: 0.439, rewarded fraction: 0.448, relative distance: 156.696, critic loss: 0.984, actor loss: 11.926\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 11.125, mean reward: 5.035, reward rate: 0.453, rewarded fraction: 0.480, relative distance: 145.324, critic loss: 0.976, actor loss: 13.181\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 11.136, mean reward: 4.818, reward rate: 0.433, rewarded fraction: 0.455, relative distance: 155.127, critic loss: 1.010, actor loss: 14.270\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 11.315, mean reward: 5.241, reward rate: 0.463, rewarded fraction: 0.503, relative distance: 146.899, critic loss: 1.039, actor loss: 15.292\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 11.281, mean reward: 5.150, reward rate: 0.456, rewarded fraction: 0.489, relative distance: 149.516, critic loss: 1.011, actor loss: 16.194\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 11.188, mean reward: 5.050, reward rate: 0.451, rewarded fraction: 0.481, relative distance: 148.488, critic loss: 1.000, actor loss: 16.935\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 11.605, mean reward: 5.520, reward rate: 0.476, rewarded fraction: 0.531, relative distance: 143.132, critic loss: 1.031, actor loss: 17.603\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 11.602, mean reward: 5.534, reward rate: 0.477, rewarded fraction: 0.532, relative distance: 141.073, critic loss: 1.042, actor loss: 18.195\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 11.947, mean reward: 5.935, reward rate: 0.497, rewarded fraction: 0.577, relative distance: 134.612, critic loss: 1.043, actor loss: 18.704\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 12.096, mean reward: 6.043, reward rate: 0.500, rewarded fraction: 0.588, relative distance: 130.959, critic loss: 1.050, actor loss: 19.207\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 11.846, mean reward: 6.004, reward rate: 0.507, rewarded fraction: 0.582, relative distance: 134.374, critic loss: 1.046, actor loss: 19.673\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 12.027, mean reward: 5.818, reward rate: 0.484, rewarded fraction: 0.561, relative distance: 133.836, critic loss: 1.063, actor loss: 20.114\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 12.145, mean reward: 5.894, reward rate: 0.485, rewarded fraction: 0.569, relative distance: 133.113, critic loss: 1.091, actor loss: 20.527\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 12.328, mean reward: 5.739, reward rate: 0.466, rewarded fraction: 0.550, relative distance: 132.643, critic loss: 1.113, actor loss: 20.886\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 12.112, mean reward: 5.735, reward rate: 0.474, rewarded fraction: 0.546, relative distance: 134.096, critic loss: 1.113, actor loss: 21.210\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 12.006, mean reward: 5.998, reward rate: 0.500, rewarded fraction: 0.584, relative distance: 132.686, critic loss: 1.110, actor loss: 21.480\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 11.994, mean reward: 5.884, reward rate: 0.491, rewarded fraction: 0.570, relative distance: 133.493, critic loss: 1.132, actor loss: 21.740\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 12.349, mean reward: 6.184, reward rate: 0.501, rewarded fraction: 0.599, relative distance: 126.229, critic loss: 1.137, actor loss: 21.946\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 12.274, mean reward: 6.012, reward rate: 0.490, rewarded fraction: 0.581, relative distance: 130.464, critic loss: 1.123, actor loss: 22.160\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 12.060, mean reward: 5.818, reward rate: 0.482, rewarded fraction: 0.557, relative distance: 134.383, critic loss: 1.134, actor loss: 22.414\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 12.307, mean reward: 6.024, reward rate: 0.489, rewarded fraction: 0.578, relative distance: 126.842, critic loss: 1.130, actor loss: 22.560\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 12.774, mean reward: 6.286, reward rate: 0.492, rewarded fraction: 0.603, relative distance: 118.025, critic loss: 1.146, actor loss: 22.708\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 12.874, mean reward: 6.425, reward rate: 0.499, rewarded fraction: 0.620, relative distance: 118.571, critic loss: 1.140, actor loss: 22.835\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 12.628, mean reward: 6.273, reward rate: 0.497, rewarded fraction: 0.606, relative distance: 122.972, critic loss: 1.135, actor loss: 22.968\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 12.533, mean reward: 6.102, reward rate: 0.487, rewarded fraction: 0.587, relative distance: 124.941, critic loss: 1.151, actor loss: 23.090\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 12.395, mean reward: 5.970, reward rate: 0.482, rewarded fraction: 0.575, relative distance: 131.458, critic loss: 1.113, actor loss: 23.180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 12.757, mean reward: 6.124, reward rate: 0.480, rewarded fraction: 0.589, relative distance: 123.717, critic loss: 1.123, actor loss: 23.272\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 12.723, mean reward: 6.482, reward rate: 0.509, rewarded fraction: 0.630, relative distance: 119.648, critic loss: 1.143, actor loss: 23.348\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 12.897, mean reward: 6.609, reward rate: 0.512, rewarded fraction: 0.643, relative distance: 117.146, critic loss: 1.121, actor loss: 23.439\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 12.939, mean reward: 6.414, reward rate: 0.496, rewarded fraction: 0.615, relative distance: 115.582, critic loss: 1.130, actor loss: 23.491\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 13.106, mean reward: 6.462, reward rate: 0.493, rewarded fraction: 0.624, relative distance: 116.846, critic loss: 1.138, actor loss: 23.589\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 13.283, mean reward: 6.754, reward rate: 0.508, rewarded fraction: 0.654, relative distance: 108.247, critic loss: 1.116, actor loss: 23.671\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 12.580, mean reward: 6.221, reward rate: 0.494, rewarded fraction: 0.601, relative distance: 127.164, critic loss: 1.106, actor loss: 23.710\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 12.733, mean reward: 6.441, reward rate: 0.506, rewarded fraction: 0.625, relative distance: 119.710, critic loss: 1.108, actor loss: 23.786\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 12.141, mean reward: 5.894, reward rate: 0.485, rewarded fraction: 0.566, relative distance: 131.334, critic loss: 1.128, actor loss: 23.869\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 12.638, mean reward: 6.369, reward rate: 0.504, rewarded fraction: 0.619, relative distance: 122.219, critic loss: 1.141, actor loss: 23.920\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 12.584, mean reward: 6.092, reward rate: 0.484, rewarded fraction: 0.586, relative distance: 124.916, critic loss: 1.109, actor loss: 23.948\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 12.816, mean reward: 6.353, reward rate: 0.496, rewarded fraction: 0.617, relative distance: 121.721, critic loss: 1.122, actor loss: 24.015\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 12.789, mean reward: 6.337, reward rate: 0.496, rewarded fraction: 0.616, relative distance: 125.270, critic loss: 1.104, actor loss: 24.051\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 13.024, mean reward: 6.782, reward rate: 0.521, rewarded fraction: 0.660, relative distance: 112.034, critic loss: 1.088, actor loss: 24.130\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 13.013, mean reward: 6.537, reward rate: 0.502, rewarded fraction: 0.632, relative distance: 114.614, critic loss: 1.109, actor loss: 24.195\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 12.898, mean reward: 6.649, reward rate: 0.516, rewarded fraction: 0.647, relative distance: 115.405, critic loss: 1.081, actor loss: 24.203\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 12.720, mean reward: 6.337, reward rate: 0.498, rewarded fraction: 0.614, relative distance: 121.732, critic loss: 1.093, actor loss: 24.245\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 12.668, mean reward: 6.699, reward rate: 0.529, rewarded fraction: 0.655, relative distance: 114.432, critic loss: 1.107, actor loss: 24.262\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 12.474, mean reward: 6.368, reward rate: 0.511, rewarded fraction: 0.621, relative distance: 125.279, critic loss: 1.074, actor loss: 24.281\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 12.759, mean reward: 6.508, reward rate: 0.510, rewarded fraction: 0.633, relative distance: 118.882, critic loss: 1.099, actor loss: 24.355\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 12.855, mean reward: 6.471, reward rate: 0.503, rewarded fraction: 0.626, relative distance: 118.171, critic loss: 1.119, actor loss: 24.394\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 12.667, mean reward: 6.413, reward rate: 0.506, rewarded fraction: 0.624, relative distance: 122.562, critic loss: 1.099, actor loss: 24.453\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 12.690, mean reward: 6.539, reward rate: 0.515, rewarded fraction: 0.639, relative distance: 118.356, critic loss: 1.077, actor loss: 24.477\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 12.842, mean reward: 6.159, reward rate: 0.480, rewarded fraction: 0.596, relative distance: 124.006, critic loss: 1.082, actor loss: 24.519\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 12.634, mean reward: 6.498, reward rate: 0.514, rewarded fraction: 0.631, relative distance: 121.635, critic loss: 1.105, actor loss: 24.560\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 12.847, mean reward: 6.582, reward rate: 0.512, rewarded fraction: 0.641, relative distance: 120.138, critic loss: 1.069, actor loss: 24.546\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 12.391, mean reward: 6.349, reward rate: 0.512, rewarded fraction: 0.613, relative distance: 120.876, critic loss: 1.097, actor loss: 24.593\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 12.644, mean reward: 5.969, reward rate: 0.472, rewarded fraction: 0.577, relative distance: 131.800, critic loss: 1.071, actor loss: 24.578\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 12.891, mean reward: 6.267, reward rate: 0.486, rewarded fraction: 0.599, relative distance: 117.485, critic loss: 1.130, actor loss: 24.528\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 12.601, mean reward: 6.270, reward rate: 0.498, rewarded fraction: 0.605, relative distance: 123.573, critic loss: 1.129, actor loss: 24.529\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 12.696, mean reward: 6.452, reward rate: 0.508, rewarded fraction: 0.623, relative distance: 118.423, critic loss: 1.107, actor loss: 24.533\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 12.489, mean reward: 6.257, reward rate: 0.501, rewarded fraction: 0.605, relative distance: 124.067, critic loss: 1.107, actor loss: 24.554\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 12.530, mean reward: 6.416, reward rate: 0.512, rewarded fraction: 0.619, relative distance: 120.018, critic loss: 1.119, actor loss: 24.506\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 12.707, mean reward: 6.433, reward rate: 0.506, rewarded fraction: 0.623, relative distance: 116.339, critic loss: 1.122, actor loss: 24.474\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 12.636, mean reward: 6.332, reward rate: 0.501, rewarded fraction: 0.610, relative distance: 122.080, critic loss: 1.112, actor loss: 24.484\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 12.919, mean reward: 6.687, reward rate: 0.518, rewarded fraction: 0.648, relative distance: 110.680, critic loss: 1.135, actor loss: 24.510\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 12.408, mean reward: 6.250, reward rate: 0.504, rewarded fraction: 0.606, relative distance: 126.039, critic loss: 1.126, actor loss: 24.563\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 12.675, mean reward: 6.328, reward rate: 0.499, rewarded fraction: 0.609, relative distance: 117.490, critic loss: 1.103, actor loss: 24.586\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 12.512, mean reward: 6.356, reward rate: 0.508, rewarded fraction: 0.612, relative distance: 120.142, critic loss: 1.091, actor loss: 24.610\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 12.364, mean reward: 6.378, reward rate: 0.516, rewarded fraction: 0.623, relative distance: 124.153, critic loss: 1.065, actor loss: 24.638\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 12.251, mean reward: 6.171, reward rate: 0.504, rewarded fraction: 0.593, relative distance: 123.590, critic loss: 1.109, actor loss: 24.647\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 12.387, mean reward: 6.540, reward rate: 0.528, rewarded fraction: 0.636, relative distance: 120.476, critic loss: 1.093, actor loss: 24.622\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 12.825, mean reward: 6.796, reward rate: 0.530, rewarded fraction: 0.659, relative distance: 109.824, critic loss: 1.077, actor loss: 24.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 12.658, mean reward: 6.291, reward rate: 0.497, rewarded fraction: 0.611, relative distance: 124.347, critic loss: 1.082, actor loss: 24.668\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 12.540, mean reward: 6.333, reward rate: 0.505, rewarded fraction: 0.616, relative distance: 122.447, critic loss: 1.094, actor loss: 24.688\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 12.364, mean reward: 6.257, reward rate: 0.506, rewarded fraction: 0.609, relative distance: 125.097, critic loss: 1.116, actor loss: 24.666\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 12.470, mean reward: 6.326, reward rate: 0.507, rewarded fraction: 0.612, relative distance: 121.248, critic loss: 1.082, actor loss: 24.675\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 12.605, mean reward: 6.539, reward rate: 0.519, rewarded fraction: 0.636, relative distance: 118.994, critic loss: 1.068, actor loss: 24.703\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 12.624, mean reward: 6.563, reward rate: 0.520, rewarded fraction: 0.640, relative distance: 118.641, critic loss: 1.082, actor loss: 24.724\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 12.586, mean reward: 6.616, reward rate: 0.526, rewarded fraction: 0.642, relative distance: 116.334, critic loss: 1.092, actor loss: 24.734\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 12.321, mean reward: 6.185, reward rate: 0.502, rewarded fraction: 0.602, relative distance: 129.713, critic loss: 1.100, actor loss: 24.763\n"
     ]
    }
   ],
   "source": [
    "for actor, critic, seed_ in zip(actors, critics, seeds):\n",
    "    for seed in seed_:\n",
    "        datapath = folder_path / f'{actor}{critic}' / f'seed{seed}' \n",
    "        exec(f'from {actor} import *'); exec(f'from {critic} import *')\n",
    "        training(datapath, seed, Actor, Critic, expnoise_std, TOTAL_EPISODE, \n",
    "                 value_noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
