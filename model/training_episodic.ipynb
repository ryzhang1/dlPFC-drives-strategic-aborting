{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from Environment import Env\n",
    "from Agent_LSTM import *\n",
    "from pathlib import Path\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(datapath, seed_number, Actor, Critic, \n",
    "             expnoise_std=0.8, TOTAL_EPISODE=5e4, value_noise_std=0):\n",
    "    # get configures\n",
    "    arg = config.ConfigGain(datapath)\n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.save()\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "\n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg, Actor, Critic)\n",
    "    agent.actor.value_noise_std = value_noise_std\n",
    "    \n",
    "    # define exploration noise\n",
    "    noise = ActionNoise(arg.ACTION_DIM, mean=0, std=expnoise_std)\n",
    "    \n",
    "    # Loop now\n",
    "    tot_t = 0\n",
    "    episode = agent.initial_episode\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    actor_loss_log = 0\n",
    "    critic_loss_log = 0\n",
    "    num_update = 1e-5\n",
    "    dist_log = []\n",
    "\n",
    "    LOG_FREQ = 100\n",
    "    REPLAY_PERIOD = 4\n",
    "    PRE_LEARN_PERIOD = arg.BATCH_SIZE * 50\n",
    "    random_stop = True\n",
    "    pre_phase = True\n",
    "\n",
    "    # Start loop\n",
    "    while episode < TOTAL_EPISODE:\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        x = env.reset()\n",
    "        agent.bstep.reset(env.pro_gains)\n",
    "        \n",
    "        last_action = torch.zeros(1, 1, arg.ACTION_DIM)\n",
    "        last_action_raw = last_action.clone()\n",
    "\n",
    "        state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                           env.target_position_obs.view(1, 1, -1), \n",
    "                           torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "\n",
    "        hiddenin = None\n",
    "        tend = 0\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(arg.EPISODE_LEN):\n",
    "            # 1. Check start threshold.\n",
    "            if not cross_start_threshold and (last_action.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state \n",
    "            # and previous hidden & cell states of LSTM units.\n",
    "            action, action_raw, hiddenout = agent.select_action(state, hiddenin, action_noise=noise)\n",
    "            if random_stop and np.random.rand() > 0.95:\n",
    "                action = torch.zeros_like(action)\n",
    "\n",
    "            # 3. Track next x in the environment.\n",
    "            next_x, reached_target, relative_dist = env(x, action, t - tend)\n",
    "\n",
    "            # 4. Next observation given next x.\n",
    "            next_ox = agent.bstep(next_x)\n",
    "            next_state = torch.cat([next_ox.view(1, 1, -1), action, env.target_position_obs.view(1, 1, -1),\n",
    "                                    torch.ones(1, 1, 1) * t - tend + 1], dim=2).to(arg.device)\n",
    "\n",
    "            # 5. Check whether stop.\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if stopped.          \n",
    "            if is_stop and cross_start_threshold:\n",
    "                reward = env.return_reward(x, reward_mode='mixed')\n",
    "                done = torch.ones(1, 1, 1)\n",
    "            else:\n",
    "                reward = torch.zeros(1, 1, 1)\n",
    "                done = torch.zeros(1, 1, 1)\n",
    "\n",
    "            # 7. Append data.\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            # 8. Update timestep.\n",
    "            last_action_raw = action_raw\n",
    "            last_action = action\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            hiddenin = hiddenout\n",
    "            tot_t += 1\n",
    "\n",
    "            # 9. Update model.\n",
    "            if len(agent.memory.memory) > PRE_LEARN_PERIOD and tot_t % REPLAY_PERIOD == 0:\n",
    "                actor_loss, critic_loss = agent.learn()\n",
    "                actor_loss_log += actor_loss\n",
    "                critic_loss_log += critic_loss\n",
    "                num_update += 1\n",
    "\n",
    "            # 10. whether break.\n",
    "            if is_stop and cross_start_threshold:\n",
    "                step_log.append(t + 1 - tend)\n",
    "                reward_log.append(reward.item())\n",
    "                rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "                dist_log.append(relative_dist.item())\n",
    "                # initialize a trial\n",
    "                cross_start_threshold = False\n",
    "                x = env.reset()\n",
    "                agent.bstep.reset(env.pro_gains)\n",
    "                state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                                   env.target_position_obs.view(1, 1, -1),\n",
    "                                   torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "                tend = t + 1\n",
    "\n",
    "\n",
    "        # store the last state\n",
    "        states.append(state)\n",
    "        # End of one trial, store trajectory into buffer.\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions).to(arg.device)\n",
    "        rewards = torch.cat(rewards).to(arg.device)\n",
    "        dones = torch.cat(dones).to(arg.device)\n",
    "        agent.memory.push(states, actions, rewards, dones) \n",
    "\n",
    "        # store mirrored trajectories reflected along y-axis\n",
    "        agent.memory.push(*agent.mirror_traj(states, actions), rewards, dones) \n",
    "\n",
    "        if episode % LOG_FREQ == LOG_FREQ - 1:\n",
    "            # save\n",
    "            agent.save(save_memory=False, episode=episode, pre_phase=pre_phase, full_param=True)\n",
    "            \n",
    "            print(f\"t: {tot_t}, Ep: {episode}, action std: {noise.std:0.2f}\")\n",
    "            print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "                  f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "                  f\"reward rate: {np.sum(reward_log) / np.sum(step_log):0.3f}, \"\n",
    "                  f\"rewarded fraction: {np.mean(rewarded_trial_log):0.3f}, \"\n",
    "                  f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "                  f\"critic loss: {critic_loss_log / num_update:0.3f}, \"\n",
    "                  f\"actor loss: {-actor_loss_log / (num_update/2):0.3f}\")\n",
    "            \n",
    "            reward_rate = np.sum(reward_log) / np.sum(step_log)\n",
    "            if pre_phase and reward_rate > 0.2:\n",
    "                random_stop = False\n",
    "                pre_phase = False\n",
    "                episode = 0\n",
    "                \n",
    "            if not pre_phase:\n",
    "                noise.reset(mean=0, std=0.5)  \n",
    "                \n",
    "            if np.mean(rewarded_trial_log) > 0.85:\n",
    "                break\n",
    "\n",
    "            reward_log = []\n",
    "            rewarded_trial_log = []\n",
    "            step_log = []\n",
    "            actor_loss_log = 0\n",
    "            critic_loss_log = 0\n",
    "            num_update = 1e-5\n",
    "            dist_log = []\n",
    "            \n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ['Actor_novalue']\n",
    "critics = ['Critic']\n",
    "seeds = [[21,22]]\n",
    "expnoise_std = 0.8\n",
    "TOTAL_EPISODE = 1e4\n",
    "folder_path = Path('D:/quitting_data/agents')\n",
    "\n",
    "value_noise_std = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 352.954, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 4.920, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 353.863, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 342.342, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 4.545, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 346.930, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 10.593, mean reward: 0.077, reward rate: 0.007, rewarded fraction: 0.004, relative distance: 287.132, critic loss: 0.008, actor loss: 0.016\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.602, mean reward: 0.057, reward rate: 0.003, rewarded fraction: 0.002, relative distance: 280.341, critic loss: 0.013, actor loss: 0.036\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 7.557, mean reward: 0.050, reward rate: 0.007, rewarded fraction: 0.002, relative distance: 273.896, critic loss: 0.016, actor loss: 0.048\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 15.283, mean reward: 0.306, reward rate: 0.020, rewarded fraction: 0.020, relative distance: 250.310, critic loss: 0.030, actor loss: 0.155\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 17.096, mean reward: 0.330, reward rate: 0.019, rewarded fraction: 0.022, relative distance: 263.538, critic loss: 0.057, actor loss: 0.258\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 16.956, mean reward: 0.612, reward rate: 0.036, rewarded fraction: 0.044, relative distance: 220.976, critic loss: 0.077, actor loss: 0.232\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 17.105, mean reward: 0.795, reward rate: 0.046, rewarded fraction: 0.064, relative distance: 221.523, critic loss: 0.119, actor loss: 0.181\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 15.423, mean reward: 0.689, reward rate: 0.045, rewarded fraction: 0.052, relative distance: 220.769, critic loss: 0.120, actor loss: 0.122\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 11.002, mean reward: 0.463, reward rate: 0.042, rewarded fraction: 0.036, relative distance: 237.575, critic loss: 0.149, actor loss: 0.364\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 14.850, mean reward: 1.939, reward rate: 0.131, rewarded fraction: 0.170, relative distance: 179.642, critic loss: 0.264, actor loss: 1.476\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 14.413, mean reward: 1.727, reward rate: 0.120, rewarded fraction: 0.150, relative distance: 180.165, critic loss: 0.469, actor loss: 2.541\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 14.606, mean reward: 2.094, reward rate: 0.143, rewarded fraction: 0.188, relative distance: 174.864, critic loss: 0.394, actor loss: 3.259\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 13.928, mean reward: 2.713, reward rate: 0.195, rewarded fraction: 0.252, relative distance: 168.721, critic loss: 0.379, actor loss: 3.787\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 14.175, mean reward: 3.456, reward rate: 0.244, rewarded fraction: 0.320, relative distance: 151.491, critic loss: 0.407, actor loss: 4.451\n",
      "t: 189900, Ep: 99, action std: 0.50\n",
      "mean steps: 20.242, mean reward: 8.602, reward rate: 0.425, rewarded fraction: 0.830, relative distance: 46.496, critic loss: 0.417, actor loss: 4.891\n",
      "t: 199900, Ep: 199, action std: 0.50\n",
      "mean steps: 17.899, mean reward: 8.778, reward rate: 0.490, rewarded fraction: 0.850, relative distance: 43.146, critic loss: 0.391, actor loss: 5.271\n",
      "t: 10000, Ep: 99, action std: 0.80\n",
      "mean steps: 5.167, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 355.917, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.80\n",
      "mean steps: 4.958, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 359.109, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.80\n",
      "mean steps: 5.000, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 372.743, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.80\n",
      "mean steps: 4.565, mean reward: 0.010, reward rate: 0.002, rewarded fraction: 0.000, relative distance: 358.389, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.80\n",
      "mean steps: 16.415, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 429.699, critic loss: 0.000, actor loss: 0.005\n",
      "t: 60000, Ep: 599, action std: 0.80\n",
      "mean steps: 16.156, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 433.775, critic loss: 0.000, actor loss: -0.014\n",
      "t: 70000, Ep: 699, action std: 0.80\n",
      "mean steps: 17.591, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 440.339, critic loss: 0.000, actor loss: -0.017\n",
      "t: 80000, Ep: 799, action std: 0.80\n",
      "mean steps: 16.114, mean reward: 0.032, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 417.790, critic loss: 0.000, actor loss: -0.018\n",
      "t: 90000, Ep: 899, action std: 0.80\n",
      "mean steps: 15.648, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 434.166, critic loss: 0.002, actor loss: -0.018\n",
      "t: 100000, Ep: 999, action std: 0.80\n",
      "mean steps: 15.430, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 419.522, critic loss: 0.002, actor loss: -0.019\n",
      "t: 110000, Ep: 1099, action std: 0.80\n",
      "mean steps: 16.150, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 424.616, critic loss: 0.002, actor loss: -0.021\n",
      "t: 120000, Ep: 1199, action std: 0.80\n",
      "mean steps: 16.235, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.941, critic loss: 0.002, actor loss: -0.023\n",
      "t: 130000, Ep: 1299, action std: 0.80\n",
      "mean steps: 15.827, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 428.221, critic loss: 0.002, actor loss: -0.025\n",
      "t: 140000, Ep: 1399, action std: 0.80\n",
      "mean steps: 16.831, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 426.307, critic loss: 0.001, actor loss: -0.027\n",
      "t: 150000, Ep: 1499, action std: 0.80\n",
      "mean steps: 15.796, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 431.369, critic loss: 0.001, actor loss: -0.028\n",
      "t: 160000, Ep: 1599, action std: 0.80\n",
      "mean steps: 16.847, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 436.812, critic loss: 0.001, actor loss: -0.029\n",
      "t: 170000, Ep: 1699, action std: 0.80\n",
      "mean steps: 17.063, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 412.403, critic loss: 0.001, actor loss: -0.029\n",
      "t: 180000, Ep: 1799, action std: 0.80\n",
      "mean steps: 15.746, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 411.245, critic loss: 0.001, actor loss: -0.027\n",
      "t: 190000, Ep: 1899, action std: 0.80\n",
      "mean steps: 15.215, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 401.927, critic loss: 0.001, actor loss: -0.025\n",
      "t: 200000, Ep: 1999, action std: 0.80\n",
      "mean steps: 16.055, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 418.526, critic loss: 0.001, actor loss: -0.025\n",
      "t: 210000, Ep: 2099, action std: 0.80\n",
      "mean steps: 16.263, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 404.278, critic loss: 0.001, actor loss: -0.024\n",
      "t: 220000, Ep: 2199, action std: 0.80\n",
      "mean steps: 16.914, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 408.609, critic loss: 0.001, actor loss: -0.022\n",
      "t: 230000, Ep: 2299, action std: 0.80\n",
      "mean steps: 16.161, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 403.677, critic loss: 0.001, actor loss: -0.022\n",
      "t: 240000, Ep: 2399, action std: 0.80\n",
      "mean steps: 17.297, mean reward: 0.033, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 412.035, critic loss: 0.001, actor loss: -0.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 250000, Ep: 2499, action std: 0.80\n",
      "mean steps: 15.875, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 420.202, critic loss: 0.002, actor loss: -0.020\n",
      "t: 260000, Ep: 2599, action std: 0.80\n",
      "mean steps: 16.448, mean reward: 0.052, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 404.761, critic loss: 0.002, actor loss: -0.019\n",
      "t: 270000, Ep: 2699, action std: 0.80\n",
      "mean steps: 16.655, mean reward: 0.017, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 409.400, critic loss: 0.003, actor loss: -0.019\n",
      "t: 280000, Ep: 2799, action std: 0.80\n",
      "mean steps: 16.876, mean reward: 0.015, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 402.380, critic loss: 0.003, actor loss: -0.019\n",
      "t: 290000, Ep: 2899, action std: 0.80\n",
      "mean steps: 16.933, mean reward: 0.042, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 413.986, critic loss: 0.004, actor loss: -0.018\n",
      "t: 300000, Ep: 2999, action std: 0.80\n",
      "mean steps: 17.172, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 398.084, critic loss: 0.004, actor loss: -0.019\n",
      "t: 310000, Ep: 3099, action std: 0.80\n",
      "mean steps: 16.221, mean reward: 0.037, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 404.136, critic loss: 0.004, actor loss: -0.019\n",
      "t: 320000, Ep: 3199, action std: 0.80\n",
      "mean steps: 16.139, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 405.294, critic loss: 0.004, actor loss: -0.020\n",
      "t: 330000, Ep: 3299, action std: 0.80\n",
      "mean steps: 15.591, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 393.906, critic loss: 0.005, actor loss: -0.021\n",
      "t: 340000, Ep: 3399, action std: 0.80\n",
      "mean steps: 17.158, mean reward: 0.020, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 396.987, critic loss: 0.004, actor loss: -0.022\n",
      "t: 350000, Ep: 3499, action std: 0.80\n",
      "mean steps: 15.859, mean reward: 0.038, reward rate: 0.002, rewarded fraction: 0.002, relative distance: 390.192, critic loss: 0.004, actor loss: -0.023\n",
      "t: 360000, Ep: 3599, action std: 0.80\n",
      "mean steps: 17.110, mean reward: 0.014, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 392.397, critic loss: 0.004, actor loss: -0.022\n",
      "t: 370000, Ep: 3699, action std: 0.80\n",
      "mean steps: 17.076, mean reward: 0.056, reward rate: 0.003, rewarded fraction: 0.004, relative distance: 383.010, critic loss: 0.005, actor loss: -0.021\n",
      "t: 380000, Ep: 3799, action std: 0.80\n",
      "mean steps: 15.725, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 377.834, critic loss: 0.005, actor loss: -0.021\n",
      "t: 390000, Ep: 3899, action std: 0.80\n",
      "mean steps: 16.299, mean reward: 0.016, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 382.016, critic loss: 0.006, actor loss: -0.021\n",
      "t: 400000, Ep: 3999, action std: 0.80\n",
      "mean steps: 16.085, mean reward: 0.013, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 381.777, critic loss: 0.004, actor loss: -0.021\n",
      "t: 410000, Ep: 4099, action std: 0.80\n",
      "mean steps: 16.148, mean reward: 0.019, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 372.142, critic loss: 0.005, actor loss: -0.020\n",
      "t: 420000, Ep: 4199, action std: 0.80\n",
      "mean steps: 17.229, mean reward: 0.011, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 389.478, critic loss: 0.005, actor loss: -0.020\n",
      "t: 430000, Ep: 4299, action std: 0.80\n",
      "mean steps: 16.601, mean reward: 0.012, reward rate: 0.001, rewarded fraction: 0.000, relative distance: 389.317, critic loss: 0.005, actor loss: -0.019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m datapath \u001b[38;5;241m=\u001b[39m folder_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcritic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisodic\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m exec(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m import *\u001b[39m\u001b[38;5;124m'\u001b[39m); exec(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m import *\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mActor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpnoise_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_EPISODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mvalue_noise_std\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(datapath, seed_number, Actor, Critic, expnoise_std, TOTAL_EPISODE, value_noise_std)\u001b[0m\n\u001b[0;32m     61\u001b[0m     cross_start_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 2. Take an action based on current state \u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# and previous hidden & cell states of LSTM units.\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m action, action_raw, hiddenout \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddenin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_stop \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.95\u001b[39m:\n\u001b[0;32m     67\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(action)\n",
      "File \u001b[1;32m~\\Desktop\\code\\quitting\\model\\Agent_LSTM.py:119\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[1;34m(self, state, hidden_in, action_noise)\u001b[0m\n\u001b[0;32m    117\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    118\u001b[0m action_raw \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (action_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43maction_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTERMINAL_ACTION\u001b[49m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    120\u001b[0m     action \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action_noise\u001b[38;5;241m.\u001b[39mnoise()\u001b[38;5;241m.\u001b[39mview_as(action)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), action_raw, hidden_out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for actor, critic, seed_ in zip(actors, critics, seeds):\n",
    "    for seed in seed_:\n",
    "        datapath = folder_path / f'{actor}{critic}' / f'seed{seed}' / 'episodic'\n",
    "        exec(f'from {actor} import *'); exec(f'from {critic} import *')\n",
    "        training(datapath, seed, Actor, Critic, expnoise_std, TOTAL_EPISODE, \n",
    "                 value_noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
