{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from Environment import Env\n",
    "from Agent_LSTM import *\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(datapath, seed_number, Actor, Critic, agent_name, logfile, epi, value_noise_std, total_epi=300):     \n",
    "    # get configures\n",
    "    arg = config.ConfigGain(datapath)\n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.device = 'cpu'\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "\n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg, Actor, Critic)\n",
    "    agent.actor.value_noise_std = value_noise_std\n",
    "    agent.load(agent_name, load_memory=False, load_optimzer=False)\n",
    "    \n",
    "    # Loop now\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    dist_log = []\n",
    "    skipped_log = []\n",
    "    tot_t = 0\n",
    "\n",
    "    # Start loop\n",
    "    for _ in range(total_epi):\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        x = env.reset()\n",
    "        agent.bstep.reset(env.pro_gains)\n",
    "        \n",
    "        last_action = torch.zeros(1, 1, arg.ACTION_DIM)\n",
    "        last_action_raw = last_action.clone()\n",
    "\n",
    "        state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                           env.target_position_obs.view(1, 1, -1), \n",
    "                           torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "\n",
    "        hiddenin = None\n",
    "        tend = 0\n",
    "\n",
    "        for t in range(30):\n",
    "            # 1. Check start threshold.\n",
    "            if not cross_start_threshold and (last_action_raw.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state \n",
    "            # and previous hidden & cell states of LSTM units.\n",
    "            action, action_raw, hiddenout = agent.select_action(state, hiddenin, action_noise=None)\n",
    "\n",
    "            # 3. Track next x in the environment.\n",
    "            next_x, reached_target, relative_dist = env(x, action, t - tend)\n",
    "\n",
    "            # 4. Next observation given next x.\n",
    "            next_ox = agent.bstep(next_x)\n",
    "            next_state = torch.cat([next_ox.view(1, 1, -1), action,\n",
    "                                    env.target_position_obs.view(1, 1, -1),\n",
    "                                    torch.ones(1, 1, 1) * (t - tend + 1)], dim=2).to(arg.device)\n",
    "\n",
    "            # 5. Check whether stop.\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if stopped.          \n",
    "            if is_stop and cross_start_threshold:\n",
    "                reward = env.return_reward(x, reward_mode='mixed')\n",
    "            else:\n",
    "                reward = torch.zeros(1, 1, 1)\n",
    "\n",
    "            # 8. Update timestep.\n",
    "            last_action_raw = action_raw\n",
    "            last_action = action\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            hiddenin = hiddenout\n",
    "\n",
    "            # 10. whether break.\n",
    "            if is_stop and cross_start_threshold:\n",
    "                break\n",
    "\n",
    "        step_log.append(t + 1 - tend)\n",
    "        reward_log.append(reward.item())\n",
    "        rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "        dist_log.append(relative_dist.item())\n",
    "        skipped_log.append(np.linalg.norm(x[:2]) < np.linalg.norm(env.target_position) * 0.3)\n",
    "        tot_t += t\n",
    "\n",
    "\n",
    "    print(f\"episode: {epi}, \"\n",
    "          f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "          f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "          f\"reward rate: {np.sum(rewarded_trial_log) / (tot_t * arg.DT):0.3f}, \"\n",
    "          f\"rewarded fraction: {np.sum(rewarded_trial_log) / total_epi:0.3f}, \"\n",
    "          f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "          f\"skipped fraction: {np.sum(skipped_log) / total_epi:0.3f}\")\n",
    "    \n",
    "    return logfile.append(\n",
    "                        pd.DataFrame({'episode': [epi],\n",
    "                                      'reward_fraction': [np.sum(rewarded_trial_log) / total_epi], \n",
    "                                      'error_distance': [np.mean(dist_log) * arg.LINEAR_SCALE],\n",
    "                                      'reward_rate': [np.sum(rewarded_trial_log) / (tot_t * arg.DT)],\n",
    "                                      'skipped fraction': [np.sum(skipped_log) / total_epi]}), \n",
    "                                      ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ['Actor_novalue']\n",
    "critics = ['Critic']\n",
    "seeds = [[19,20]]\n",
    "TOTAL_EPISODE = int(1e4)\n",
    "folder_path = Path('D:/quitting_data/agents')\n",
    "\n",
    "value_noise_std = 0\n",
    "agent_type = 'no_freeze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99, mean steps: 15.283, mean reward: 9.558, reward rate: 0.660, rewarded fraction: 0.943, relative distance: 35.794, skipped fraction: 0.000\n",
      "episode: 199, mean steps: 15.283, mean reward: 9.558, reward rate: 0.660, rewarded fraction: 0.943, relative distance: 35.794, skipped fraction: 0.000\n",
      "episode: 299, mean steps: 15.283, mean reward: 9.558, reward rate: 0.660, rewarded fraction: 0.943, relative distance: 35.794, skipped fraction: 0.000\n",
      "episode: 399, mean steps: 15.283, mean reward: 9.558, reward rate: 0.660, rewarded fraction: 0.943, relative distance: 35.794, skipped fraction: 0.000\n",
      "episode: 499, mean steps: 14.457, mean reward: 9.203, reward rate: 0.666, rewarded fraction: 0.897, relative distance: 40.932, skipped fraction: 0.000\n",
      "episode: 599, mean steps: 14.067, mean reward: 8.643, reward rate: 0.635, rewarded fraction: 0.830, relative distance: 46.629, skipped fraction: 0.000\n",
      "episode: 699, mean steps: 17.647, mean reward: 8.284, reward rate: 0.481, rewarded fraction: 0.800, relative distance: 39.648, skipped fraction: 0.000\n",
      "episode: 799, mean steps: 14.903, mean reward: 8.951, reward rate: 0.621, rewarded fraction: 0.863, relative distance: 44.169, skipped fraction: 0.000\n",
      "episode: 899, mean steps: 14.793, mean reward: 8.475, reward rate: 0.585, rewarded fraction: 0.807, relative distance: 47.249, skipped fraction: 0.000\n",
      "episode: 999, mean steps: 14.280, mean reward: 8.440, reward rate: 0.600, rewarded fraction: 0.797, relative distance: 47.315, skipped fraction: 0.000\n",
      "episode: 1099, mean steps: 14.443, mean reward: 8.721, reward rate: 0.622, rewarded fraction: 0.837, relative distance: 44.484, skipped fraction: 0.000\n",
      "episode: 1199, mean steps: 14.740, mean reward: 8.783, reward rate: 0.614, rewarded fraction: 0.843, relative distance: 42.518, skipped fraction: 0.000\n",
      "episode: 1299, mean steps: 14.697, mean reward: 8.764, reward rate: 0.613, rewarded fraction: 0.840, relative distance: 43.654, skipped fraction: 0.000\n",
      "episode: 1399, mean steps: 14.737, mean reward: 8.809, reward rate: 0.616, rewarded fraction: 0.847, relative distance: 42.044, skipped fraction: 0.000\n",
      "episode: 1499, mean steps: 14.673, mean reward: 8.692, reward rate: 0.609, rewarded fraction: 0.833, relative distance: 45.811, skipped fraction: 0.000\n",
      "episode: 1599, mean steps: 14.787, mean reward: 9.081, reward rate: 0.638, rewarded fraction: 0.880, relative distance: 40.582, skipped fraction: 0.000\n",
      "episode: 1699, mean steps: 14.787, mean reward: 9.131, reward rate: 0.643, rewarded fraction: 0.887, relative distance: 42.557, skipped fraction: 0.000\n",
      "episode: 1799, mean steps: 15.193, mean reward: 9.140, reward rate: 0.625, rewarded fraction: 0.887, relative distance: 41.597, skipped fraction: 0.000\n",
      "episode: 1899, mean steps: 14.737, mean reward: 8.947, reward rate: 0.628, rewarded fraction: 0.863, relative distance: 41.950, skipped fraction: 0.000\n",
      "episode: 1999, mean steps: 14.893, mean reward: 8.969, reward rate: 0.626, rewarded fraction: 0.870, relative distance: 42.999, skipped fraction: 0.000\n",
      "episode: 2099, mean steps: 14.943, mean reward: 9.233, reward rate: 0.645, rewarded fraction: 0.900, relative distance: 39.554, skipped fraction: 0.000\n",
      "episode: 2199, mean steps: 14.663, mean reward: 8.965, reward rate: 0.634, rewarded fraction: 0.867, relative distance: 40.661, skipped fraction: 0.000\n",
      "episode: 2299, mean steps: 14.670, mean reward: 8.733, reward rate: 0.612, rewarded fraction: 0.837, relative distance: 43.912, skipped fraction: 0.000\n",
      "episode: 2399, mean steps: 15.417, mean reward: 9.289, reward rate: 0.629, rewarded fraction: 0.907, relative distance: 39.910, skipped fraction: 0.000\n",
      "episode: 2499, mean steps: 15.057, mean reward: 9.418, reward rate: 0.657, rewarded fraction: 0.923, relative distance: 38.330, skipped fraction: 0.000\n",
      "episode: 2599, mean steps: 15.110, mean reward: 9.206, reward rate: 0.635, rewarded fraction: 0.897, relative distance: 41.799, skipped fraction: 0.000\n",
      "episode: 2699, mean steps: 14.787, mean reward: 9.421, reward rate: 0.672, rewarded fraction: 0.927, relative distance: 38.021, skipped fraction: 0.000\n",
      "episode: 2799, mean steps: 15.083, mean reward: 8.862, reward rate: 0.606, rewarded fraction: 0.853, relative distance: 40.691, skipped fraction: 0.000\n",
      "episode: 2899, mean steps: 14.797, mean reward: 8.995, reward rate: 0.631, rewarded fraction: 0.870, relative distance: 41.895, skipped fraction: 0.000\n",
      "episode: 2999, mean steps: 14.873, mean reward: 9.130, reward rate: 0.639, rewarded fraction: 0.887, relative distance: 40.318, skipped fraction: 0.000\n",
      "episode: 3099, mean steps: 15.280, mean reward: 9.312, reward rate: 0.640, rewarded fraction: 0.913, relative distance: 40.104, skipped fraction: 0.000\n",
      "episode: 3199, mean steps: 14.800, mean reward: 9.324, reward rate: 0.662, rewarded fraction: 0.913, relative distance: 39.621, skipped fraction: 0.000\n",
      "episode: 3299, mean steps: 15.073, mean reward: 9.242, reward rate: 0.640, rewarded fraction: 0.900, relative distance: 39.265, skipped fraction: 0.000\n",
      "episode: 3399, mean steps: 14.740, mean reward: 9.075, reward rate: 0.640, rewarded fraction: 0.880, relative distance: 40.910, skipped fraction: 0.000\n",
      "episode: 3499, mean steps: 14.550, mean reward: 8.805, reward rate: 0.625, rewarded fraction: 0.847, relative distance: 43.591, skipped fraction: 0.000\n",
      "episode: 3599, mean steps: 15.043, mean reward: 9.289, reward rate: 0.646, rewarded fraction: 0.907, relative distance: 38.319, skipped fraction: 0.000\n",
      "episode: 3699, mean steps: 14.533, mean reward: 8.954, reward rate: 0.638, rewarded fraction: 0.863, relative distance: 42.442, skipped fraction: 0.000\n",
      "episode: 3799, mean steps: 14.727, mean reward: 9.333, reward rate: 0.665, rewarded fraction: 0.913, relative distance: 39.506, skipped fraction: 0.000\n",
      "episode: 3899, mean steps: 15.017, mean reward: 9.241, reward rate: 0.644, rewarded fraction: 0.903, relative distance: 39.244, skipped fraction: 0.000\n",
      "episode: 3999, mean steps: 15.397, mean reward: 9.271, reward rate: 0.627, rewarded fraction: 0.903, relative distance: 38.432, skipped fraction: 0.000\n",
      "episode: 4099, mean steps: 14.740, mean reward: 9.188, reward rate: 0.653, rewarded fraction: 0.897, relative distance: 39.112, skipped fraction: 0.000\n",
      "episode: 4199, mean steps: 15.177, mean reward: 9.367, reward rate: 0.647, rewarded fraction: 0.917, relative distance: 38.674, skipped fraction: 0.000\n",
      "episode: 4299, mean steps: 15.197, mean reward: 9.109, reward rate: 0.622, rewarded fraction: 0.883, relative distance: 38.929, skipped fraction: 0.000\n",
      "episode: 4399, mean steps: 15.103, mean reward: 9.340, reward rate: 0.648, rewarded fraction: 0.913, relative distance: 36.780, skipped fraction: 0.000\n",
      "episode: 4499, mean steps: 14.700, mean reward: 9.166, reward rate: 0.652, rewarded fraction: 0.893, relative distance: 40.280, skipped fraction: 0.000\n",
      "episode: 4599, mean steps: 15.320, mean reward: 9.467, reward rate: 0.649, rewarded fraction: 0.930, relative distance: 38.247, skipped fraction: 0.000\n",
      "episode: 4699, mean steps: 14.160, mean reward: 9.185, reward rate: 0.679, rewarded fraction: 0.893, relative distance: 40.849, skipped fraction: 0.000\n",
      "episode: 4799, mean steps: 15.103, mean reward: 8.922, reward rate: 0.607, rewarded fraction: 0.857, relative distance: 39.903, skipped fraction: 0.000\n",
      "episode: 4899, mean steps: 15.050, mean reward: 8.990, reward rate: 0.619, rewarded fraction: 0.870, relative distance: 41.264, skipped fraction: 0.000\n",
      "episode: 4999, mean steps: 14.697, mean reward: 9.298, reward rate: 0.664, rewarded fraction: 0.910, relative distance: 37.162, skipped fraction: 0.000\n",
      "episode: 5099, mean steps: 14.230, mean reward: 9.235, reward rate: 0.680, rewarded fraction: 0.900, relative distance: 40.311, skipped fraction: 0.000\n",
      "episode: 5199, mean steps: 14.887, mean reward: 9.306, reward rate: 0.655, rewarded fraction: 0.910, relative distance: 37.498, skipped fraction: 0.000\n",
      "episode: 5299, mean steps: 15.250, mean reward: 9.336, reward rate: 0.641, rewarded fraction: 0.913, relative distance: 37.048, skipped fraction: 0.000\n",
      "episode: 5399, mean steps: 14.743, mean reward: 9.446, reward rate: 0.674, rewarded fraction: 0.927, relative distance: 37.918, skipped fraction: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5499, mean steps: 15.103, mean reward: 9.387, reward rate: 0.652, rewarded fraction: 0.920, relative distance: 35.486, skipped fraction: 0.000\n",
      "episode: 5599, mean steps: 14.657, mean reward: 9.358, reward rate: 0.671, rewarded fraction: 0.917, relative distance: 37.298, skipped fraction: 0.000\n",
      "episode: 5699, mean steps: 14.660, mean reward: 9.262, reward rate: 0.661, rewarded fraction: 0.903, relative distance: 39.307, skipped fraction: 0.000\n",
      "episode: 5799, mean steps: 15.027, mean reward: 9.228, reward rate: 0.642, rewarded fraction: 0.900, relative distance: 39.559, skipped fraction: 0.000\n",
      "episode: 5899, mean steps: 15.100, mean reward: 9.527, reward rate: 0.664, rewarded fraction: 0.937, relative distance: 37.632, skipped fraction: 0.000\n",
      "episode: 5999, mean steps: 14.310, mean reward: 9.316, reward rate: 0.686, rewarded fraction: 0.913, relative distance: 39.814, skipped fraction: 0.000\n",
      "episode: 6099, mean steps: 14.853, mean reward: 9.600, reward rate: 0.683, rewarded fraction: 0.947, relative distance: 37.203, skipped fraction: 0.000\n",
      "episode: 6199, mean steps: 14.697, mean reward: 9.267, reward rate: 0.660, rewarded fraction: 0.903, relative distance: 36.928, skipped fraction: 0.000\n",
      "episode: 6299, mean steps: 14.663, mean reward: 9.571, reward rate: 0.690, rewarded fraction: 0.943, relative distance: 36.443, skipped fraction: 0.000\n",
      "episode: 6399, mean steps: 14.747, mean reward: 9.570, reward rate: 0.686, rewarded fraction: 0.943, relative distance: 36.241, skipped fraction: 0.000\n",
      "episode: 6499, mean steps: 14.700, mean reward: 9.442, reward rate: 0.676, rewarded fraction: 0.927, relative distance: 37.089, skipped fraction: 0.000\n",
      "episode: 6599, mean steps: 14.747, mean reward: 9.321, reward rate: 0.664, rewarded fraction: 0.913, relative distance: 38.468, skipped fraction: 0.000\n",
      "episode: 6699, mean steps: 15.377, mean reward: 9.651, reward rate: 0.663, rewarded fraction: 0.953, relative distance: 35.468, skipped fraction: 0.000\n",
      "episode: 6799, mean steps: 14.950, mean reward: 9.542, reward rate: 0.674, rewarded fraction: 0.940, relative distance: 37.148, skipped fraction: 0.000\n",
      "episode: 6899, mean steps: 15.150, mean reward: 9.426, reward rate: 0.653, rewarded fraction: 0.923, relative distance: 37.462, skipped fraction: 0.000\n",
      "episode: 6999, mean steps: 14.560, mean reward: 9.254, reward rate: 0.666, rewarded fraction: 0.903, relative distance: 39.131, skipped fraction: 0.000\n",
      "episode: 7099, mean steps: 14.960, mean reward: 9.526, reward rate: 0.671, rewarded fraction: 0.937, relative distance: 36.899, skipped fraction: 0.000\n",
      "episode: 7199, mean steps: 14.523, mean reward: 9.459, reward rate: 0.688, rewarded fraction: 0.930, relative distance: 35.187, skipped fraction: 0.000\n",
      "episode: 7299, mean steps: 15.300, mean reward: 9.355, reward rate: 0.641, rewarded fraction: 0.917, relative distance: 36.682, skipped fraction: 0.000\n",
      "episode: 7399, mean steps: 15.040, mean reward: 9.303, reward rate: 0.648, rewarded fraction: 0.910, relative distance: 38.250, skipped fraction: 0.000\n",
      "episode: 7499, mean steps: 15.287, mean reward: 9.599, reward rate: 0.663, rewarded fraction: 0.947, relative distance: 36.744, skipped fraction: 0.000\n",
      "episode: 7599, mean steps: 14.630, mean reward: 9.423, reward rate: 0.680, rewarded fraction: 0.927, relative distance: 37.012, skipped fraction: 0.000\n",
      "episode: 7699, mean steps: 15.007, mean reward: 9.368, reward rate: 0.654, rewarded fraction: 0.917, relative distance: 38.207, skipped fraction: 0.000\n",
      "episode: 7799, mean steps: 15.000, mean reward: 9.594, reward rate: 0.676, rewarded fraction: 0.947, relative distance: 34.986, skipped fraction: 0.000\n",
      "episode: 7899, mean steps: 15.170, mean reward: 9.442, reward rate: 0.654, rewarded fraction: 0.927, relative distance: 39.015, skipped fraction: 0.000\n",
      "episode: 7999, mean steps: 14.697, mean reward: 9.514, reward rate: 0.684, rewarded fraction: 0.937, relative distance: 36.674, skipped fraction: 0.000\n",
      "episode: 8099, mean steps: 14.890, mean reward: 9.563, reward rate: 0.679, rewarded fraction: 0.943, relative distance: 35.734, skipped fraction: 0.000\n",
      "episode: 8199, mean steps: 14.877, mean reward: 9.146, reward rate: 0.641, rewarded fraction: 0.890, relative distance: 38.059, skipped fraction: 0.000\n",
      "episode: 8299, mean steps: 15.280, mean reward: 9.427, reward rate: 0.647, rewarded fraction: 0.923, relative distance: 35.559, skipped fraction: 0.000\n",
      "episode: 8399, mean steps: 14.627, mean reward: 9.323, reward rate: 0.668, rewarded fraction: 0.910, relative distance: 38.380, skipped fraction: 0.000\n",
      "episode: 8499, mean steps: 14.873, mean reward: 9.516, reward rate: 0.675, rewarded fraction: 0.937, relative distance: 37.745, skipped fraction: 0.000\n",
      "episode: 8599, mean steps: 14.480, mean reward: 8.844, reward rate: 0.631, rewarded fraction: 0.850, relative distance: 45.210, skipped fraction: 0.000\n",
      "episode: 8699, mean steps: 14.983, mean reward: 9.650, reward rate: 0.682, rewarded fraction: 0.953, relative distance: 34.149, skipped fraction: 0.000\n",
      "episode: 8799, mean steps: 14.960, mean reward: 9.281, reward rate: 0.649, rewarded fraction: 0.907, relative distance: 38.652, skipped fraction: 0.000\n",
      "episode: 8899, mean steps: 14.733, mean reward: 9.411, reward rate: 0.672, rewarded fraction: 0.923, relative distance: 35.888, skipped fraction: 0.000\n",
      "episode: 8999, mean steps: 14.983, mean reward: 9.507, reward rate: 0.670, rewarded fraction: 0.937, relative distance: 35.587, skipped fraction: 0.000\n",
      "episode: 9099, mean steps: 14.693, mean reward: 9.313, reward rate: 0.665, rewarded fraction: 0.910, relative distance: 38.303, skipped fraction: 0.000\n",
      "episode: 9199, mean steps: 15.277, mean reward: 9.076, reward rate: 0.616, rewarded fraction: 0.880, relative distance: 40.513, skipped fraction: 0.000\n",
      "episode: 9299, mean steps: 14.853, mean reward: 9.225, reward rate: 0.650, rewarded fraction: 0.900, relative distance: 38.479, skipped fraction: 0.000\n",
      "episode: 9399, mean steps: 15.100, mean reward: 9.351, reward rate: 0.650, rewarded fraction: 0.917, relative distance: 38.292, skipped fraction: 0.000\n",
      "episode: 9499, mean steps: 14.987, mean reward: 9.717, reward rate: 0.689, rewarded fraction: 0.963, relative distance: 34.508, skipped fraction: 0.000\n",
      "episode: 9599, mean steps: 15.050, mean reward: 9.519, reward rate: 0.667, rewarded fraction: 0.937, relative distance: 36.600, skipped fraction: 0.000\n",
      "episode: 9699, mean steps: 15.357, mean reward: 9.547, reward rate: 0.655, rewarded fraction: 0.940, relative distance: 35.183, skipped fraction: 0.000\n",
      "episode: 9799, mean steps: 15.227, mean reward: 9.455, reward rate: 0.651, rewarded fraction: 0.927, relative distance: 36.079, skipped fraction: 0.000\n",
      "episode: 9899, mean steps: 14.797, mean reward: 9.305, reward rate: 0.660, rewarded fraction: 0.910, relative distance: 37.689, skipped fraction: 0.000\n",
      "episode: 9999, mean steps: 14.990, mean reward: 9.457, reward rate: 0.665, rewarded fraction: 0.930, relative distance: 37.038, skipped fraction: 0.000\n",
      "episode: 99, mean steps: 14.930, mean reward: 9.150, reward rate: 0.644, rewarded fraction: 0.897, relative distance: 37.712, skipped fraction: 0.000\n",
      "episode: 199, mean steps: 14.930, mean reward: 9.150, reward rate: 0.644, rewarded fraction: 0.897, relative distance: 37.712, skipped fraction: 0.000\n",
      "episode: 299, mean steps: 14.930, mean reward: 9.150, reward rate: 0.644, rewarded fraction: 0.897, relative distance: 37.712, skipped fraction: 0.000\n",
      "episode: 399, mean steps: 14.930, mean reward: 9.150, reward rate: 0.644, rewarded fraction: 0.897, relative distance: 37.712, skipped fraction: 0.000\n",
      "episode: 499, mean steps: 15.643, mean reward: 9.046, reward rate: 0.599, rewarded fraction: 0.877, relative distance: 41.253, skipped fraction: 0.000\n",
      "episode: 599, mean steps: 15.120, mean reward: 8.853, reward rate: 0.604, rewarded fraction: 0.853, relative distance: 42.593, skipped fraction: 0.000\n",
      "episode: 699, mean steps: 14.683, mean reward: 8.832, reward rate: 0.624, rewarded fraction: 0.853, relative distance: 44.215, skipped fraction: 0.000\n",
      "episode: 799, mean steps: 14.140, mean reward: 7.975, reward rate: 0.566, rewarded fraction: 0.743, relative distance: 52.084, skipped fraction: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 899, mean steps: 14.317, mean reward: 8.451, reward rate: 0.603, rewarded fraction: 0.803, relative distance: 46.974, skipped fraction: 0.000\n",
      "episode: 999, mean steps: 14.573, mean reward: 8.925, reward rate: 0.634, rewarded fraction: 0.860, relative distance: 44.175, skipped fraction: 0.000\n",
      "episode: 1099, mean steps: 14.433, mean reward: 8.922, reward rate: 0.640, rewarded fraction: 0.860, relative distance: 42.488, skipped fraction: 0.000\n",
      "episode: 1199, mean steps: 14.550, mean reward: 8.943, reward rate: 0.637, rewarded fraction: 0.863, relative distance: 39.970, skipped fraction: 0.000\n",
      "episode: 1299, mean steps: 15.023, mean reward: 9.378, reward rate: 0.656, rewarded fraction: 0.920, relative distance: 40.762, skipped fraction: 0.000\n",
      "episode: 1399, mean steps: 14.860, mean reward: 8.945, reward rate: 0.623, rewarded fraction: 0.863, relative distance: 41.128, skipped fraction: 0.000\n",
      "episode: 1499, mean steps: 15.337, mean reward: 9.053, reward rate: 0.611, rewarded fraction: 0.877, relative distance: 41.579, skipped fraction: 0.000\n",
      "episode: 1599, mean steps: 15.140, mean reward: 9.244, reward rate: 0.636, rewarded fraction: 0.900, relative distance: 40.387, skipped fraction: 0.000\n",
      "episode: 1699, mean steps: 14.653, mean reward: 8.793, reward rate: 0.618, rewarded fraction: 0.843, relative distance: 43.154, skipped fraction: 0.000\n",
      "episode: 1799, mean steps: 14.550, mean reward: 9.480, reward rate: 0.689, rewarded fraction: 0.933, relative distance: 39.366, skipped fraction: 0.000\n",
      "episode: 1899, mean steps: 15.057, mean reward: 8.906, reward rate: 0.612, rewarded fraction: 0.860, relative distance: 43.327, skipped fraction: 0.000\n",
      "episode: 1999, mean steps: 14.530, mean reward: 8.574, reward rate: 0.604, rewarded fraction: 0.817, relative distance: 45.830, skipped fraction: 0.000\n",
      "episode: 2099, mean steps: 14.670, mean reward: 8.786, reward rate: 0.617, rewarded fraction: 0.843, relative distance: 43.506, skipped fraction: 0.000\n",
      "episode: 2199, mean steps: 14.753, mean reward: 9.293, reward rate: 0.659, rewarded fraction: 0.907, relative distance: 39.005, skipped fraction: 0.000\n",
      "episode: 2299, mean steps: 14.620, mean reward: 9.044, reward rate: 0.644, rewarded fraction: 0.877, relative distance: 40.689, skipped fraction: 0.000\n",
      "episode: 2399, mean steps: 15.207, mean reward: 9.382, reward rate: 0.648, rewarded fraction: 0.920, relative distance: 36.159, skipped fraction: 0.000\n",
      "episode: 2499, mean steps: 14.707, mean reward: 9.169, reward rate: 0.652, rewarded fraction: 0.893, relative distance: 38.316, skipped fraction: 0.000\n",
      "episode: 2599, mean steps: 14.943, mean reward: 9.095, reward rate: 0.634, rewarded fraction: 0.883, relative distance: 41.247, skipped fraction: 0.000\n",
      "episode: 2699, mean steps: 15.000, mean reward: 9.260, reward rate: 0.648, rewarded fraction: 0.907, relative distance: 39.841, skipped fraction: 0.000\n",
      "episode: 2799, mean steps: 14.930, mean reward: 9.282, reward rate: 0.651, rewarded fraction: 0.907, relative distance: 39.383, skipped fraction: 0.000\n",
      "episode: 2899, mean steps: 15.227, mean reward: 9.347, reward rate: 0.644, rewarded fraction: 0.917, relative distance: 37.562, skipped fraction: 0.000\n",
      "episode: 2999, mean steps: 14.580, mean reward: 9.317, reward rate: 0.673, rewarded fraction: 0.913, relative distance: 38.240, skipped fraction: 0.000\n",
      "episode: 3099, mean steps: 14.860, mean reward: 9.662, reward rate: 0.690, rewarded fraction: 0.957, relative distance: 36.092, skipped fraction: 0.000\n",
      "episode: 3199, mean steps: 15.623, mean reward: 9.400, reward rate: 0.629, rewarded fraction: 0.920, relative distance: 38.042, skipped fraction: 0.000\n",
      "episode: 3299, mean steps: 14.713, mean reward: 9.113, reward rate: 0.644, rewarded fraction: 0.883, relative distance: 38.875, skipped fraction: 0.000\n",
      "episode: 3399, mean steps: 14.757, mean reward: 9.135, reward rate: 0.645, rewarded fraction: 0.887, relative distance: 40.904, skipped fraction: 0.000\n",
      "episode: 3499, mean steps: 15.007, mean reward: 9.422, reward rate: 0.659, rewarded fraction: 0.923, relative distance: 38.386, skipped fraction: 0.000\n",
      "episode: 3599, mean steps: 15.303, mean reward: 9.540, reward rate: 0.657, rewarded fraction: 0.940, relative distance: 37.293, skipped fraction: 0.000\n",
      "episode: 3699, mean steps: 14.613, mean reward: 9.352, reward rate: 0.671, rewarded fraction: 0.913, relative distance: 38.563, skipped fraction: 0.000\n",
      "episode: 3799, mean steps: 14.443, mean reward: 9.266, reward rate: 0.672, rewarded fraction: 0.903, relative distance: 37.746, skipped fraction: 0.000\n",
      "episode: 3899, mean steps: 14.523, mean reward: 9.085, reward rate: 0.653, rewarded fraction: 0.883, relative distance: 40.828, skipped fraction: 0.000\n",
      "episode: 3999, mean steps: 15.287, mean reward: 9.543, reward rate: 0.658, rewarded fraction: 0.940, relative distance: 37.711, skipped fraction: 0.000\n",
      "episode: 4099, mean steps: 15.397, mean reward: 9.273, reward rate: 0.630, rewarded fraction: 0.907, relative distance: 37.957, skipped fraction: 0.000\n",
      "episode: 4199, mean steps: 14.877, mean reward: 9.385, reward rate: 0.663, rewarded fraction: 0.920, relative distance: 37.373, skipped fraction: 0.000\n",
      "episode: 4299, mean steps: 15.283, mean reward: 9.346, reward rate: 0.639, rewarded fraction: 0.913, relative distance: 36.738, skipped fraction: 0.000\n",
      "episode: 4399, mean steps: 15.447, mean reward: 9.327, reward rate: 0.632, rewarded fraction: 0.913, relative distance: 37.538, skipped fraction: 0.000\n",
      "episode: 4499, mean steps: 15.130, mean reward: 9.315, reward rate: 0.644, rewarded fraction: 0.910, relative distance: 38.331, skipped fraction: 0.000\n",
      "episode: 4599, mean steps: 15.177, mean reward: 9.383, reward rate: 0.649, rewarded fraction: 0.920, relative distance: 37.342, skipped fraction: 0.000\n",
      "episode: 4699, mean steps: 15.207, mean reward: 9.585, reward rate: 0.666, rewarded fraction: 0.947, relative distance: 37.425, skipped fraction: 0.000\n",
      "episode: 4799, mean steps: 15.030, mean reward: 9.682, reward rate: 0.682, rewarded fraction: 0.957, relative distance: 33.177, skipped fraction: 0.000\n",
      "episode: 4899, mean steps: 15.173, mean reward: 9.393, reward rate: 0.649, rewarded fraction: 0.920, relative distance: 35.928, skipped fraction: 0.000\n",
      "episode: 4999, mean steps: 15.430, mean reward: 9.386, reward rate: 0.638, rewarded fraction: 0.920, relative distance: 36.992, skipped fraction: 0.000\n",
      "episode: 5099, mean steps: 15.230, mean reward: 9.442, reward rate: 0.651, rewarded fraction: 0.927, relative distance: 36.230, skipped fraction: 0.000\n",
      "episode: 5199, mean steps: 14.210, mean reward: 9.239, reward rate: 0.681, rewarded fraction: 0.900, relative distance: 37.422, skipped fraction: 0.000\n",
      "episode: 5299, mean steps: 15.400, mean reward: 9.486, reward rate: 0.646, rewarded fraction: 0.930, relative distance: 34.382, skipped fraction: 0.000\n",
      "episode: 5399, mean steps: 15.083, mean reward: 9.547, reward rate: 0.667, rewarded fraction: 0.940, relative distance: 36.173, skipped fraction: 0.000\n",
      "episode: 5499, mean steps: 15.020, mean reward: 9.325, reward rate: 0.654, rewarded fraction: 0.917, relative distance: 36.694, skipped fraction: 0.000\n",
      "episode: 5599, mean steps: 15.203, mean reward: 9.338, reward rate: 0.643, rewarded fraction: 0.913, relative distance: 37.419, skipped fraction: 0.000\n",
      "episode: 5699, mean steps: 15.567, mean reward: 9.648, reward rate: 0.654, rewarded fraction: 0.953, relative distance: 35.628, skipped fraction: 0.000\n",
      "episode: 5799, mean steps: 14.897, mean reward: 9.556, reward rate: 0.676, rewarded fraction: 0.940, relative distance: 36.306, skipped fraction: 0.000\n",
      "episode: 5899, mean steps: 15.297, mean reward: 9.667, reward rate: 0.669, rewarded fraction: 0.957, relative distance: 34.899, skipped fraction: 0.000\n",
      "episode: 5999, mean steps: 14.893, mean reward: 9.481, reward rate: 0.672, rewarded fraction: 0.933, relative distance: 36.139, skipped fraction: 0.000\n",
      "episode: 6099, mean steps: 15.020, mean reward: 9.511, reward rate: 0.668, rewarded fraction: 0.937, relative distance: 34.827, skipped fraction: 0.000\n",
      "episode: 6199, mean steps: 14.943, mean reward: 9.419, reward rate: 0.662, rewarded fraction: 0.923, relative distance: 36.805, skipped fraction: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6299, mean steps: 15.107, mean reward: 9.328, reward rate: 0.647, rewarded fraction: 0.913, relative distance: 36.359, skipped fraction: 0.000\n",
      "episode: 6399, mean steps: 15.300, mean reward: 9.490, reward rate: 0.653, rewarded fraction: 0.933, relative distance: 36.879, skipped fraction: 0.000\n",
      "episode: 6499, mean steps: 15.003, mean reward: 9.341, reward rate: 0.652, rewarded fraction: 0.913, relative distance: 36.867, skipped fraction: 0.000\n",
      "episode: 6599, mean steps: 15.553, mean reward: 9.568, reward rate: 0.648, rewarded fraction: 0.943, relative distance: 36.462, skipped fraction: 0.000\n",
      "episode: 6699, mean steps: 14.670, mean reward: 9.492, reward rate: 0.683, rewarded fraction: 0.933, relative distance: 34.077, skipped fraction: 0.000\n",
      "episode: 6799, mean steps: 15.193, mean reward: 9.626, reward rate: 0.669, rewarded fraction: 0.950, relative distance: 34.536, skipped fraction: 0.000\n",
      "episode: 6899, mean steps: 14.753, mean reward: 9.391, reward rate: 0.669, rewarded fraction: 0.920, relative distance: 37.823, skipped fraction: 0.000\n",
      "episode: 6999, mean steps: 15.353, mean reward: 9.431, reward rate: 0.646, rewarded fraction: 0.927, relative distance: 35.807, skipped fraction: 0.000\n",
      "episode: 7099, mean steps: 15.340, mean reward: 9.590, reward rate: 0.660, rewarded fraction: 0.947, relative distance: 35.101, skipped fraction: 0.000\n",
      "episode: 7199, mean steps: 14.707, mean reward: 9.328, reward rate: 0.666, rewarded fraction: 0.913, relative distance: 38.766, skipped fraction: 0.000\n",
      "episode: 7299, mean steps: 14.987, mean reward: 9.502, reward rate: 0.667, rewarded fraction: 0.933, relative distance: 36.811, skipped fraction: 0.000\n",
      "episode: 7399, mean steps: 15.117, mean reward: 9.560, reward rate: 0.666, rewarded fraction: 0.940, relative distance: 35.375, skipped fraction: 0.000\n",
      "episode: 7499, mean steps: 15.103, mean reward: 9.677, reward rate: 0.678, rewarded fraction: 0.957, relative distance: 34.597, skipped fraction: 0.000\n",
      "episode: 7599, mean steps: 15.033, mean reward: 9.541, reward rate: 0.670, rewarded fraction: 0.940, relative distance: 35.740, skipped fraction: 0.000\n",
      "episode: 7699, mean steps: 15.013, mean reward: 9.378, reward rate: 0.657, rewarded fraction: 0.920, relative distance: 37.044, skipped fraction: 0.000\n",
      "episode: 7799, mean steps: 15.013, mean reward: 9.395, reward rate: 0.657, rewarded fraction: 0.920, relative distance: 36.227, skipped fraction: 0.000\n",
      "episode: 7899, mean steps: 14.897, mean reward: 9.557, reward rate: 0.676, rewarded fraction: 0.940, relative distance: 37.851, skipped fraction: 0.000\n",
      "episode: 7999, mean steps: 15.307, mean reward: 9.281, reward rate: 0.631, rewarded fraction: 0.903, relative distance: 40.418, skipped fraction: 0.000\n",
      "episode: 8099, mean steps: 14.910, mean reward: 9.457, reward rate: 0.669, rewarded fraction: 0.930, relative distance: 38.641, skipped fraction: 0.000\n",
      "episode: 8199, mean steps: 15.413, mean reward: 9.773, reward rate: 0.673, rewarded fraction: 0.970, relative distance: 34.869, skipped fraction: 0.000\n",
      "episode: 8299, mean steps: 14.900, mean reward: 9.374, reward rate: 0.659, rewarded fraction: 0.917, relative distance: 37.940, skipped fraction: 0.000\n",
      "episode: 8399, mean steps: 14.930, mean reward: 9.301, reward rate: 0.651, rewarded fraction: 0.907, relative distance: 38.213, skipped fraction: 0.000\n",
      "episode: 8499, mean steps: 15.517, mean reward: 9.538, reward rate: 0.648, rewarded fraction: 0.940, relative distance: 34.645, skipped fraction: 0.000\n",
      "episode: 8599, mean steps: 15.030, mean reward: 9.564, reward rate: 0.672, rewarded fraction: 0.943, relative distance: 35.401, skipped fraction: 0.000\n",
      "episode: 8699, mean steps: 15.117, mean reward: 9.449, reward rate: 0.656, rewarded fraction: 0.927, relative distance: 35.764, skipped fraction: 0.000\n",
      "episode: 8799, mean steps: 14.893, mean reward: 9.700, reward rate: 0.691, rewarded fraction: 0.960, relative distance: 33.072, skipped fraction: 0.000\n",
      "episode: 8899, mean steps: 15.203, mean reward: 9.629, reward rate: 0.669, rewarded fraction: 0.950, relative distance: 34.510, skipped fraction: 0.000\n",
      "episode: 8999, mean steps: 15.143, mean reward: 9.305, reward rate: 0.643, rewarded fraction: 0.910, relative distance: 37.870, skipped fraction: 0.000\n",
      "episode: 9099, mean steps: 15.307, mean reward: 9.650, reward rate: 0.669, rewarded fraction: 0.957, relative distance: 33.131, skipped fraction: 0.000\n",
      "episode: 9199, mean steps: 15.123, mean reward: 9.337, reward rate: 0.647, rewarded fraction: 0.913, relative distance: 38.872, skipped fraction: 0.000\n",
      "episode: 9299, mean steps: 14.637, mean reward: 9.424, reward rate: 0.680, rewarded fraction: 0.927, relative distance: 38.946, skipped fraction: 0.000\n",
      "episode: 9399, mean steps: 15.123, mean reward: 9.659, reward rate: 0.677, rewarded fraction: 0.957, relative distance: 36.405, skipped fraction: 0.000\n",
      "episode: 9499, mean steps: 14.893, mean reward: 9.437, reward rate: 0.667, rewarded fraction: 0.927, relative distance: 38.422, skipped fraction: 0.000\n",
      "episode: 9599, mean steps: 14.870, mean reward: 9.519, reward rate: 0.675, rewarded fraction: 0.937, relative distance: 35.662, skipped fraction: 0.000\n",
      "episode: 9699, mean steps: 15.303, mean reward: 9.499, reward rate: 0.653, rewarded fraction: 0.933, relative distance: 35.410, skipped fraction: 0.000\n",
      "episode: 9799, mean steps: 14.903, mean reward: 9.649, reward rate: 0.686, rewarded fraction: 0.953, relative distance: 36.149, skipped fraction: 0.000\n",
      "episode: 9899, mean steps: 15.120, mean reward: 9.883, reward rate: 0.696, rewarded fraction: 0.983, relative distance: 32.516, skipped fraction: 0.000\n",
      "episode: 9999, mean steps: 14.817, mean reward: 9.597, reward rate: 0.685, rewarded fraction: 0.947, relative distance: 35.440, skipped fraction: 0.000\n"
     ]
    }
   ],
   "source": [
    "for actor, critic, seed_ in zip(actors, critics, seeds):\n",
    "    for seed in seed_:\n",
    "        datapath = folder_path / f'{actor}{critic}' / f'seed{seed}' / f'{agent_type}'\n",
    "        exec(f'from {actor} import *'); exec(f'from {critic} import *')\n",
    "        logfile = pd.DataFrame(columns=['episode', 'reward_fraction', 'error_distance', 'reward_rate'])\n",
    "        agent_name = [v.stem.split('_')[0] for v in datapath.glob('*.pkl')][0]\n",
    "        #pre_epis = [int(v.name.split('_')[0].split('-')[-1]) for v in datapath.glob('*_pre.pth.tar')]\n",
    "        #last_pre_epi = np.sort(pre_epis)[-1]\n",
    "        #logfile = validation(datapath, seed, Actor, Critic, f'{agent_name}-{last_pre_epi}_pre', logfile, 0, value_noise_std)\n",
    "        for epi in np.arange(99, int(TOTAL_EPISODE), 100):\n",
    "            logfile = validation(datapath, seed, Actor, Critic, f'{agent_name}-{epi}', logfile, epi, value_noise_std)\n",
    "        logfile.to_csv(datapath / f'{agent_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
