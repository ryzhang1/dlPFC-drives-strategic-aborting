{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from Environment import Env\n",
    "from Agent_LSTM import *\n",
    "from pathlib import Path\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(datapath, seed_number, Actor, Critic, TOTAL_EPISODE=5e4, value_noise_std=0,\n",
    "             freeze_RNN=False, freeze_actor=False):\n",
    "    # get configures\n",
    "    arg = config.ConfigGain(datapath)\n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.save()\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "\n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg, Actor, Critic)\n",
    "    agent.actor.value_noise_std = value_noise_std\n",
    "    agent.episodic = False\n",
    "    agent.data_path = datapath.parent / 'episodic'\n",
    "    \n",
    "    epis = [int(v.stem.split('.')[0].split('-')[-1])\n",
    "        for v in agent.data_path.glob('*.pth.tar')\n",
    "        if v.stem.split('.')[0].split('-')[-1].isdigit()]\n",
    "    epi_load = str(np.sort(epis)[-1])\n",
    "    name_load = [v.stem.split('_')[0] for v in agent.data_path.glob('*.pkl')][0]\n",
    "    name_load = '-'.join([name_load, epi_load])\n",
    "    print(name_load)\n",
    "    agent.load(name_load, load_memory=False, load_optimzer=True, full_param=True, load_name=False)\n",
    "    agent.data_path = datapath\n",
    "\n",
    "    if freeze_RNN and not freeze_actor:\n",
    "        for param in agent.critic.rnn1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in agent.critic.rnn2.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('RNN is frozen.')\n",
    "    elif freeze_actor and not freeze_RNN:\n",
    "        for param in agent.actor.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('Actor is frozen.')\n",
    "    elif not freeze_actor and not freeze_RNN:\n",
    "        print('No param. frozen.')\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    # define exploration noise\n",
    "    noise = ActionNoise(arg.ACTION_DIM, mean=0, std=0.5)\n",
    "    \n",
    "    # Loop now\n",
    "    tot_t = 0\n",
    "    episode = agent.initial_episode\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    actor_loss_log = 0\n",
    "    critic_loss_log = 0\n",
    "    num_update = 1e-5\n",
    "    dist_log = []\n",
    "\n",
    "    LOG_FREQ = 100\n",
    "    REPLAY_PERIOD = 4\n",
    "    PRE_LEARN_PERIOD = arg.BATCH_SIZE * 50\n",
    "\n",
    "    # Start loop\n",
    "    while episode < TOTAL_EPISODE:\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        x = env.reset()\n",
    "        agent.bstep.reset(env.pro_gains)\n",
    "        \n",
    "        last_action = torch.zeros(1, 1, arg.ACTION_DIM)\n",
    "        last_action_raw = last_action.clone()\n",
    "\n",
    "        state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                           env.target_position_obs.view(1, 1, -1), \n",
    "                           torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "\n",
    "        hiddenin = None\n",
    "        tend = 0\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(arg.EPISODE_LEN):\n",
    "            # 1. Check start threshold.\n",
    "            if not cross_start_threshold and (last_action.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state \n",
    "            # and previous hidden & cell states of LSTM units.\n",
    "            action, action_raw, hiddenout = agent.select_action(state, hiddenin, action_noise=noise)\n",
    "            if len(agent.memory.memory) <= PRE_LEARN_PERIOD and np.random.rand() > 0.95:\n",
    "                action = torch.zeros_like(action)\n",
    "\n",
    "            # 3. Track next x in the environment.\n",
    "            next_x, reached_target, relative_dist = env(x, action, t - tend)\n",
    "\n",
    "            # 4. Next observation given next x.\n",
    "            next_ox = agent.bstep(next_x)\n",
    "            next_state = torch.cat([next_ox.view(1, 1, -1), action, env.target_position_obs.view(1, 1, -1),\n",
    "                                    torch.ones(1, 1, 1) * t - tend + 1], dim=2).to(arg.device)\n",
    "\n",
    "            # 5. Check whether stop.\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if stopped.          \n",
    "            if is_stop and cross_start_threshold:\n",
    "                reward = env.return_reward(x, reward_mode='mixed')\n",
    "                done = torch.ones(1, 1, 1)\n",
    "            else:\n",
    "                reward = torch.zeros(1, 1, 1)\n",
    "                done = torch.zeros(1, 1, 1)\n",
    "\n",
    "            # 7. Append data.\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            # 8. Update timestep.\n",
    "            last_action_raw = action_raw\n",
    "            last_action = action\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            hiddenin = hiddenout\n",
    "            tot_t += 1\n",
    "\n",
    "            # 9. Update model.\n",
    "            if len(agent.memory.memory) > PRE_LEARN_PERIOD and tot_t % REPLAY_PERIOD == 0:\n",
    "                actor_loss, critic_loss = agent.learn()\n",
    "                actor_loss_log += actor_loss\n",
    "                critic_loss_log += critic_loss\n",
    "                num_update += 1\n",
    "\n",
    "            # 10. whether break.\n",
    "            if is_stop and cross_start_threshold:\n",
    "                step_log.append(t + 1 - tend)\n",
    "                reward_log.append(reward.item())\n",
    "                rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "                dist_log.append(relative_dist.item())\n",
    "                # initialize a trial\n",
    "                cross_start_threshold = False\n",
    "                x = env.reset()\n",
    "                agent.bstep.reset(env.pro_gains)\n",
    "                state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                                   env.target_position_obs.view(1, 1, -1),\n",
    "                                   torch.zeros(1, 1, 1)], dim=2).to(arg.device)\n",
    "                tend = t + 1\n",
    "\n",
    "\n",
    "        # store the last state\n",
    "        states.append(state)\n",
    "        # End of one trial, store trajectory into buffer.\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions).to(arg.device)\n",
    "        rewards = torch.cat(rewards).to(arg.device)\n",
    "        dones = torch.cat(dones).to(arg.device)\n",
    "        agent.memory.push(states, actions, rewards, dones) \n",
    "\n",
    "        # store mirrored trajectories reflected along y-axis\n",
    "        agent.memory.push(*agent.mirror_traj(states, actions), rewards, dones) \n",
    "\n",
    "        if episode % LOG_FREQ == LOG_FREQ - 1:\n",
    "            # save\n",
    "            agent.save(save_memory=False, episode=episode, pre_phase=False, full_param=False)\n",
    "            \n",
    "            print(f\"t: {tot_t}, Ep: {episode}, action std: {noise.std:0.2f}\")\n",
    "            print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "                  f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "                  f\"reward rate: {np.sum(reward_log) / np.sum(step_log):0.3f}, \"\n",
    "                  f\"rewarded fraction: {np.mean(rewarded_trial_log):0.3f}, \"\n",
    "                  f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "                  f\"critic loss: {critic_loss_log / num_update:0.3f}, \"\n",
    "                  f\"actor loss: {-actor_loss_log / (num_update/2):0.3f}\")\n",
    "            \n",
    "            reward_rate = np.sum(reward_log) / np.sum(step_log)    \n",
    "\n",
    "            reward_log = []\n",
    "            rewarded_trial_log = []\n",
    "            step_log = []\n",
    "            actor_loss_log = 0\n",
    "            critic_loss_log = 0\n",
    "            num_update = 1e-5\n",
    "            dist_log = []\n",
    "            \n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ['Actor_novalue']\n",
    "critics = ['Critic']\n",
    "seeds = [[19,20,21]]\n",
    "expnoise_std = 0.5\n",
    "TOTAL_EPISODE = 1e4\n",
    "folder_path = Path('D:/quitting_data/agents')\n",
    "\n",
    "value_noise_std = 0\n",
    "\n",
    "freeze_RNN = False; freeze_actor = False\n",
    "\n",
    "if freeze_RNN and not freeze_actor:  \n",
    "    agent_type = 'freeze_RNN'\n",
    "elif freeze_actor and not freeze_RNN:\n",
    "    agent_type = 'freeze_actor'\n",
    "elif not freeze_RNN and not freeze_RNN:\n",
    "    agent_type = 'no_freeze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240430-183238-299\n",
      "No param. frozen.\n",
      "t: 10000, Ep: 99, action std: 0.50\n",
      "mean steps: 12.510, mean reward: 4.321, reward rate: 0.345, rewarded fraction: 0.402, relative distance: 128.326, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.50\n",
      "mean steps: 11.852, mean reward: 4.246, reward rate: 0.358, rewarded fraction: 0.404, relative distance: 133.714, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.50\n",
      "mean steps: 11.967, mean reward: 4.512, reward rate: 0.377, rewarded fraction: 0.428, relative distance: 126.045, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.50\n",
      "mean steps: 11.669, mean reward: 4.098, reward rate: 0.351, rewarded fraction: 0.383, relative distance: 135.060, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.50\n",
      "mean steps: 16.861, mean reward: 6.878, reward rate: 0.408, rewarded fraction: 0.630, relative distance: 59.141, critic loss: 1.161, actor loss: 7.713\n",
      "t: 60000, Ep: 599, action std: 0.50\n",
      "mean steps: 17.275, mean reward: 8.715, reward rate: 0.505, rewarded fraction: 0.839, relative distance: 43.598, critic loss: 1.404, actor loss: 10.123\n",
      "t: 70000, Ep: 699, action std: 0.50\n",
      "mean steps: 18.045, mean reward: 8.474, reward rate: 0.470, rewarded fraction: 0.811, relative distance: 49.046, critic loss: 1.479, actor loss: 12.480\n",
      "t: 80000, Ep: 799, action std: 0.50\n",
      "mean steps: 18.020, mean reward: 8.671, reward rate: 0.481, rewarded fraction: 0.836, relative distance: 44.331, critic loss: 1.103, actor loss: 14.360\n",
      "t: 90000, Ep: 899, action std: 0.50\n",
      "mean steps: 17.743, mean reward: 8.413, reward rate: 0.474, rewarded fraction: 0.802, relative distance: 47.398, critic loss: 1.049, actor loss: 15.884\n",
      "t: 100000, Ep: 999, action std: 0.50\n",
      "mean steps: 17.584, mean reward: 8.465, reward rate: 0.481, rewarded fraction: 0.809, relative distance: 47.369, critic loss: 1.060, actor loss: 17.192\n",
      "t: 110000, Ep: 1099, action std: 0.50\n",
      "mean steps: 17.720, mean reward: 8.671, reward rate: 0.489, rewarded fraction: 0.833, relative distance: 47.268, critic loss: 1.025, actor loss: 18.293\n",
      "t: 120000, Ep: 1199, action std: 0.50\n",
      "mean steps: 17.776, mean reward: 8.146, reward rate: 0.458, rewarded fraction: 0.778, relative distance: 51.284, critic loss: 1.017, actor loss: 19.316\n",
      "t: 130000, Ep: 1299, action std: 0.50\n",
      "mean steps: 17.302, mean reward: 7.835, reward rate: 0.453, rewarded fraction: 0.742, relative distance: 54.769, critic loss: 0.989, actor loss: 20.184\n",
      "t: 140000, Ep: 1399, action std: 0.50\n",
      "mean steps: 17.617, mean reward: 8.069, reward rate: 0.458, rewarded fraction: 0.765, relative distance: 53.531, critic loss: 0.979, actor loss: 20.892\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 17.327, mean reward: 8.390, reward rate: 0.484, rewarded fraction: 0.799, relative distance: 48.480, critic loss: 1.002, actor loss: 21.567\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 17.606, mean reward: 8.377, reward rate: 0.476, rewarded fraction: 0.801, relative distance: 49.069, critic loss: 1.002, actor loss: 22.156\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 17.451, mean reward: 8.359, reward rate: 0.479, rewarded fraction: 0.792, relative distance: 48.223, critic loss: 0.983, actor loss: 22.599\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 17.090, mean reward: 8.483, reward rate: 0.496, rewarded fraction: 0.819, relative distance: 49.991, critic loss: 0.975, actor loss: 23.000\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 17.287, mean reward: 8.483, reward rate: 0.491, rewarded fraction: 0.816, relative distance: 48.678, critic loss: 0.974, actor loss: 23.353\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 17.400, mean reward: 8.346, reward rate: 0.480, rewarded fraction: 0.792, relative distance: 52.440, critic loss: 0.969, actor loss: 23.644\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 17.189, mean reward: 8.629, reward rate: 0.502, rewarded fraction: 0.828, relative distance: 47.463, critic loss: 0.961, actor loss: 23.897\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 17.145, mean reward: 8.525, reward rate: 0.497, rewarded fraction: 0.814, relative distance: 47.296, critic loss: 0.971, actor loss: 24.108\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 16.977, mean reward: 8.258, reward rate: 0.486, rewarded fraction: 0.786, relative distance: 50.658, critic loss: 0.945, actor loss: 24.281\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 17.201, mean reward: 8.106, reward rate: 0.471, rewarded fraction: 0.765, relative distance: 50.529, critic loss: 0.950, actor loss: 24.418\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 16.802, mean reward: 8.380, reward rate: 0.499, rewarded fraction: 0.797, relative distance: 48.699, critic loss: 0.949, actor loss: 24.519\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 17.037, mean reward: 8.774, reward rate: 0.515, rewarded fraction: 0.846, relative distance: 45.829, critic loss: 0.940, actor loss: 24.597\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 17.128, mean reward: 8.398, reward rate: 0.490, rewarded fraction: 0.800, relative distance: 48.464, critic loss: 0.939, actor loss: 24.649\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 17.539, mean reward: 8.754, reward rate: 0.499, rewarded fraction: 0.844, relative distance: 46.491, critic loss: 0.948, actor loss: 24.691\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 17.291, mean reward: 8.678, reward rate: 0.502, rewarded fraction: 0.835, relative distance: 47.347, critic loss: 0.917, actor loss: 24.713\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 17.595, mean reward: 8.633, reward rate: 0.491, rewarded fraction: 0.827, relative distance: 46.810, critic loss: 0.924, actor loss: 24.736\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 17.132, mean reward: 8.570, reward rate: 0.500, rewarded fraction: 0.823, relative distance: 46.490, critic loss: 0.922, actor loss: 24.752\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 17.312, mean reward: 8.493, reward rate: 0.491, rewarded fraction: 0.810, relative distance: 49.484, critic loss: 0.921, actor loss: 24.761\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 17.479, mean reward: 8.489, reward rate: 0.486, rewarded fraction: 0.810, relative distance: 47.097, critic loss: 0.916, actor loss: 24.762\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 17.610, mean reward: 8.506, reward rate: 0.483, rewarded fraction: 0.817, relative distance: 48.010, critic loss: 0.940, actor loss: 24.763\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 17.706, mean reward: 8.566, reward rate: 0.484, rewarded fraction: 0.825, relative distance: 47.419, critic loss: 0.932, actor loss: 24.732\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 17.350, mean reward: 8.732, reward rate: 0.503, rewarded fraction: 0.844, relative distance: 47.174, critic loss: 0.915, actor loss: 24.743\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 18.070, mean reward: 8.633, reward rate: 0.478, rewarded fraction: 0.826, relative distance: 46.116, critic loss: 0.928, actor loss: 24.730\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 17.258, mean reward: 8.620, reward rate: 0.499, rewarded fraction: 0.825, relative distance: 46.037, critic loss: 0.939, actor loss: 24.693\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 17.666, mean reward: 8.756, reward rate: 0.496, rewarded fraction: 0.846, relative distance: 45.558, critic loss: 0.915, actor loss: 24.666\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 17.208, mean reward: 8.669, reward rate: 0.504, rewarded fraction: 0.834, relative distance: 46.959, critic loss: 0.926, actor loss: 24.673\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 17.340, mean reward: 8.714, reward rate: 0.503, rewarded fraction: 0.836, relative distance: 44.957, critic loss: 0.926, actor loss: 24.682\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 17.439, mean reward: 8.604, reward rate: 0.493, rewarded fraction: 0.824, relative distance: 46.943, critic loss: 0.932, actor loss: 24.682\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 17.260, mean reward: 8.406, reward rate: 0.487, rewarded fraction: 0.803, relative distance: 49.471, critic loss: 0.941, actor loss: 24.681\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 17.225, mean reward: 8.535, reward rate: 0.495, rewarded fraction: 0.820, relative distance: 47.475, critic loss: 0.941, actor loss: 24.670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 17.197, mean reward: 8.504, reward rate: 0.494, rewarded fraction: 0.814, relative distance: 47.163, critic loss: 0.937, actor loss: 24.670\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 17.144, mean reward: 8.412, reward rate: 0.491, rewarded fraction: 0.801, relative distance: 48.795, critic loss: 0.947, actor loss: 24.654\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 17.184, mean reward: 8.451, reward rate: 0.492, rewarded fraction: 0.807, relative distance: 47.244, critic loss: 0.955, actor loss: 24.632\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 17.453, mean reward: 8.748, reward rate: 0.501, rewarded fraction: 0.845, relative distance: 46.259, critic loss: 0.939, actor loss: 24.615\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 17.219, mean reward: 8.495, reward rate: 0.493, rewarded fraction: 0.816, relative distance: 48.640, critic loss: 0.937, actor loss: 24.614\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 16.854, mean reward: 8.491, reward rate: 0.504, rewarded fraction: 0.813, relative distance: 48.592, critic loss: 0.940, actor loss: 24.613\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 16.897, mean reward: 8.461, reward rate: 0.501, rewarded fraction: 0.811, relative distance: 47.533, critic loss: 0.931, actor loss: 24.618\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 17.171, mean reward: 8.184, reward rate: 0.477, rewarded fraction: 0.774, relative distance: 49.427, critic loss: 0.955, actor loss: 24.606\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 17.778, mean reward: 8.566, reward rate: 0.482, rewarded fraction: 0.822, relative distance: 47.094, critic loss: 0.958, actor loss: 24.611\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 17.209, mean reward: 8.587, reward rate: 0.499, rewarded fraction: 0.822, relative distance: 46.371, critic loss: 0.932, actor loss: 24.638\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 17.128, mean reward: 8.734, reward rate: 0.510, rewarded fraction: 0.841, relative distance: 44.957, critic loss: 0.940, actor loss: 24.662\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 17.613, mean reward: 8.685, reward rate: 0.493, rewarded fraction: 0.839, relative distance: 47.529, critic loss: 1.019, actor loss: 24.677\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 17.317, mean reward: 8.537, reward rate: 0.493, rewarded fraction: 0.821, relative distance: 48.740, critic loss: 0.968, actor loss: 24.697\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 17.566, mean reward: 8.696, reward rate: 0.495, rewarded fraction: 0.839, relative distance: 46.937, critic loss: 0.978, actor loss: 24.728\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 17.451, mean reward: 8.640, reward rate: 0.495, rewarded fraction: 0.828, relative distance: 44.407, critic loss: 0.976, actor loss: 24.741\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 17.131, mean reward: 8.590, reward rate: 0.501, rewarded fraction: 0.827, relative distance: 47.803, critic loss: 0.993, actor loss: 24.761\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 16.912, mean reward: 8.410, reward rate: 0.497, rewarded fraction: 0.799, relative distance: 48.648, critic loss: 1.134, actor loss: 24.747\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 17.242, mean reward: 8.770, reward rate: 0.509, rewarded fraction: 0.850, relative distance: 45.221, critic loss: 1.016, actor loss: 24.801\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 17.086, mean reward: 8.733, reward rate: 0.511, rewarded fraction: 0.842, relative distance: 46.808, critic loss: 1.006, actor loss: 24.871\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 17.081, mean reward: 8.379, reward rate: 0.491, rewarded fraction: 0.803, relative distance: 48.532, critic loss: 1.132, actor loss: 24.884\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 17.242, mean reward: 8.684, reward rate: 0.504, rewarded fraction: 0.839, relative distance: 46.931, critic loss: 1.063, actor loss: 24.849\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 16.935, mean reward: 8.732, reward rate: 0.516, rewarded fraction: 0.846, relative distance: 47.500, critic loss: 0.992, actor loss: 24.925\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 16.886, mean reward: 8.379, reward rate: 0.496, rewarded fraction: 0.803, relative distance: 48.183, critic loss: 1.421, actor loss: 24.899\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 16.897, mean reward: 8.685, reward rate: 0.514, rewarded fraction: 0.836, relative distance: 45.877, critic loss: 1.172, actor loss: 24.946\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 16.934, mean reward: 8.782, reward rate: 0.519, rewarded fraction: 0.851, relative distance: 44.272, critic loss: 1.057, actor loss: 24.998\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 16.978, mean reward: 8.577, reward rate: 0.505, rewarded fraction: 0.823, relative distance: 46.640, critic loss: 0.967, actor loss: 25.051\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 16.993, mean reward: 8.800, reward rate: 0.518, rewarded fraction: 0.850, relative distance: 45.786, critic loss: 1.013, actor loss: 25.105\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 17.043, mean reward: 8.478, reward rate: 0.497, rewarded fraction: 0.812, relative distance: 48.499, critic loss: 1.315, actor loss: 25.149\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 17.363, mean reward: 8.708, reward rate: 0.502, rewarded fraction: 0.841, relative distance: 45.101, critic loss: 1.157, actor loss: 25.170\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 16.874, mean reward: 8.510, reward rate: 0.504, rewarded fraction: 0.815, relative distance: 46.593, critic loss: 1.030, actor loss: 25.301\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 16.943, mean reward: 8.309, reward rate: 0.490, rewarded fraction: 0.792, relative distance: 49.723, critic loss: 1.412, actor loss: 25.260\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 17.592, mean reward: 8.374, reward rate: 0.476, rewarded fraction: 0.800, relative distance: 49.098, critic loss: 1.051, actor loss: 25.198\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 17.061, mean reward: 8.756, reward rate: 0.513, rewarded fraction: 0.851, relative distance: 47.517, critic loss: 1.041, actor loss: 25.259\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 17.192, mean reward: 8.754, reward rate: 0.509, rewarded fraction: 0.853, relative distance: 49.048, critic loss: 1.062, actor loss: 25.294\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 16.405, mean reward: 6.676, reward rate: 0.407, rewarded fraction: 0.617, relative distance: 66.917, critic loss: 1.635, actor loss: 25.048\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 17.328, mean reward: 8.485, reward rate: 0.490, rewarded fraction: 0.814, relative distance: 47.504, critic loss: 1.340, actor loss: 24.683\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 17.205, mean reward: 8.854, reward rate: 0.515, rewarded fraction: 0.855, relative distance: 43.916, critic loss: 1.789, actor loss: 24.785\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 17.244, mean reward: 8.479, reward rate: 0.492, rewarded fraction: 0.811, relative distance: 47.217, critic loss: 1.632, actor loss: 24.824\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 16.869, mean reward: 8.701, reward rate: 0.516, rewarded fraction: 0.841, relative distance: 48.600, critic loss: 1.084, actor loss: 24.947\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 16.845, mean reward: 8.627, reward rate: 0.512, rewarded fraction: 0.828, relative distance: 47.111, critic loss: 1.083, actor loss: 25.078\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 16.891, mean reward: 8.716, reward rate: 0.516, rewarded fraction: 0.841, relative distance: 46.188, critic loss: 1.055, actor loss: 25.187\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 16.737, mean reward: 7.842, reward rate: 0.469, rewarded fraction: 0.740, relative distance: 51.976, critic loss: 1.651, actor loss: 25.191\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 16.951, mean reward: 8.839, reward rate: 0.521, rewarded fraction: 0.853, relative distance: 44.751, critic loss: 1.285, actor loss: 25.054\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 17.314, mean reward: 8.820, reward rate: 0.509, rewarded fraction: 0.855, relative distance: 45.135, critic loss: 1.039, actor loss: 25.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 17.165, mean reward: 8.901, reward rate: 0.519, rewarded fraction: 0.865, relative distance: 45.856, critic loss: 1.060, actor loss: 25.275\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 16.860, mean reward: 8.598, reward rate: 0.510, rewarded fraction: 0.832, relative distance: 48.520, critic loss: 1.089, actor loss: 25.398\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 17.217, mean reward: 8.764, reward rate: 0.509, rewarded fraction: 0.850, relative distance: 46.276, critic loss: 1.005, actor loss: 25.509\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 17.206, mean reward: 8.377, reward rate: 0.487, rewarded fraction: 0.805, relative distance: 52.620, critic loss: 1.007, actor loss: 25.555\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 17.236, mean reward: 8.516, reward rate: 0.494, rewarded fraction: 0.819, relative distance: 48.250, critic loss: 1.005, actor loss: 25.589\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 16.953, mean reward: 8.524, reward rate: 0.503, rewarded fraction: 0.817, relative distance: 48.642, critic loss: 1.285, actor loss: 25.503\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 17.026, mean reward: 8.310, reward rate: 0.488, rewarded fraction: 0.787, relative distance: 48.510, critic loss: 1.360, actor loss: 25.486\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 16.734, mean reward: 8.433, reward rate: 0.504, rewarded fraction: 0.806, relative distance: 48.168, critic loss: 1.164, actor loss: 25.514\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 16.502, mean reward: 8.652, reward rate: 0.524, rewarded fraction: 0.837, relative distance: 47.394, critic loss: 1.099, actor loss: 25.566\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 16.993, mean reward: 8.668, reward rate: 0.510, rewarded fraction: 0.835, relative distance: 46.598, critic loss: 1.263, actor loss: 25.610\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 17.707, mean reward: 8.765, reward rate: 0.495, rewarded fraction: 0.846, relative distance: 45.974, critic loss: 1.038, actor loss: 25.684\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 16.966, mean reward: 8.786, reward rate: 0.518, rewarded fraction: 0.850, relative distance: 47.076, critic loss: 1.042, actor loss: 25.739\n",
      "20240430-185634-299\n",
      "No param. frozen.\n",
      "t: 10000, Ep: 99, action std: 0.50\n",
      "mean steps: 11.880, mean reward: 4.249, reward rate: 0.358, rewarded fraction: 0.405, relative distance: 135.436, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.50\n",
      "mean steps: 11.918, mean reward: 4.409, reward rate: 0.370, rewarded fraction: 0.415, relative distance: 132.009, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.50\n",
      "mean steps: 12.134, mean reward: 4.207, reward rate: 0.347, rewarded fraction: 0.393, relative distance: 132.955, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.50\n",
      "mean steps: 12.295, mean reward: 4.483, reward rate: 0.365, rewarded fraction: 0.423, relative distance: 127.958, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.50\n",
      "mean steps: 16.246, mean reward: 6.872, reward rate: 0.423, rewarded fraction: 0.619, relative distance: 59.774, critic loss: 0.778, actor loss: 7.278\n",
      "t: 60000, Ep: 599, action std: 0.50\n",
      "mean steps: 17.000, mean reward: 8.517, reward rate: 0.501, rewarded fraction: 0.813, relative distance: 47.922, critic loss: 0.808, actor loss: 9.286\n",
      "t: 70000, Ep: 699, action std: 0.50\n",
      "mean steps: 16.925, mean reward: 8.028, reward rate: 0.474, rewarded fraction: 0.757, relative distance: 52.205, critic loss: 0.902, actor loss: 11.218\n",
      "t: 80000, Ep: 799, action std: 0.50\n",
      "mean steps: 16.634, mean reward: 8.183, reward rate: 0.492, rewarded fraction: 0.775, relative distance: 51.732, critic loss: 0.801, actor loss: 12.733\n",
      "t: 90000, Ep: 899, action std: 0.50\n",
      "mean steps: 17.148, mean reward: 8.149, reward rate: 0.475, rewarded fraction: 0.766, relative distance: 49.950, critic loss: 0.753, actor loss: 13.917\n",
      "t: 100000, Ep: 999, action std: 0.50\n",
      "mean steps: 17.006, mean reward: 8.279, reward rate: 0.487, rewarded fraction: 0.783, relative distance: 48.315, critic loss: 0.741, actor loss: 14.943\n",
      "t: 110000, Ep: 1099, action std: 0.50\n",
      "mean steps: 17.124, mean reward: 8.071, reward rate: 0.471, rewarded fraction: 0.766, relative distance: 52.213, critic loss: 0.761, actor loss: 15.872\n",
      "t: 120000, Ep: 1199, action std: 0.50\n",
      "mean steps: 17.337, mean reward: 8.193, reward rate: 0.473, rewarded fraction: 0.778, relative distance: 49.987, critic loss: 0.772, actor loss: 16.709\n",
      "t: 130000, Ep: 1299, action std: 0.50\n",
      "mean steps: 17.143, mean reward: 8.442, reward rate: 0.492, rewarded fraction: 0.804, relative distance: 49.048, critic loss: 0.769, actor loss: 17.438\n",
      "t: 140000, Ep: 1399, action std: 0.50\n",
      "mean steps: 16.972, mean reward: 8.341, reward rate: 0.491, rewarded fraction: 0.796, relative distance: 49.190, critic loss: 0.798, actor loss: 18.053\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 16.964, mean reward: 8.235, reward rate: 0.485, rewarded fraction: 0.784, relative distance: 50.181, critic loss: 0.818, actor loss: 18.576\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 17.494, mean reward: 8.404, reward rate: 0.480, rewarded fraction: 0.802, relative distance: 49.331, critic loss: 0.843, actor loss: 19.023\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 17.367, mean reward: 8.382, reward rate: 0.483, rewarded fraction: 0.802, relative distance: 49.115, critic loss: 0.834, actor loss: 19.397\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 17.534, mean reward: 8.316, reward rate: 0.474, rewarded fraction: 0.791, relative distance: 48.553, critic loss: 0.837, actor loss: 19.762\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 16.865, mean reward: 8.568, reward rate: 0.508, rewarded fraction: 0.820, relative distance: 47.167, critic loss: 0.845, actor loss: 20.076\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 16.966, mean reward: 8.444, reward rate: 0.498, rewarded fraction: 0.803, relative distance: 46.641, critic loss: 0.849, actor loss: 20.373\n",
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 17.165, mean reward: 8.710, reward rate: 0.507, rewarded fraction: 0.840, relative distance: 45.998, critic loss: 0.868, actor loss: 20.657\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 16.640, mean reward: 8.293, reward rate: 0.498, rewarded fraction: 0.789, relative distance: 49.740, critic loss: 0.882, actor loss: 20.914\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 17.006, mean reward: 8.510, reward rate: 0.500, rewarded fraction: 0.817, relative distance: 49.386, critic loss: 0.888, actor loss: 21.146\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 16.639, mean reward: 8.436, reward rate: 0.507, rewarded fraction: 0.808, relative distance: 48.297, critic loss: 0.904, actor loss: 21.354\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 16.955, mean reward: 8.254, reward rate: 0.487, rewarded fraction: 0.785, relative distance: 49.734, critic loss: 0.901, actor loss: 21.501\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 17.099, mean reward: 8.433, reward rate: 0.493, rewarded fraction: 0.807, relative distance: 48.173, critic loss: 0.906, actor loss: 21.635\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 16.989, mean reward: 8.391, reward rate: 0.494, rewarded fraction: 0.799, relative distance: 48.138, critic loss: 0.927, actor loss: 21.753\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 16.876, mean reward: 8.439, reward rate: 0.500, rewarded fraction: 0.811, relative distance: 49.925, critic loss: 0.930, actor loss: 21.874\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 16.933, mean reward: 8.620, reward rate: 0.509, rewarded fraction: 0.832, relative distance: 47.515, critic loss: 0.938, actor loss: 21.973\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 17.529, mean reward: 8.466, reward rate: 0.483, rewarded fraction: 0.811, relative distance: 49.529, critic loss: 0.913, actor loss: 22.055\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 17.060, mean reward: 8.652, reward rate: 0.507, rewarded fraction: 0.831, relative distance: 47.269, critic loss: 0.907, actor loss: 22.140\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 17.408, mean reward: 8.335, reward rate: 0.479, rewarded fraction: 0.790, relative distance: 45.843, critic loss: 0.888, actor loss: 22.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 16.880, mean reward: 8.358, reward rate: 0.495, rewarded fraction: 0.798, relative distance: 49.728, critic loss: 0.905, actor loss: 22.292\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 17.019, mean reward: 8.514, reward rate: 0.500, rewarded fraction: 0.813, relative distance: 48.525, critic loss: 0.896, actor loss: 22.370\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 17.149, mean reward: 8.805, reward rate: 0.513, rewarded fraction: 0.849, relative distance: 45.283, critic loss: 0.891, actor loss: 22.467\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 17.344, mean reward: 8.283, reward rate: 0.478, rewarded fraction: 0.788, relative distance: 49.115, critic loss: 0.918, actor loss: 22.528\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 17.281, mean reward: 8.634, reward rate: 0.500, rewarded fraction: 0.829, relative distance: 48.212, critic loss: 0.904, actor loss: 22.600\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 17.683, mean reward: 8.553, reward rate: 0.484, rewarded fraction: 0.820, relative distance: 47.845, critic loss: 0.906, actor loss: 22.667\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 17.252, mean reward: 8.308, reward rate: 0.482, rewarded fraction: 0.793, relative distance: 49.656, critic loss: 0.913, actor loss: 22.730\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 17.082, mean reward: 8.464, reward rate: 0.495, rewarded fraction: 0.812, relative distance: 48.151, critic loss: 0.916, actor loss: 22.778\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 17.210, mean reward: 8.481, reward rate: 0.493, rewarded fraction: 0.815, relative distance: 47.909, critic loss: 0.926, actor loss: 22.818\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 17.191, mean reward: 8.439, reward rate: 0.491, rewarded fraction: 0.809, relative distance: 49.350, critic loss: 0.923, actor loss: 22.853\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 17.531, mean reward: 8.502, reward rate: 0.485, rewarded fraction: 0.813, relative distance: 47.653, critic loss: 0.924, actor loss: 22.908\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 17.017, mean reward: 8.389, reward rate: 0.493, rewarded fraction: 0.801, relative distance: 50.092, critic loss: 0.923, actor loss: 22.972\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 17.215, mean reward: 8.348, reward rate: 0.485, rewarded fraction: 0.795, relative distance: 49.373, critic loss: 0.934, actor loss: 23.025\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 17.389, mean reward: 8.352, reward rate: 0.480, rewarded fraction: 0.798, relative distance: 49.694, critic loss: 0.931, actor loss: 23.069\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 17.077, mean reward: 8.516, reward rate: 0.499, rewarded fraction: 0.816, relative distance: 46.293, critic loss: 0.950, actor loss: 23.118\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 17.077, mean reward: 8.291, reward rate: 0.485, rewarded fraction: 0.788, relative distance: 48.416, critic loss: 0.955, actor loss: 23.153\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 16.866, mean reward: 8.517, reward rate: 0.505, rewarded fraction: 0.817, relative distance: 48.830, critic loss: 0.964, actor loss: 23.190\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 17.028, mean reward: 8.488, reward rate: 0.498, rewarded fraction: 0.814, relative distance: 50.325, critic loss: 0.963, actor loss: 23.214\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 16.872, mean reward: 8.397, reward rate: 0.498, rewarded fraction: 0.806, relative distance: 49.968, critic loss: 0.934, actor loss: 23.261\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 17.406, mean reward: 8.256, reward rate: 0.474, rewarded fraction: 0.787, relative distance: 51.830, critic loss: 0.954, actor loss: 23.293\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 17.104, mean reward: 8.533, reward rate: 0.499, rewarded fraction: 0.820, relative distance: 49.177, critic loss: 0.961, actor loss: 23.340\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 17.182, mean reward: 8.331, reward rate: 0.485, rewarded fraction: 0.792, relative distance: 48.893, critic loss: 0.969, actor loss: 23.373\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 16.949, mean reward: 8.461, reward rate: 0.499, rewarded fraction: 0.809, relative distance: 47.327, critic loss: 0.970, actor loss: 23.412\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 17.440, mean reward: 8.518, reward rate: 0.488, rewarded fraction: 0.815, relative distance: 46.904, critic loss: 0.988, actor loss: 23.439\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 17.231, mean reward: 8.398, reward rate: 0.487, rewarded fraction: 0.796, relative distance: 45.567, critic loss: 0.981, actor loss: 23.453\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 17.104, mean reward: 8.444, reward rate: 0.494, rewarded fraction: 0.810, relative distance: 48.959, critic loss: 0.977, actor loss: 23.448\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 16.825, mean reward: 8.203, reward rate: 0.488, rewarded fraction: 0.781, relative distance: 50.316, critic loss: 0.971, actor loss: 23.468\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 17.586, mean reward: 8.427, reward rate: 0.479, rewarded fraction: 0.805, relative distance: 48.415, critic loss: 0.991, actor loss: 23.485\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 17.166, mean reward: 8.536, reward rate: 0.497, rewarded fraction: 0.817, relative distance: 48.132, critic loss: 1.000, actor loss: 23.495\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 17.275, mean reward: 8.398, reward rate: 0.486, rewarded fraction: 0.801, relative distance: 48.948, critic loss: 1.028, actor loss: 23.489\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 17.249, mean reward: 8.600, reward rate: 0.499, rewarded fraction: 0.826, relative distance: 47.331, critic loss: 1.015, actor loss: 23.472\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 16.822, mean reward: 8.379, reward rate: 0.498, rewarded fraction: 0.803, relative distance: 51.347, critic loss: 1.011, actor loss: 23.484\n",
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 16.966, mean reward: 8.462, reward rate: 0.499, rewarded fraction: 0.807, relative distance: 49.699, critic loss: 1.001, actor loss: 23.498\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 17.450, mean reward: 8.287, reward rate: 0.475, rewarded fraction: 0.791, relative distance: 49.628, critic loss: 0.988, actor loss: 23.530\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 17.283, mean reward: 8.533, reward rate: 0.494, rewarded fraction: 0.821, relative distance: 48.848, critic loss: 0.971, actor loss: 23.580\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 16.783, mean reward: 8.513, reward rate: 0.507, rewarded fraction: 0.816, relative distance: 48.366, critic loss: 0.992, actor loss: 23.609\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 17.088, mean reward: 8.359, reward rate: 0.489, rewarded fraction: 0.797, relative distance: 48.740, critic loss: 1.008, actor loss: 23.634\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 17.226, mean reward: 8.295, reward rate: 0.482, rewarded fraction: 0.792, relative distance: 50.087, critic loss: 1.025, actor loss: 23.667\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 17.390, mean reward: 8.626, reward rate: 0.496, rewarded fraction: 0.830, relative distance: 46.904, critic loss: 0.998, actor loss: 23.690\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 16.959, mean reward: 8.195, reward rate: 0.483, rewarded fraction: 0.780, relative distance: 51.827, critic loss: 1.011, actor loss: 23.693\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 16.788, mean reward: 8.389, reward rate: 0.500, rewarded fraction: 0.803, relative distance: 50.624, critic loss: 1.014, actor loss: 23.707\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 16.959, mean reward: 8.485, reward rate: 0.500, rewarded fraction: 0.818, relative distance: 50.889, critic loss: 0.999, actor loss: 23.721\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 17.079, mean reward: 8.415, reward rate: 0.493, rewarded fraction: 0.805, relative distance: 53.016, critic loss: 1.020, actor loss: 23.716\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 16.970, mean reward: 8.380, reward rate: 0.494, rewarded fraction: 0.805, relative distance: 51.023, critic loss: 1.022, actor loss: 23.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 17.175, mean reward: 8.250, reward rate: 0.480, rewarded fraction: 0.791, relative distance: 53.480, critic loss: 1.017, actor loss: 23.754\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 17.102, mean reward: 8.525, reward rate: 0.499, rewarded fraction: 0.821, relative distance: 49.908, critic loss: 1.015, actor loss: 23.782\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 17.216, mean reward: 8.390, reward rate: 0.487, rewarded fraction: 0.802, relative distance: 50.806, critic loss: 1.025, actor loss: 23.791\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 17.113, mean reward: 8.425, reward rate: 0.492, rewarded fraction: 0.805, relative distance: 49.353, critic loss: 1.034, actor loss: 23.812\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 16.772, mean reward: 8.302, reward rate: 0.495, rewarded fraction: 0.794, relative distance: 50.975, critic loss: 1.016, actor loss: 23.815\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 17.062, mean reward: 8.590, reward rate: 0.503, rewarded fraction: 0.824, relative distance: 47.774, critic loss: 1.016, actor loss: 23.817\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 17.194, mean reward: 8.796, reward rate: 0.512, rewarded fraction: 0.852, relative distance: 46.985, critic loss: 1.032, actor loss: 23.809\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 16.769, mean reward: 8.384, reward rate: 0.500, rewarded fraction: 0.803, relative distance: 51.493, critic loss: 1.000, actor loss: 23.816\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 17.132, mean reward: 8.387, reward rate: 0.490, rewarded fraction: 0.806, relative distance: 50.411, critic loss: 1.015, actor loss: 23.807\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 16.826, mean reward: 8.544, reward rate: 0.508, rewarded fraction: 0.820, relative distance: 46.022, critic loss: 1.009, actor loss: 23.807\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 16.723, mean reward: 8.271, reward rate: 0.495, rewarded fraction: 0.790, relative distance: 52.511, critic loss: 1.042, actor loss: 23.802\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 17.202, mean reward: 8.287, reward rate: 0.482, rewarded fraction: 0.792, relative distance: 49.454, critic loss: 1.029, actor loss: 23.801\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 17.258, mean reward: 8.642, reward rate: 0.501, rewarded fraction: 0.831, relative distance: 48.677, critic loss: 1.032, actor loss: 23.797\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 17.191, mean reward: 8.567, reward rate: 0.498, rewarded fraction: 0.822, relative distance: 48.041, critic loss: 1.047, actor loss: 23.794\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 17.117, mean reward: 8.493, reward rate: 0.496, rewarded fraction: 0.812, relative distance: 46.731, critic loss: 1.041, actor loss: 23.775\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 17.004, mean reward: 8.566, reward rate: 0.504, rewarded fraction: 0.822, relative distance: 48.998, critic loss: 1.003, actor loss: 23.797\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 17.337, mean reward: 8.221, reward rate: 0.474, rewarded fraction: 0.780, relative distance: 50.813, critic loss: 1.016, actor loss: 23.801\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 16.929, mean reward: 8.470, reward rate: 0.500, rewarded fraction: 0.812, relative distance: 49.578, critic loss: 1.033, actor loss: 23.796\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 17.084, mean reward: 8.361, reward rate: 0.489, rewarded fraction: 0.800, relative distance: 50.452, critic loss: 1.044, actor loss: 23.755\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 16.550, mean reward: 8.387, reward rate: 0.507, rewarded fraction: 0.804, relative distance: 50.901, critic loss: 1.027, actor loss: 23.750\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 17.207, mean reward: 8.637, reward rate: 0.502, rewarded fraction: 0.835, relative distance: 49.283, critic loss: 1.062, actor loss: 23.730\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 17.418, mean reward: 8.494, reward rate: 0.488, rewarded fraction: 0.814, relative distance: 50.897, critic loss: 1.043, actor loss: 23.725\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 16.951, mean reward: 8.314, reward rate: 0.490, rewarded fraction: 0.792, relative distance: 50.143, critic loss: 1.053, actor loss: 23.737\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 16.614, mean reward: 8.229, reward rate: 0.495, rewarded fraction: 0.781, relative distance: 51.656, critic loss: 1.035, actor loss: 23.755\n",
      "20240430-183340-199\n",
      "No param. frozen.\n",
      "t: 10000, Ep: 99, action std: 0.50\n",
      "mean steps: 12.128, mean reward: 4.212, reward rate: 0.347, rewarded fraction: 0.395, relative distance: 132.898, critic loss: 0.000, actor loss: 0.000\n",
      "t: 20000, Ep: 199, action std: 0.50\n",
      "mean steps: 11.968, mean reward: 4.391, reward rate: 0.367, rewarded fraction: 0.418, relative distance: 133.149, critic loss: 0.000, actor loss: 0.000\n",
      "t: 30000, Ep: 299, action std: 0.50\n",
      "mean steps: 12.049, mean reward: 4.472, reward rate: 0.371, rewarded fraction: 0.422, relative distance: 131.378, critic loss: 0.000, actor loss: 0.000\n",
      "t: 40000, Ep: 399, action std: 0.50\n",
      "mean steps: 11.968, mean reward: 4.423, reward rate: 0.370, rewarded fraction: 0.421, relative distance: 131.946, critic loss: 0.000, actor loss: 0.000\n",
      "t: 50000, Ep: 499, action std: 0.50\n",
      "mean steps: 16.326, mean reward: 7.129, reward rate: 0.437, rewarded fraction: 0.658, relative distance: 58.176, critic loss: 0.965, actor loss: 8.335\n",
      "t: 60000, Ep: 599, action std: 0.50\n",
      "mean steps: 17.843, mean reward: 8.677, reward rate: 0.486, rewarded fraction: 0.839, relative distance: 46.095, critic loss: 1.236, actor loss: 10.600\n",
      "t: 70000, Ep: 699, action std: 0.50\n",
      "mean steps: 17.021, mean reward: 8.324, reward rate: 0.489, rewarded fraction: 0.791, relative distance: 49.616, critic loss: 1.530, actor loss: 12.899\n",
      "t: 80000, Ep: 799, action std: 0.50\n",
      "mean steps: 16.816, mean reward: 8.544, reward rate: 0.508, rewarded fraction: 0.816, relative distance: 47.400, critic loss: 1.088, actor loss: 14.714\n",
      "t: 90000, Ep: 899, action std: 0.50\n",
      "mean steps: 17.570, mean reward: 8.601, reward rate: 0.490, rewarded fraction: 0.829, relative distance: 47.688, critic loss: 0.990, actor loss: 16.250\n",
      "t: 100000, Ep: 999, action std: 0.50\n",
      "mean steps: 17.562, mean reward: 8.381, reward rate: 0.477, rewarded fraction: 0.800, relative distance: 48.996, critic loss: 1.027, actor loss: 17.566\n",
      "t: 110000, Ep: 1099, action std: 0.50\n",
      "mean steps: 17.196, mean reward: 8.335, reward rate: 0.485, rewarded fraction: 0.796, relative distance: 48.702, critic loss: 0.968, actor loss: 18.630\n",
      "t: 120000, Ep: 1199, action std: 0.50\n",
      "mean steps: 17.336, mean reward: 8.319, reward rate: 0.480, rewarded fraction: 0.791, relative distance: 49.494, critic loss: 0.939, actor loss: 19.448\n",
      "t: 130000, Ep: 1299, action std: 0.50\n",
      "mean steps: 17.140, mean reward: 8.300, reward rate: 0.484, rewarded fraction: 0.793, relative distance: 52.238, critic loss: 0.927, actor loss: 20.145\n",
      "t: 140000, Ep: 1399, action std: 0.50\n",
      "mean steps: 17.196, mean reward: 8.284, reward rate: 0.482, rewarded fraction: 0.785, relative distance: 48.542, critic loss: 0.902, actor loss: 20.782\n",
      "t: 150000, Ep: 1499, action std: 0.50\n",
      "mean steps: 17.293, mean reward: 8.433, reward rate: 0.488, rewarded fraction: 0.805, relative distance: 49.357, critic loss: 0.924, actor loss: 21.352\n",
      "t: 160000, Ep: 1599, action std: 0.50\n",
      "mean steps: 17.083, mean reward: 8.370, reward rate: 0.490, rewarded fraction: 0.799, relative distance: 50.915, critic loss: 0.933, actor loss: 21.857\n",
      "t: 170000, Ep: 1699, action std: 0.50\n",
      "mean steps: 17.358, mean reward: 8.422, reward rate: 0.485, rewarded fraction: 0.810, relative distance: 49.086, critic loss: 0.936, actor loss: 22.334\n",
      "t: 180000, Ep: 1799, action std: 0.50\n",
      "mean steps: 16.788, mean reward: 8.188, reward rate: 0.488, rewarded fraction: 0.773, relative distance: 50.175, critic loss: 0.964, actor loss: 22.743\n",
      "t: 190000, Ep: 1899, action std: 0.50\n",
      "mean steps: 17.599, mean reward: 8.796, reward rate: 0.500, rewarded fraction: 0.852, relative distance: 45.416, critic loss: 0.938, actor loss: 23.102\n",
      "t: 200000, Ep: 1999, action std: 0.50\n",
      "mean steps: 17.676, mean reward: 8.638, reward rate: 0.489, rewarded fraction: 0.830, relative distance: 47.009, critic loss: 0.948, actor loss: 23.432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 210000, Ep: 2099, action std: 0.50\n",
      "mean steps: 17.340, mean reward: 8.539, reward rate: 0.492, rewarded fraction: 0.820, relative distance: 47.926, critic loss: 0.945, actor loss: 23.696\n",
      "t: 220000, Ep: 2199, action std: 0.50\n",
      "mean steps: 17.277, mean reward: 8.369, reward rate: 0.484, rewarded fraction: 0.802, relative distance: 51.873, critic loss: 0.929, actor loss: 23.906\n",
      "t: 230000, Ep: 2299, action std: 0.50\n",
      "mean steps: 17.195, mean reward: 8.612, reward rate: 0.501, rewarded fraction: 0.832, relative distance: 49.515, critic loss: 0.927, actor loss: 24.099\n",
      "t: 240000, Ep: 2399, action std: 0.50\n",
      "mean steps: 17.308, mean reward: 8.660, reward rate: 0.500, rewarded fraction: 0.836, relative distance: 46.712, critic loss: 0.929, actor loss: 24.258\n",
      "t: 250000, Ep: 2499, action std: 0.50\n",
      "mean steps: 17.372, mean reward: 8.323, reward rate: 0.479, rewarded fraction: 0.792, relative distance: 49.679, critic loss: 0.921, actor loss: 24.381\n",
      "t: 260000, Ep: 2599, action std: 0.50\n",
      "mean steps: 17.250, mean reward: 8.445, reward rate: 0.490, rewarded fraction: 0.806, relative distance: 48.674, critic loss: 0.935, actor loss: 24.472\n",
      "t: 270000, Ep: 2699, action std: 0.50\n",
      "mean steps: 17.439, mean reward: 8.659, reward rate: 0.497, rewarded fraction: 0.830, relative distance: 45.565, critic loss: 0.920, actor loss: 24.579\n",
      "t: 280000, Ep: 2799, action std: 0.50\n",
      "mean steps: 17.139, mean reward: 8.390, reward rate: 0.490, rewarded fraction: 0.799, relative distance: 48.749, critic loss: 0.915, actor loss: 24.668\n",
      "t: 290000, Ep: 2899, action std: 0.50\n",
      "mean steps: 17.673, mean reward: 8.537, reward rate: 0.483, rewarded fraction: 0.815, relative distance: 47.612, critic loss: 0.920, actor loss: 24.719\n",
      "t: 300000, Ep: 2999, action std: 0.50\n",
      "mean steps: 16.960, mean reward: 8.790, reward rate: 0.518, rewarded fraction: 0.849, relative distance: 46.385, critic loss: 0.927, actor loss: 24.762\n",
      "t: 310000, Ep: 3099, action std: 0.50\n",
      "mean steps: 17.017, mean reward: 8.736, reward rate: 0.513, rewarded fraction: 0.845, relative distance: 46.593, critic loss: 0.938, actor loss: 24.792\n",
      "t: 320000, Ep: 3199, action std: 0.50\n",
      "mean steps: 16.925, mean reward: 8.403, reward rate: 0.496, rewarded fraction: 0.803, relative distance: 49.136, critic loss: 0.944, actor loss: 24.820\n",
      "t: 330000, Ep: 3299, action std: 0.50\n",
      "mean steps: 16.928, mean reward: 8.470, reward rate: 0.500, rewarded fraction: 0.813, relative distance: 48.444, critic loss: 0.940, actor loss: 24.864\n",
      "t: 340000, Ep: 3399, action std: 0.50\n",
      "mean steps: 17.424, mean reward: 8.297, reward rate: 0.476, rewarded fraction: 0.792, relative distance: 50.244, critic loss: 0.941, actor loss: 24.912\n",
      "t: 350000, Ep: 3499, action std: 0.50\n",
      "mean steps: 17.234, mean reward: 8.647, reward rate: 0.502, rewarded fraction: 0.832, relative distance: 46.832, critic loss: 0.934, actor loss: 24.957\n",
      "t: 360000, Ep: 3599, action std: 0.50\n",
      "mean steps: 17.820, mean reward: 8.577, reward rate: 0.481, rewarded fraction: 0.822, relative distance: 47.356, critic loss: 0.946, actor loss: 24.990\n",
      "t: 370000, Ep: 3699, action std: 0.50\n",
      "mean steps: 17.110, mean reward: 8.667, reward rate: 0.507, rewarded fraction: 0.837, relative distance: 48.639, critic loss: 0.948, actor loss: 24.995\n",
      "t: 380000, Ep: 3799, action std: 0.50\n",
      "mean steps: 16.876, mean reward: 8.494, reward rate: 0.503, rewarded fraction: 0.809, relative distance: 46.876, critic loss: 0.966, actor loss: 24.995\n",
      "t: 390000, Ep: 3899, action std: 0.50\n",
      "mean steps: 16.952, mean reward: 8.395, reward rate: 0.495, rewarded fraction: 0.797, relative distance: 47.443, critic loss: 0.939, actor loss: 24.999\n",
      "t: 400000, Ep: 3999, action std: 0.50\n",
      "mean steps: 17.090, mean reward: 8.663, reward rate: 0.507, rewarded fraction: 0.836, relative distance: 47.140, critic loss: 0.937, actor loss: 24.995\n",
      "t: 410000, Ep: 4099, action std: 0.50\n",
      "mean steps: 16.880, mean reward: 8.776, reward rate: 0.520, rewarded fraction: 0.846, relative distance: 46.006, critic loss: 0.939, actor loss: 24.974\n",
      "t: 420000, Ep: 4199, action std: 0.50\n",
      "mean steps: 16.903, mean reward: 8.591, reward rate: 0.508, rewarded fraction: 0.821, relative distance: 46.585, critic loss: 0.950, actor loss: 24.962\n",
      "t: 430000, Ep: 4299, action std: 0.50\n",
      "mean steps: 17.130, mean reward: 8.497, reward rate: 0.496, rewarded fraction: 0.816, relative distance: 48.954, critic loss: 0.961, actor loss: 24.947\n",
      "t: 440000, Ep: 4399, action std: 0.50\n",
      "mean steps: 17.478, mean reward: 8.502, reward rate: 0.486, rewarded fraction: 0.812, relative distance: 47.198, critic loss: 0.940, actor loss: 24.935\n",
      "t: 450000, Ep: 4499, action std: 0.50\n",
      "mean steps: 17.509, mean reward: 8.579, reward rate: 0.490, rewarded fraction: 0.823, relative distance: 46.638, critic loss: 0.947, actor loss: 24.936\n",
      "t: 460000, Ep: 4599, action std: 0.50\n",
      "mean steps: 17.337, mean reward: 8.719, reward rate: 0.503, rewarded fraction: 0.843, relative distance: 46.007, critic loss: 0.949, actor loss: 24.948\n",
      "t: 470000, Ep: 4699, action std: 0.50\n",
      "mean steps: 17.363, mean reward: 8.479, reward rate: 0.488, rewarded fraction: 0.808, relative distance: 46.348, critic loss: 0.963, actor loss: 24.948\n",
      "t: 480000, Ep: 4799, action std: 0.50\n",
      "mean steps: 17.007, mean reward: 8.537, reward rate: 0.502, rewarded fraction: 0.817, relative distance: 47.290, critic loss: 0.970, actor loss: 24.925\n",
      "t: 490000, Ep: 4899, action std: 0.50\n",
      "mean steps: 17.144, mean reward: 8.649, reward rate: 0.504, rewarded fraction: 0.829, relative distance: 45.805, critic loss: 0.959, actor loss: 24.922\n",
      "t: 500000, Ep: 4999, action std: 0.50\n",
      "mean steps: 17.216, mean reward: 8.564, reward rate: 0.497, rewarded fraction: 0.823, relative distance: 47.200, critic loss: 0.974, actor loss: 24.938\n",
      "t: 510000, Ep: 5099, action std: 0.50\n",
      "mean steps: 17.293, mean reward: 8.486, reward rate: 0.491, rewarded fraction: 0.813, relative distance: 48.987, critic loss: 0.961, actor loss: 24.948\n",
      "t: 520000, Ep: 5199, action std: 0.50\n",
      "mean steps: 17.379, mean reward: 8.539, reward rate: 0.491, rewarded fraction: 0.819, relative distance: 48.574, critic loss: 0.968, actor loss: 24.952\n",
      "t: 530000, Ep: 5299, action std: 0.50\n",
      "mean steps: 16.969, mean reward: 8.918, reward rate: 0.526, rewarded fraction: 0.866, relative distance: 45.093, critic loss: 0.956, actor loss: 24.961\n",
      "t: 540000, Ep: 5399, action std: 0.50\n",
      "mean steps: 17.176, mean reward: 8.685, reward rate: 0.506, rewarded fraction: 0.839, relative distance: 46.177, critic loss: 0.968, actor loss: 24.969\n",
      "t: 550000, Ep: 5499, action std: 0.50\n",
      "mean steps: 17.172, mean reward: 8.809, reward rate: 0.513, rewarded fraction: 0.848, relative distance: 45.250, critic loss: 0.967, actor loss: 24.995\n",
      "t: 560000, Ep: 5599, action std: 0.50\n",
      "mean steps: 17.141, mean reward: 8.732, reward rate: 0.509, rewarded fraction: 0.842, relative distance: 46.324, critic loss: 0.974, actor loss: 25.001\n",
      "t: 570000, Ep: 5699, action std: 0.50\n",
      "mean steps: 17.179, mean reward: 8.640, reward rate: 0.503, rewarded fraction: 0.832, relative distance: 47.923, critic loss: 0.993, actor loss: 25.008\n",
      "t: 580000, Ep: 5799, action std: 0.50\n",
      "mean steps: 17.056, mean reward: 8.494, reward rate: 0.498, rewarded fraction: 0.815, relative distance: 48.936, critic loss: 0.987, actor loss: 25.026\n",
      "t: 590000, Ep: 5899, action std: 0.50\n",
      "mean steps: 17.424, mean reward: 8.672, reward rate: 0.498, rewarded fraction: 0.832, relative distance: 45.306, critic loss: 0.994, actor loss: 25.046\n",
      "t: 600000, Ep: 5999, action std: 0.50\n",
      "mean steps: 17.076, mean reward: 8.539, reward rate: 0.500, rewarded fraction: 0.816, relative distance: 47.637, critic loss: 1.016, actor loss: 25.041\n",
      "t: 610000, Ep: 6099, action std: 0.50\n",
      "mean steps: 17.226, mean reward: 8.662, reward rate: 0.503, rewarded fraction: 0.834, relative distance: 47.882, critic loss: 0.977, actor loss: 25.036\n",
      "t: 620000, Ep: 6199, action std: 0.50\n",
      "mean steps: 16.628, mean reward: 8.486, reward rate: 0.510, rewarded fraction: 0.811, relative distance: 47.048, critic loss: 0.990, actor loss: 25.041\n",
      "t: 630000, Ep: 6299, action std: 0.50\n",
      "mean steps: 17.112, mean reward: 8.832, reward rate: 0.516, rewarded fraction: 0.856, relative distance: 45.832, critic loss: 0.969, actor loss: 25.040\n",
      "t: 640000, Ep: 6399, action std: 0.50\n",
      "mean steps: 16.766, mean reward: 8.895, reward rate: 0.531, rewarded fraction: 0.860, relative distance: 44.458, critic loss: 0.978, actor loss: 25.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 650000, Ep: 6499, action std: 0.50\n",
      "mean steps: 17.032, mean reward: 8.363, reward rate: 0.491, rewarded fraction: 0.799, relative distance: 48.476, critic loss: 0.967, actor loss: 25.040\n",
      "t: 660000, Ep: 6599, action std: 0.50\n",
      "mean steps: 16.981, mean reward: 8.541, reward rate: 0.503, rewarded fraction: 0.816, relative distance: 47.892, critic loss: 0.980, actor loss: 25.021\n",
      "t: 670000, Ep: 6699, action std: 0.50\n",
      "mean steps: 17.623, mean reward: 8.806, reward rate: 0.500, rewarded fraction: 0.851, relative distance: 46.313, critic loss: 0.970, actor loss: 25.019\n",
      "t: 680000, Ep: 6799, action std: 0.50\n",
      "mean steps: 17.495, mean reward: 8.560, reward rate: 0.489, rewarded fraction: 0.820, relative distance: 46.087, critic loss: 0.986, actor loss: 25.015\n",
      "t: 690000, Ep: 6899, action std: 0.50\n",
      "mean steps: 16.983, mean reward: 8.357, reward rate: 0.492, rewarded fraction: 0.792, relative distance: 47.743, critic loss: 0.956, actor loss: 25.007\n",
      "t: 700000, Ep: 6999, action std: 0.50\n",
      "mean steps: 17.105, mean reward: 8.641, reward rate: 0.505, rewarded fraction: 0.827, relative distance: 44.775, critic loss: 0.975, actor loss: 24.979\n",
      "t: 710000, Ep: 7099, action std: 0.50\n",
      "mean steps: 17.090, mean reward: 8.382, reward rate: 0.491, rewarded fraction: 0.799, relative distance: 49.479, critic loss: 0.968, actor loss: 24.968\n",
      "t: 720000, Ep: 7199, action std: 0.50\n",
      "mean steps: 17.037, mean reward: 8.391, reward rate: 0.493, rewarded fraction: 0.803, relative distance: 48.932, critic loss: 0.973, actor loss: 24.983\n",
      "t: 730000, Ep: 7299, action std: 0.50\n",
      "mean steps: 16.769, mean reward: 8.562, reward rate: 0.511, rewarded fraction: 0.822, relative distance: 46.817, critic loss: 0.978, actor loss: 24.989\n",
      "t: 740000, Ep: 7399, action std: 0.50\n",
      "mean steps: 17.052, mean reward: 8.331, reward rate: 0.489, rewarded fraction: 0.789, relative distance: 48.489, critic loss: 0.975, actor loss: 25.005\n",
      "t: 750000, Ep: 7499, action std: 0.50\n",
      "mean steps: 17.710, mean reward: 8.593, reward rate: 0.485, rewarded fraction: 0.825, relative distance: 45.760, critic loss: 0.971, actor loss: 25.018\n",
      "t: 760000, Ep: 7599, action std: 0.50\n",
      "mean steps: 16.998, mean reward: 8.694, reward rate: 0.511, rewarded fraction: 0.835, relative distance: 45.818, critic loss: 0.991, actor loss: 25.014\n",
      "t: 770000, Ep: 7699, action std: 0.50\n",
      "mean steps: 16.953, mean reward: 8.676, reward rate: 0.512, rewarded fraction: 0.832, relative distance: 46.885, critic loss: 0.983, actor loss: 25.014\n",
      "t: 780000, Ep: 7799, action std: 0.50\n",
      "mean steps: 17.148, mean reward: 8.929, reward rate: 0.521, rewarded fraction: 0.869, relative distance: 44.122, critic loss: 0.971, actor loss: 25.009\n",
      "t: 790000, Ep: 7899, action std: 0.50\n",
      "mean steps: 16.961, mean reward: 8.408, reward rate: 0.496, rewarded fraction: 0.804, relative distance: 49.067, critic loss: 0.984, actor loss: 25.007\n",
      "t: 800000, Ep: 7999, action std: 0.50\n",
      "mean steps: 17.163, mean reward: 8.682, reward rate: 0.506, rewarded fraction: 0.833, relative distance: 46.715, critic loss: 0.987, actor loss: 25.003\n",
      "t: 810000, Ep: 8099, action std: 0.50\n",
      "mean steps: 17.163, mean reward: 8.597, reward rate: 0.501, rewarded fraction: 0.824, relative distance: 46.288, critic loss: 0.977, actor loss: 24.992\n",
      "t: 820000, Ep: 8199, action std: 0.50\n",
      "mean steps: 16.693, mean reward: 8.663, reward rate: 0.519, rewarded fraction: 0.830, relative distance: 45.126, critic loss: 0.967, actor loss: 25.011\n",
      "t: 830000, Ep: 8299, action std: 0.50\n",
      "mean steps: 17.213, mean reward: 8.578, reward rate: 0.498, rewarded fraction: 0.818, relative distance: 45.707, critic loss: 0.983, actor loss: 25.003\n",
      "t: 840000, Ep: 8399, action std: 0.50\n",
      "mean steps: 17.144, mean reward: 8.600, reward rate: 0.502, rewarded fraction: 0.828, relative distance: 47.229, critic loss: 0.977, actor loss: 24.984\n",
      "t: 850000, Ep: 8499, action std: 0.50\n",
      "mean steps: 17.267, mean reward: 8.576, reward rate: 0.497, rewarded fraction: 0.821, relative distance: 46.570, critic loss: 0.992, actor loss: 24.967\n",
      "t: 860000, Ep: 8599, action std: 0.50\n",
      "mean steps: 16.696, mean reward: 8.730, reward rate: 0.523, rewarded fraction: 0.840, relative distance: 45.537, critic loss: 0.981, actor loss: 24.951\n",
      "t: 870000, Ep: 8699, action std: 0.50\n",
      "mean steps: 17.071, mean reward: 8.536, reward rate: 0.500, rewarded fraction: 0.818, relative distance: 46.944, critic loss: 0.983, actor loss: 24.948\n",
      "t: 880000, Ep: 8799, action std: 0.50\n",
      "mean steps: 16.749, mean reward: 8.547, reward rate: 0.510, rewarded fraction: 0.819, relative distance: 48.395, critic loss: 0.988, actor loss: 24.935\n",
      "t: 890000, Ep: 8899, action std: 0.50\n",
      "mean steps: 16.526, mean reward: 8.572, reward rate: 0.519, rewarded fraction: 0.823, relative distance: 46.472, critic loss: 0.995, actor loss: 24.914\n",
      "t: 900000, Ep: 8999, action std: 0.50\n",
      "mean steps: 17.257, mean reward: 8.396, reward rate: 0.486, rewarded fraction: 0.802, relative distance: 49.501, critic loss: 0.979, actor loss: 24.909\n",
      "t: 910000, Ep: 9099, action std: 0.50\n",
      "mean steps: 17.060, mean reward: 8.600, reward rate: 0.504, rewarded fraction: 0.829, relative distance: 47.604, critic loss: 0.993, actor loss: 24.919\n",
      "t: 920000, Ep: 9199, action std: 0.50\n",
      "mean steps: 16.953, mean reward: 8.228, reward rate: 0.485, rewarded fraction: 0.781, relative distance: 50.463, critic loss: 1.018, actor loss: 24.896\n",
      "t: 930000, Ep: 9299, action std: 0.50\n",
      "mean steps: 17.022, mean reward: 8.533, reward rate: 0.501, rewarded fraction: 0.817, relative distance: 47.757, critic loss: 1.003, actor loss: 24.872\n",
      "t: 940000, Ep: 9399, action std: 0.50\n",
      "mean steps: 17.250, mean reward: 8.573, reward rate: 0.497, rewarded fraction: 0.819, relative distance: 46.294, critic loss: 0.983, actor loss: 24.871\n",
      "t: 950000, Ep: 9499, action std: 0.50\n",
      "mean steps: 16.851, mean reward: 8.441, reward rate: 0.501, rewarded fraction: 0.803, relative distance: 47.818, critic loss: 0.985, actor loss: 24.867\n",
      "t: 960000, Ep: 9599, action std: 0.50\n",
      "mean steps: 16.626, mean reward: 8.533, reward rate: 0.513, rewarded fraction: 0.817, relative distance: 48.104, critic loss: 1.000, actor loss: 24.866\n",
      "t: 970000, Ep: 9699, action std: 0.50\n",
      "mean steps: 16.686, mean reward: 8.758, reward rate: 0.525, rewarded fraction: 0.841, relative distance: 45.537, critic loss: 0.993, actor loss: 24.871\n",
      "t: 980000, Ep: 9799, action std: 0.50\n",
      "mean steps: 17.067, mean reward: 8.673, reward rate: 0.508, rewarded fraction: 0.832, relative distance: 44.910, critic loss: 1.016, actor loss: 24.885\n",
      "t: 990000, Ep: 9899, action std: 0.50\n",
      "mean steps: 17.174, mean reward: 8.598, reward rate: 0.501, rewarded fraction: 0.826, relative distance: 46.391, critic loss: 1.026, actor loss: 24.872\n",
      "t: 1000000, Ep: 9999, action std: 0.50\n",
      "mean steps: 17.215, mean reward: 8.687, reward rate: 0.505, rewarded fraction: 0.830, relative distance: 45.362, critic loss: 1.037, actor loss: 24.856\n"
     ]
    }
   ],
   "source": [
    "for actor, critic, seed_ in zip(actors, critics, seeds):\n",
    "    for seed in seed_:\n",
    "        datapath = folder_path / f'{actor}{critic}' / f'seed{seed}' / f'{agent_type}'\n",
    "        exec(f'from {actor} import *'); exec(f'from {critic} import *')\n",
    "        training(datapath, seed, Actor, Critic, TOTAL_EPISODE, \n",
    "                 value_noise_std, freeze_RNN, freeze_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
